
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Torch Stable API &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="_static/collapsible-lists/css/tree_view.css?v=a885cde7" />
    <link rel="stylesheet" type="text/css" href="_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=a8da1a53"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/collapsible-lists/js/CollapsibleLists.compressed.js?v=73120307"></script>
    <script src="_static/collapsible-lists/js/apply-collapsible-lists.js?v=660e4f45"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'stable';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Library API" href="api/library_root.html" />
    <link rel="prev" title="The C++ Frontend" href="frontend.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Torch Stable API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api/library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/faq.html">
    FAQ
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="notes/versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Torch Stable API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api/library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="notes/versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Torch Stable API</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Torch Stable API">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="torch-stable-api">
<h1>Torch Stable API<a class="headerlink" href="#torch-stable-api" title="Link to this heading">#</a></h1>
<p>The PyTorch Stable C++ API provides a convenient high level interface to call
ABI-stable tensor operations and other utilities commonly used in custom operators.
These functions are designed to maintain binary compatibility across PyTorch versions,
making them suitable for use in ahead-of-time compiled code.</p>
<p>For more information on the stable ABI, see the
<a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/libtorch_stable_abi.html">Stable ABI notes</a>.</p>
<section id="library-registration-macros">
<h2>Library Registration Macros<a class="headerlink" href="#library-registration-macros" title="Link to this heading">#</a></h2>
<p>These macros provide stable ABI equivalents of the standard PyTorch operator
registration macros (<code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code>, <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code>, etc.).
Use these when building custom operators that need to maintain binary
compatibility across PyTorch versions.</p>
<section id="stable-torch-library-ns-m">
<h3><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY(ns,</span> <span class="pre">m)</span></code><a class="headerlink" href="#stable-torch-library-ns-m" title="Link to this heading">#</a></h3>
<p>Defines a library of operators in a namespace using the stable ABI.</p>
<p>This is the stable ABI equivalent of <a class="reference internal" href="api/define_library_8h_1a0bd5fb09d25dfb58e750d712fc5afb84.html#c.TORCH_LIBRARY" title="TORCH_LIBRARY"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TORCH_LIBRARY</span></code></a>.
Use this macro to define operator schemas that will maintain
binary compatibility across PyTorch versions. Only one <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY</span></code>
block can exist per namespace; use <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_FRAGMENT</span></code> for
additional definitions in the same namespace from different translation units.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ns</span></code> - The namespace in which to define operators (e.g., <code class="docutils literal notranslate"><span class="pre">mylib</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> - The name of the StableLibrary variable available in the block.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STABLE_TORCH_LIBRARY</span><span class="p">(</span><span class="n">mylib</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;my_op(Tensor input, int size) -&gt; Tensor&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;another_op(Tensor a, Tensor b) -&gt; Tensor&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Minimum compatible version: PyTorch 2.9.</p>
</section>
<section id="stable-torch-library-impl-ns-k-m">
<h3><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_IMPL(ns,</span> <span class="pre">k,</span> <span class="pre">m)</span></code><a class="headerlink" href="#stable-torch-library-impl-ns-k-m" title="Link to this heading">#</a></h3>
<p>Registers operator implementations for a specific dispatch key using the stable ABI.</p>
<p>This is the stable ABI equivalent of <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_IMPL</span></code>. Use this macro
to provide implementations of operators for a specific dispatch key (e.g.,
CPU, CUDA) while maintaining binary compatibility across PyTorch versions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All kernel functions registered with this macro must be boxed using
the <code class="docutils literal notranslate"><span class="pre">TORCH_BOX</span></code> macro.</p>
</div>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ns</span></code> - The namespace in which the operators are defined.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> - The dispatch key (e.g., <code class="docutils literal notranslate"><span class="pre">CPU</span></code>, <code class="docutils literal notranslate"><span class="pre">CUDA</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> - The name of the StableLibrary variable available in the block.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">mylib</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;my_op&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">my_cpu_kernel</span><span class="p">));</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">mylib</span><span class="p">,</span><span class="w"> </span><span class="n">CUDA</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;my_op&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">my_cuda_kernel</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Minimum compatible version: PyTorch 2.9.</p>
</section>
<section id="stable-torch-library-fragment-ns-m">
<h3><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_FRAGMENT(ns,</span> <span class="pre">m)</span></code><a class="headerlink" href="#stable-torch-library-fragment-ns-m" title="Link to this heading">#</a></h3>
<p>Extends operator definitions in an existing namespace using the stable ABI.</p>
<p>This is the stable ABI equivalent of <code class="docutils literal notranslate"><span class="pre">TORCH_LIBRARY_FRAGMENT</span></code>. Use this macro
to add additional operator definitions to a namespace that was already
created with <code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY</span></code>.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ns</span></code> - The namespace to extend.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> - The name of the StableLibrary variable available in the block.</p></li>
</ul>
<p>Minimum compatible version: PyTorch 2.9.</p>
</section>
<section id="torch-box-func">
<h3><code class="docutils literal notranslate"><span class="pre">TORCH_BOX(&amp;func)</span></code><a class="headerlink" href="#torch-box-func" title="Link to this heading">#</a></h3>
<p>Wraps a function to conform to the stable boxed kernel calling convention.</p>
<p>This macro takes an unboxed kernel function pointer and generates a boxed wrapper
that can be registered with the stable library API.</p>
<p><strong>Parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">func</span></code> - The unboxed kernel function to wrap.</p></li>
</ul>
<p><strong>Example:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">Tensor</span><span class="w"> </span><span class="nf">my_kernel</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">input</span><span class="p">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">size</span><span class="p">});</span>
<span class="p">}</span>

<span class="n">STABLE_TORCH_LIBRARY_IMPL</span><span class="p">(</span><span class="n">my_namespace</span><span class="p">,</span><span class="w"> </span><span class="n">CPU</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">m</span><span class="p">.</span><span class="n">impl</span><span class="p">(</span><span class="s">&quot;my_op&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">TORCH_BOX</span><span class="p">(</span><span class="o">&amp;</span><span class="n">my_kernel</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Minimum compatible version: PyTorch 2.9.</p>
</section>
</section>
<section id="tensor-class">
<h2>Tensor Class<a class="headerlink" href="#tensor-class" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">torch::stable::Tensor</span></code> class offers a user-friendly C++ interface similar
to <code class="docutils literal notranslate"><span class="pre">torch::Tensor</span></code> while maintaining binary compatibility across PyTorch versions.</p>
<dl class="cpp class">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6TensorE">
<span id="_CPPv3N5torch6stable6TensorE"></span><span id="_CPPv2N5torch6stable6TensorE"></span><span id="torch::stable::Tensor"></span><span class="target" id="classtorch_1_1stable_1_1_tensor"></span><span class="k"><span class="pre">class</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#_CPPv4N5torch6stable6TensorE" title="Link to this definition">#</a><br /></dt>
<dd><p>An ABI stable wrapper around PyTorch tensors. </p>
<p>This class is modeled after TensorBase, as custom op kernels primarily need to interact with <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">Tensor</span></a> metadata (sizes, strides, device, dtype). Other tensor operations (like <code class="docutils literal notranslate"><span class="pre">empty_like</span></code>) exist as standalone functions outside of this struct.</p>
<p>Minimum compatible version: PyTorch 2.9. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric" id="breathe-section-title-public-functions">Public Functions</p>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6Tensor6TensorEv">
<span id="_CPPv3N5torch6stable6Tensor6TensorEv"></span><span id="_CPPv2N5torch6stable6Tensor6TensorEv"></span><span id="torch::stable::Tensor::Tensor"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1abea19a0f51d767e36bffe6276c1d6f56"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">Tensor</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6Tensor6TensorEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Constructs a <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">Tensor</span></a> with an uninitialized AtenTensorHandle. </p>
<p>Creates a new <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">stable::Tensor</span></a> by allocating an uninitialized tensor handle. The ownership of the handle is managed internally via shared_ptr.</p>
<p>Minimum compatible version: PyTorch 2.9. </p>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6Tensor6TensorE16AtenTensorHandle">
<span id="_CPPv3N5torch6stable6Tensor6TensorE16AtenTensorHandle"></span><span id="_CPPv2N5torch6stable6Tensor6TensorE16AtenTensorHandle"></span><span id="torch::stable::Tensor::Tensor__AtenTensorHandle"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a4c7b1b857acb1d66e27a76d65e8eef70"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="k"><span class="pre">explicit</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">Tensor</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">AtenTensorHandle</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">ath</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6Tensor6TensorE16AtenTensorHandle" title="Link to this definition">#</a><br /></dt>
<dd><p>Constructs a <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">Tensor</span></a> from an existing AtenTensorHandle. </p>
<p>Steals ownership of the provided AtenTensorHandle.</p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>ath</strong> – The AtenTensorHandle to wrap. Ownership is transferred to this <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">Tensor</span></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor3getEv">
<span id="_CPPv3NK5torch6stable6Tensor3getEv"></span><span id="_CPPv2NK5torch6stable6Tensor3getEv"></span><span id="torch::stable::Tensor::getC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a2e46264795d00da94cdc166cbd3b55c1"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">AtenTensorHandle</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">get</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor3getEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a borrowed reference to the underlying AtenTensorHandle. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The underlying AtenTensorHandle.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor8data_ptrEv">
<span id="_CPPv3NK5torch6stable6Tensor8data_ptrEv"></span><span id="_CPPv2NK5torch6stable6Tensor8data_ptrEv"></span><span id="torch::stable::Tensor::data_ptrC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a2fbfe3ca10bbe089a08ff717752c1de4"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">data_ptr</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor8data_ptrEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a pointer to the tensor’s data. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A void pointer to the tensor’s data storage.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor16mutable_data_ptrEv">
<span id="_CPPv3NK5torch6stable6Tensor16mutable_data_ptrEv"></span><span id="_CPPv2NK5torch6stable6Tensor16mutable_data_ptrEv"></span><span id="torch::stable::Tensor::mutable_data_ptrC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1ac80d36fcfab8231145799e9b13e80cd0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">mutable_data_ptr</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor16mutable_data_ptrEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a mutable pointer to the tensor’s data. </p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A mutable void pointer to the tensor’s data storage.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor14const_data_ptrEv">
<span id="_CPPv3NK5torch6stable6Tensor14const_data_ptrEv"></span><span id="_CPPv2NK5torch6stable6Tensor14const_data_ptrEv"></span><span id="torch::stable::Tensor::const_data_ptrC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1ad9e299e80a5be881c18fe370186542b8"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">const_data_ptr</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor14const_data_ptrEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a const pointer to the tensor’s data. </p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A const void pointer to the tensor’s data storage.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4I0ENK5torch6stable6Tensor16mutable_data_ptrEP1Tv">
<span id="_CPPv3I0ENK5torch6stable6Tensor16mutable_data_ptrEv"></span><span id="_CPPv2I0ENK5torch6stable6Tensor16mutable_data_ptrEv"></span><span class="k"><span class="pre">template</span></span><span class="p"><span class="pre">&lt;</span></span><span class="k"><span class="pre">typename</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">T</span></span></span><span class="p"><span class="pre">&gt;</span></span><br /><span class="target" id="classtorch_1_1stable_1_1_tensor_1aae28bf35e9741529ae5e3e6742571959"></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4I0ENK5torch6stable6Tensor16mutable_data_ptrEP1Tv" title="torch::stable::Tensor::mutable_data_ptr::T"><span class="n"><span class="pre">T</span></span></a><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">mutable_data_ptr</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4I0ENK5torch6stable6Tensor16mutable_data_ptrEP1Tv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a typed mutable pointer to the tensor’s data. </p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Template Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>T</strong> – The type to cast the data pointer to. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A mutable pointer to the tensor’s data cast to type T*.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEPK1Tv">
<span id="_CPPv3I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEv"></span><span id="_CPPv2I0_NSt11enable_if_tIX!std::is_const_v<T>EiEEENK5torch6stable6Tensor14const_data_ptrEv"></span><span class="k"><span class="pre">template</span></span><span class="p"><span class="pre">&lt;</span></span><span class="k"><span class="pre">typename</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">T</span></span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">enable_if_t</span></span><span class="p"><span class="pre">&lt;</span></span><span class="o"><span class="pre">!</span></span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">is_const_v</span></span><span class="p"><span class="pre">&lt;</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEPK1Tv" title="torch::stable::Tensor::const_data_ptr::T"><span class="n"><span class="pre">T</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="kt"><span class="pre">int</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span><span class="p"><span class="pre">&gt;</span></span><br /><span class="target" id="classtorch_1_1stable_1_1_tensor_1abd502807ae069a245178aae99b443696"></span><span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEPK1Tv" title="torch::stable::Tensor::const_data_ptr::T"><span class="n"><span class="pre">T</span></span></a><span class="w"> </span><span class="p"><span class="pre">*</span></span><span class="sig-name descname"><span class="n"><span class="pre">const_data_ptr</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEPK1Tv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns a typed const pointer to the tensor’s data. </p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Template Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>T</strong> – The type to cast the data pointer to. Must not be const-qualified. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A const pointer to the tensor’s data cast to type const T*.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor17set_requires_gradEb">
<span id="_CPPv3NK5torch6stable6Tensor17set_requires_gradEb"></span><span id="_CPPv2NK5torch6stable6Tensor17set_requires_gradEb"></span><span id="torch::stable::Tensor::set_requires_grad__bC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a2adbdf6196be95e4045d3e9f9267ffed"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="sig-name descname"><span class="n"><span class="pre">set_requires_grad</span></span></span><span class="sig-paren">(</span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">requires_grad</span></span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor17set_requires_gradEb" title="Link to this definition">#</a><br /></dt>
<dd><p>Sets whether this tensor requires gradient computation. </p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>requires_grad</strong> – If true, gradients will be computed for this tensor during backpropagation. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A reference to this <a class="reference internal" href="#classtorch_1_1stable_1_1_tensor"><span class="std std-ref">Tensor</span></a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor3dimEv">
<span id="_CPPv3NK5torch6stable6Tensor3dimEv"></span><span id="_CPPv2NK5torch6stable6Tensor3dimEv"></span><span id="torch::stable::Tensor::dimC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a528b2cf0abb659f0002edefd9ed028ed"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">dim</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor3dimEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the number of dimensions of the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The number of dimensions (rank) of the tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor5numelEv">
<span id="_CPPv3NK5torch6stable6Tensor5numelEv"></span><span id="_CPPv2NK5torch6stable6Tensor5numelEv"></span><span id="torch::stable::Tensor::numelC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a386e18bd0956e70dfa509ecec9d24e7e"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">numel</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor5numelEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the total number of elements in the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The total number of elements across all dimensions.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor5sizesEv">
<span id="_CPPv3NK5torch6stable6Tensor5sizesEv"></span><span id="_CPPv2NK5torch6stable6Tensor5sizesEv"></span><span id="torch::stable::Tensor::sizesC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a31e8c451005fa441bd67d845a50ee9a6"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">sizes</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor5sizesEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the sizes (shape) of the tensor. </p>
<p>Returns a borrowed reference of the dimension sizes of the tensor.</p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An IntHeaderOnlyArrayRef containing the size of each dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor7stridesEv">
<span id="_CPPv3NK5torch6stable6Tensor7stridesEv"></span><span id="_CPPv2NK5torch6stable6Tensor7stridesEv"></span><span id="torch::stable::Tensor::stridesC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a43fb529a3d4de3a1ad17540110ef0970"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">strides</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor7stridesEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the strides of the tensor. </p>
<p>Returns a borrowed reference of the strides of the tensor.</p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>An IntHeaderOnlyArrayRef containing the stride of each dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor13is_contiguousEv">
<span id="_CPPv3NK5torch6stable6Tensor13is_contiguousEv"></span><span id="_CPPv2NK5torch6stable6Tensor13is_contiguousEv"></span><span id="torch::stable::Tensor::is_contiguousC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a246c05d03abaf31a7e4b62287405534b"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">is_contiguous</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor13is_contiguousEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if the tensor is contiguous in memory. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a subset of the original TensorBase API. It takes no arguments whereas the original API takes a memory format argument. Here, we assume the default contiguous memory format.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the tensor is contiguous, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor6strideE7int64_t">
<span id="_CPPv3NK5torch6stable6Tensor6strideE7int64_t"></span><span id="_CPPv2NK5torch6stable6Tensor6strideE7int64_t"></span><span id="torch::stable::Tensor::stride__int64_tC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a50ea88a6009ce29e569a07a98d8a5666"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">stride</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor6strideE7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the stride of a specific dimension. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> – The dimension index to query. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The stride of the specified dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor16get_device_indexEv">
<span id="_CPPv3NK5torch6stable6Tensor16get_device_indexEv"></span><span id="_CPPv2NK5torch6stable6Tensor16get_device_indexEv"></span><span id="torch::stable::Tensor::get_device_indexC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a86a36f956951ae83f57423bb113771b8"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1afcc34aab224ae0316a7319dbad82706c.html#_CPPv4N5torch6stable11DeviceIndexE" title="torch::stable::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">get_device_index</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor16get_device_indexEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the device index of the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The device index as DeviceIndex (int32_t).</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor7is_cudaEv">
<span id="_CPPv3NK5torch6stable6Tensor7is_cudaEv"></span><span id="_CPPv2NK5torch6stable6Tensor7is_cudaEv"></span><span id="torch::stable::Tensor::is_cudaC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1ae2813ea747f7b5ef86efe49292f375e4"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">is_cuda</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor7is_cudaEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if the tensor is on a CUDA device. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the tensor is on a CUDA device, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor6is_cpuEv">
<span id="_CPPv3NK5torch6stable6Tensor6is_cpuEv"></span><span id="_CPPv2NK5torch6stable6Tensor6is_cpuEv"></span><span id="torch::stable::Tensor::is_cpuC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a20250a09772d639e1c80df232dce8f3b"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">is_cpu</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor6is_cpuEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if the tensor is on the CPU. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the tensor is on the CPU, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor4sizeE7int64_t">
<span id="_CPPv3NK5torch6stable6Tensor4sizeE7int64_t"></span><span id="_CPPv2NK5torch6stable6Tensor4sizeE7int64_t"></span><span id="torch::stable::Tensor::size__int64_tC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1af16227b2016ec86afb61d90b0a146dee"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">size</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor4sizeE7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the size of a specific dimension. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>dim</strong> – The dimension index to query. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The size of the specified dimension.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor7definedEv">
<span id="_CPPv3NK5torch6stable6Tensor7definedEv"></span><span id="_CPPv2NK5torch6stable6Tensor7definedEv"></span><span id="torch::stable::Tensor::definedC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1abfbfeba0022d75675274473c79c96bb5"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">defined</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor7definedEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if the tensor is defined (not null). </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the tensor is defined, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor14storage_offsetEv">
<span id="_CPPv3NK5torch6stable6Tensor14storage_offsetEv"></span><span id="_CPPv2NK5torch6stable6Tensor14storage_offsetEv"></span><span id="torch::stable::Tensor::storage_offsetC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a3e5700aad664352ade1cc6021fe06aa5"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">storage_offset</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor14storage_offsetEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the storage offset of the tensor. </p>
<p>The storage offset is the number of elements from the beginning of the underlying storage to the first element of the tensor.</p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The storage offset in number of elements.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor12element_sizeEv">
<span id="_CPPv3NK5torch6stable6Tensor12element_sizeEv"></span><span id="_CPPv2NK5torch6stable6Tensor12element_sizeEv"></span><span id="torch::stable::Tensor::element_sizeC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1aa0f7b04c0e29d16f8554ecb3b6943e4e"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">size_t</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">element_size</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor12element_sizeEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the size in bytes of each element in the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The element size in bytes.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor11scalar_typeEv">
<span id="_CPPv3NK5torch6stable6Tensor11scalar_typeEv"></span><span id="_CPPv2NK5torch6stable6Tensor11scalar_typeEv"></span><span id="torch::stable::Tensor::scalar_typeC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a06bca7ea223b8465d9c2df96c7268375"></span><span class="n"><span class="pre">ScalarType</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">scalar_type</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor11scalar_typeEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the scalar type (dtype) of the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The ScalarType of the tensor.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor6deviceEv">
<span id="_CPPv3NK5torch6stable6Tensor6deviceEv"></span><span id="_CPPv2NK5torch6stable6Tensor6deviceEv"></span><span id="torch::stable::Tensor::deviceC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1a6cd897d95672be9feefd8f8de89997d6"></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">device</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor6deviceEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the device of the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The <a class="reference internal" href="#classtorch_1_1stable_1_1_device"><span class="std std-ref">Device</span></a> on which the tensor resides.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Tensor6layoutEv">
<span id="_CPPv3NK5torch6stable6Tensor6layoutEv"></span><span id="_CPPv2NK5torch6stable6Tensor6layoutEv"></span><span id="torch::stable::Tensor::layoutC"></span><span class="target" id="classtorch_1_1stable_1_1_tensor_1aae614f58df248135688a6e0000ea5ee6"></span><span class="n"><span class="pre">Layout</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">layout</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Tensor6layoutEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the layout of the tensor. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The Layout of the tensor (e.g., Strided, Sparse).</p>
</dd>
</dl>
</dd></dl>

</div>
</dd></dl>

</section>
<section id="device-class">
<h2>Device Class<a class="headerlink" href="#device-class" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">torch::stable::Device</span></code> class provides a user-friendly C++ interface similar
to <code class="docutils literal notranslate"><span class="pre">c10::Device</span></code> while maintaining binary compatibility across PyTorch versions.
It represents a compute device (CPU, CUDA, etc.) with an optional device index.</p>
<dl class="cpp class">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6DeviceE">
<span id="_CPPv3N5torch6stable6DeviceE"></span><span id="_CPPv2N5torch6stable6DeviceE"></span><span id="torch::stable::Device"></span><span class="target" id="classtorch_1_1stable_1_1_device"></span><span class="k"><span class="pre">class</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">Device</span></span></span><a class="headerlink" href="#_CPPv4N5torch6stable6DeviceE" title="Link to this definition">#</a><br /></dt>
<dd><p>A stable version of <a class="reference internal" href="api/structc10_1_1_device.html#structc10_1_1_device"><span class="std std-ref">c10::Device</span></a>. </p>
<p>Minimum compatible version: PyTorch 2.9. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric" id="breathe-section-title-public-functions">Public Functions</p>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6Device6DeviceE10DeviceType11DeviceIndex">
<span id="_CPPv3N5torch6stable6Device6DeviceE10DeviceType11DeviceIndex"></span><span id="_CPPv2N5torch6stable6Device6DeviceE10DeviceType11DeviceIndex"></span><span id="torch::stable::Device::Device__DeviceType.DeviceIndex"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a1c2448020cab2e5266a5311462a1ca19"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">Device</span></span></span><span class="sig-paren">(</span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1ae2fd2c3bc538f8cb9ca90a0821905b83.html#_CPPv4N5torch6stable10DeviceTypeE" title="torch::stable::DeviceType"><span class="n"><span class="pre">DeviceType</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">type</span></span>, <a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1afcc34aab224ae0316a7319dbad82706c.html#_CPPv4N5torch6stable11DeviceIndexE" title="torch::stable::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">index</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="o"><span class="pre">-</span></span><span class="m"><span class="pre">1</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6Device6DeviceE10DeviceType11DeviceIndex" title="Link to this definition">#</a><br /></dt>
<dd><p>Constructs a <a class="reference internal" href="#classtorch_1_1stable_1_1_device"><span class="std std-ref">Device</span></a> from a DeviceType and optional device index. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>type</strong> – The type of device (e.g., DeviceType::CPU, DeviceType::CUDA). </p></li>
<li><p><strong>index</strong> – The device index. Default is -1 (current device).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6Device6DeviceERKNSt6stringE">
<span id="_CPPv3N5torch6stable6Device6DeviceERKNSt6stringE"></span><span id="_CPPv2N5torch6stable6Device6DeviceERKNSt6stringE"></span><span id="torch::stable::Device::Device__ssCR"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a3f3e19cab8581071af3638b26926d6b0"></span><span class="sig-name descname"><span class="n"><span class="pre">Device</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">device_string</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6Device6DeviceERKNSt6stringE" title="Link to this definition">#</a><br /></dt>
<dd><p>Constructs a <a class="reference internal" href="#classtorch_1_1stable_1_1_device"><span class="std std-ref">stable::Device</span></a> from a string description. </p>
<p>The string must follow the schema: (cpu|cuda|…)[:&lt;device-index&gt;]</p>
<p>
Minimum compatible version: PyTorch 2.10. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device_string</strong> – A string describing the device (e.g., “cuda:0”, “cpu”).</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6DeviceeqERK6Device">
<span id="_CPPv3NK5torch6stable6DeviceeqERK6Device"></span><span id="_CPPv2NK5torch6stable6DeviceeqERK6Device"></span><span id="torch::stable::Device::eq-operator__DeviceCRC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1ac51e595f961ecb22d0e3bbc76e6d5da4"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="k"><span class="pre">operator</span></span><span class="o"><span class="pre">==</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">other</span></span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6DeviceeqERK6Device" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if two devices are equal. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> – The device to compare with. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>true if both type and index match, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6DeviceneERK6Device">
<span id="_CPPv3NK5torch6stable6DeviceneERK6Device"></span><span id="_CPPv2NK5torch6stable6DeviceneERK6Device"></span><span id="torch::stable::Device::neq-operator__DeviceCRC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a29b34ef030ad37c13f3a0258d2e42a2d"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="k"><span class="pre">operator</span></span><span class="o"><span class="pre">!=</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">other</span></span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6DeviceneERK6Device" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if two devices are not equal. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>other</strong> – The device to compare with. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>true if type or index differ, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6Device9set_indexE11DeviceIndex">
<span id="_CPPv3N5torch6stable6Device9set_indexE11DeviceIndex"></span><span id="_CPPv2N5torch6stable6Device9set_indexE11DeviceIndex"></span><span id="torch::stable::Device::set_index__DeviceIndex"></span><span class="target" id="classtorch_1_1stable_1_1_device_1ad98d6f32b8336f553f6f477c85116b82"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">set_index</span></span></span><span class="sig-paren">(</span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1afcc34aab224ae0316a7319dbad82706c.html#_CPPv4N5torch6stable11DeviceIndexE" title="torch::stable::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">index</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6Device9set_indexE11DeviceIndex" title="Link to this definition">#</a><br /></dt>
<dd><p>Sets the device index. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>index</strong> – The new device index.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Device4typeEv">
<span id="_CPPv3NK5torch6stable6Device4typeEv"></span><span id="_CPPv2NK5torch6stable6Device4typeEv"></span><span id="torch::stable::Device::typeC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1aa617b6a6b4a2add3c0bc65e415d668e5"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1ae2fd2c3bc538f8cb9ca90a0821905b83.html#_CPPv4N5torch6stable10DeviceTypeE" title="torch::stable::DeviceType"><span class="n"><span class="pre">DeviceType</span></span></a><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">type</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Device4typeEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the device type. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The DeviceType of this device.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Device5indexEv">
<span id="_CPPv3NK5torch6stable6Device5indexEv"></span><span id="_CPPv2NK5torch6stable6Device5indexEv"></span><span id="torch::stable::Device::indexC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a77a9d6889d89f1754820988189ae279e"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1afcc34aab224ae0316a7319dbad82706c.html#_CPPv4N5torch6stable11DeviceIndexE" title="torch::stable::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">index</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Device5indexEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Returns the device index. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The device index, or -1 if no specific index is set.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Device9has_indexEv">
<span id="_CPPv3NK5torch6stable6Device9has_indexEv"></span><span id="_CPPv2NK5torch6stable6Device9has_indexEv"></span><span id="torch::stable::Device::has_indexC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a67e6ac667e71846ca0f0dbd2ce6a01f0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">has_index</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Device9has_indexEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if this device has a specific index. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if index is not -1, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Device7is_cudaEv">
<span id="_CPPv3NK5torch6stable6Device7is_cudaEv"></span><span id="_CPPv2NK5torch6stable6Device7is_cudaEv"></span><span id="torch::stable::Device::is_cudaC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a275d9fedf56bef24bbdab8b02481bb60"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">is_cuda</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Device7is_cudaEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if this is a CUDA device. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the device type is CUDA, false otherwise.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4NK5torch6stable6Device6is_cpuEv">
<span id="_CPPv3NK5torch6stable6Device6is_cpuEv"></span><span id="_CPPv2NK5torch6stable6Device6is_cpuEv"></span><span id="torch::stable::Device::is_cpuC"></span><span class="target" id="classtorch_1_1stable_1_1_device_1a8ffaac34dd586b5a6db5d867e04ad96a"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">is_cpu</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><span class="w"> </span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="k"><span class="pre">noexcept</span></span><a class="headerlink" href="#_CPPv4NK5torch6stable6Device6is_cpuEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Checks if this is a CPU device. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>true if the device type is CPU, false otherwise.</p>
</dd>
</dl>
</dd></dl>

</div>
</dd></dl>

</section>
<section id="deviceguard-class">
<h2>DeviceGuard Class<a class="headerlink" href="#deviceguard-class" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">torch::stable::accelerator::DeviceGuard</span></code> provides a user-friendly C++
interface similar to <code class="docutils literal notranslate"><span class="pre">c10::DeviceGuard</span></code> while maintaining binary compatibility
across PyTorch versions.</p>
<dl class="cpp class">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable11accelerator11DeviceGuardE">
<span id="_CPPv3N5torch6stable11accelerator11DeviceGuardE"></span><span id="_CPPv2N5torch6stable11accelerator11DeviceGuardE"></span><span id="torch::stable::accelerator::DeviceGuard"></span><span class="target" id="classtorch_1_1stable_1_1accelerator_1_1_device_guard"></span><span class="k"><span class="pre">class</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">DeviceGuard</span></span></span><a class="headerlink" href="#_CPPv4N5torch6stable11accelerator11DeviceGuardE" title="Link to this definition">#</a><br /></dt>
<dd><p>A stable ABI version of c10::DeviceGuard. </p>
<p>RAII class that sets the current device to the specified device index on construction and restores the previous device on destruction.</p>
<p>Minimum compatible version: PyTorch 2.9. </p>
<div class="breathe-sectiondef docutils container">
<p class="breathe-sectiondef-title rubric" id="breathe-section-title-public-functions">Public Functions</p>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable11accelerator11DeviceGuard11DeviceGuardE11DeviceIndex">
<span id="_CPPv3N5torch6stable11accelerator11DeviceGuard11DeviceGuardE11DeviceIndex"></span><span id="_CPPv2N5torch6stable11accelerator11DeviceGuard11DeviceGuardE11DeviceIndex"></span><span id="torch::stable::accelerator::DeviceGuard::DeviceGuard__DeviceIndex"></span><span class="target" id="classtorch_1_1stable_1_1accelerator_1_1_device_guard_1ae5bfc05fcca25da723ed9128e270a317"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="k"><span class="pre">explicit</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">DeviceGuard</span></span></span><span class="sig-paren">(</span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1_1accelerator_1ac4fa2c7bf4d89c0ee4a01e53b72ac610.html#_CPPv4N5torch6stable11accelerator11DeviceIndexE" title="torch::stable::accelerator::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">device_index</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable11accelerator11DeviceGuard11DeviceGuardE11DeviceIndex" title="Link to this definition">#</a><br /></dt>
<dd><p>Constructs a <a class="reference internal" href="#classtorch_1_1stable_1_1accelerator_1_1_device_guard"><span class="std std-ref">DeviceGuard</span></a> that sets the current device. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device_index</strong> – The device index to set as the current device.</p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable11accelerator11DeviceGuard9set_indexE11DeviceIndex">
<span id="_CPPv3N5torch6stable11accelerator11DeviceGuard9set_indexE11DeviceIndex"></span><span id="_CPPv2N5torch6stable11accelerator11DeviceGuard9set_indexE11DeviceIndex"></span><span id="torch::stable::accelerator::DeviceGuard::set_index__DeviceIndex"></span><span class="target" id="classtorch_1_1stable_1_1accelerator_1_1_device_guard_1a8e4cdd7a2cb9ceea607050992b065230"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">set_index</span></span></span><span class="sig-paren">(</span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1_1accelerator_1ac4fa2c7bf4d89c0ee4a01e53b72ac610.html#_CPPv4N5torch6stable11accelerator11DeviceIndexE" title="torch::stable::accelerator::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">device_index</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable11accelerator11DeviceGuard9set_indexE11DeviceIndex" title="Link to this definition">#</a><br /></dt>
<dd><p>Changes the current device to the specified device index. </p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device_index</strong> – The new device index to set.</p>
</dd>
</dl>
</dd></dl>

</div>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable11accelerator21getCurrentDeviceIndexEv">
<span id="_CPPv3N5torch6stable11accelerator21getCurrentDeviceIndexEv"></span><span id="_CPPv2N5torch6stable11accelerator21getCurrentDeviceIndexEv"></span><span id="torch::stable::accelerator::getCurrentDeviceIndex"></span><span class="target" id="namespacetorch_1_1stable_1_1accelerator_1a6a19b92173bf170b6811494331cd9938"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><a class="reference internal" href="api/typedef_namespacetorch_1_1stable_1_1accelerator_1ac4fa2c7bf4d89c0ee4a01e53b72ac610.html#_CPPv4N5torch6stable11accelerator11DeviceIndexE" title="torch::stable::accelerator::DeviceIndex"><span class="n"><span class="pre">DeviceIndex</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">accelerator</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">getCurrentDeviceIndex</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable11accelerator21getCurrentDeviceIndexEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Gets the current device index. </p>
<p>Returns the index of the currently active device for the accelerator.</p>
<p>
Minimum compatible version: PyTorch 2.9. </p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The current device index.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="stream-utilities">
<h2>Stream Utilities<a class="headerlink" href="#stream-utilities" title="Link to this heading">#</a></h2>
<p>For CUDA stream access, we currently recommend the ABI stable C shim API. This
will be improved in a future release with a more ergonomic wrapper.</p>
<section id="getting-the-current-cuda-stream">
<h3>Getting the Current CUDA Stream<a class="headerlink" href="#getting-the-current-cuda-stream" title="Link to this heading">#</a></h3>
<p>To obtain the current <code class="docutils literal notranslate"><span class="pre">cudaStream_t</span></code> for use in CUDA kernels:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/inductor/aoti_torch/c/shim.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/util/shim_utils.h&gt;</span>

<span class="c1">// For now, we rely on the ABI stable C shim API to get the current CUDA stream.</span>
<span class="c1">// This will be improved in a future release.</span>
<span class="c1">// When using a C shim API, we need to use TORCH_ERROR_CODE_CHECK to</span>
<span class="c1">// check the error code and throw an appropriate runtime_error otherwise.</span>
<span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">stream_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span>
<span class="n">TORCH_ERROR_CODE_CHECK</span><span class="p">(</span>
<span class="w">    </span><span class="n">aoti_torch_get_current_cuda_stream</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">get_device_index</span><span class="p">(),</span><span class="w"> </span><span class="o">&amp;</span><span class="n">stream_ptr</span><span class="p">));</span>
<span class="n">cudaStream_t</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">cudaStream_t</span><span class="o">&gt;</span><span class="p">(</span><span class="n">stream_ptr</span><span class="p">);</span>

<span class="c1">// Now you can use &#39;stream&#39; in your CUDA kernel launches</span>
<span class="n">my_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">...);</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TORCH_ERROR_CODE_CHECK</span></code> macro is required when using C shim APIs
to properly check error codes and throw appropriate exceptions.</p>
</div>
</section>
</section>
<section id="cuda-error-checking-macros">
<h2>CUDA Error Checking Macros<a class="headerlink" href="#cuda-error-checking-macros" title="Link to this heading">#</a></h2>
<p>These macros provide stable ABI equivalents for CUDA error checking.
They wrap CUDA API calls and kernel launches, providing detailed error
messages using PyTorch’s error formatting.</p>
<section id="std-cuda-check-expr">
<h3><code class="docutils literal notranslate"><span class="pre">STD_CUDA_CHECK(EXPR)</span></code><a class="headerlink" href="#std-cuda-check-expr" title="Link to this heading">#</a></h3>
<p>Checks the result of a CUDA API call and throws an exception on error.
Users of this macro are expected to include <code class="docutils literal notranslate"><span class="pre">cuda_runtime.h</span></code>.</p>
<p><strong>Example:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">STD_CUDA_CHECK</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptr</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">));</span>
<span class="n">STD_CUDA_CHECK</span><span class="p">(</span><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">));</span>
</pre></div>
</div>
<p>Minimum compatible version: PyTorch 2.10.</p>
</section>
<section id="std-cuda-kernel-launch-check">
<h3><code class="docutils literal notranslate"><span class="pre">STD_CUDA_KERNEL_LAUNCH_CHECK()</span></code><a class="headerlink" href="#std-cuda-kernel-launch-check" title="Link to this heading">#</a></h3>
<p>Checks for errors from the most recent CUDA kernel launch. Equivalent to
<code class="docutils literal notranslate"><span class="pre">STD_CUDA_CHECK(cudaGetLastError())</span></code>.</p>
<p><strong>Example:</strong></p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">my_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocks</span><span class="p">,</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">...);</span>
<span class="n">STD_CUDA_KERNEL_LAUNCH_CHECK</span><span class="p">();</span>
</pre></div>
</div>
<p>Minimum compatible version: PyTorch 2.10.</p>
</section>
</section>
<section id="header-only-utilities">
<h2>Header-Only Utilities<a class="headerlink" href="#header-only-utilities" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">torch::headeronly</span></code> namespace provides header-only versions of common
PyTorch types and utilities. These can be used without linking against libtorch,
making them ideal for maintaining binary compatibility across PyTorch versions.</p>
<section id="error-checking">
<h3>Error Checking<a class="headerlink" href="#error-checking" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">STD_TORCH_CHECK</span></code> is a header-only macro for runtime assertions:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/util/Exception.h&gt;</span>

<span class="n">STD_TORCH_CHECK</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Error message with &quot;</span><span class="p">,</span><span class="w"> </span><span class="n">variable</span><span class="p">,</span><span class="w"> </span><span class="s">&quot; interpolation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="core-types">
<h3>Core Types<a class="headerlink" href="#core-types" title="Link to this heading">#</a></h3>
<p>The following <code class="docutils literal notranslate"><span class="pre">c10::</span></code> types are available as header-only versions under
<code class="docutils literal notranslate"><span class="pre">torch::headeronly::</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch::headeronly::ScalarType</span></code> - Tensor data types (Float, Double, Int, etc.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch::headeronly::DeviceType</span></code> - Device types (CPU, CUDA, etc.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch::headeronly::MemoryFormat</span></code> - Memory layout formats (Contiguous, ChannelsLast, etc.)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch::headeronly::Layout</span></code> - Tensor layouts (Strided, Sparse, etc.)</p></li>
</ul>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/ScalarType.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/DeviceType.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/MemoryFormat.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/Layout.h&gt;</span>

<span class="k">auto</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Float</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">device_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">memory_format</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">;</span>
<span class="k">auto</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">Layout</span><span class="o">::</span><span class="n">Strided</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="tensoraccessor">
<h3>TensorAccessor<a class="headerlink" href="#tensoraccessor" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">TensorAccessor</span></code> provides efficient, bounds-checked access to tensor data.
You can construct one from a stable tensor’s data pointer, sizes, and strides:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/TensorAccessor.h&gt;</span>

<span class="c1">// Create a TensorAccessor for a 2D float tensor</span>
<span class="k">auto</span><span class="w"> </span><span class="n">sizes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">sizes</span><span class="p">();</span>
<span class="k">auto</span><span class="w"> </span><span class="n">strides</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">strides</span><span class="p">();</span>
<span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">TensorAccessor</span><span class="o">&lt;</span><span class="kt">float</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">accessor</span><span class="p">(</span>
<span class="w">    </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">mutable_data_ptr</span><span class="p">()),</span>
<span class="w">    </span><span class="n">sizes</span><span class="p">.</span><span class="n">data</span><span class="p">(),</span>
<span class="w">    </span><span class="n">strides</span><span class="p">.</span><span class="n">data</span><span class="p">());</span>

<span class="c1">// Access elements</span>
<span class="kt">float</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">accessor</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
</pre></div>
</div>
</section>
<section id="dispatch-macros">
<h3>Dispatch Macros<a class="headerlink" href="#dispatch-macros" title="Link to this heading">#</a></h3>
<p>Header-only dispatch macros (THO = Torch Header Only) are available for
dtype and device dispatching:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/headeronly/core/Dispatch.h&gt;</span>

<span class="n">THO_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;my_kernel&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// scalar_t is the resolved type</span>
<span class="w">    </span><span class="k">auto</span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tensor</span><span class="p">.</span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">();</span>
<span class="p">});</span>
</pre></div>
</div>
</section>
<section id="full-api-list">
<h3>Full API List<a class="headerlink" href="#full-api-list" title="Link to this heading">#</a></h3>
<p>For the complete list of header-only APIs, see <code class="docutils literal notranslate"><span class="pre">torch/header_only_apis.txt</span></code>
in the PyTorch source tree.</p>
</section>
</section>
<section id="stable-operators">
<h2>Stable Operators<a class="headerlink" href="#stable-operators" title="Link to this heading">#</a></h2>
<section id="tensor-creation">
<h3>Tensor Creation<a class="headerlink" href="#tensor-creation" title="Link to this heading">#</a></h3>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable5emptyEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEENSt8optionalIN5torch10headeronly12MemoryFormatEEE">
<span id="_CPPv3N5torch6stable5emptyEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEENSt8optionalIN5torch10headeronly12MemoryFormatEEE"></span><span id="_CPPv2N5torch6stable5emptyEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEENSt8optionalIN5torch10headeronly12MemoryFormatEEE"></span><span id="torch::stable::empty__torch::headeronly::IntHeaderOnlyArrayRef.std::optional:torch::headeronly::ScalarType:.std::optional:torch::headeronly::Layout:.std::optional:torch::stable::Device:.std::optional:b:.std::optional:torch::headeronly::MemoryFormat:"></span><span class="target" id="namespacetorch_1_1stable_1a4630f53cacac471d5a37d0e110640835"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">empty</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">size</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Layout</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">layout</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pin_memory</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">MemoryFormat</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">memory_format</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable5emptyEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEENSt8optionalIN5torch10headeronly12MemoryFormatEEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the empty.memory_format op. </p>
<p>Creates a new uninitialized tensor with the specified size and options. This function supports full tensor creation options including device, dtype, layout, and memory format.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> – The desired size of the output tensor. </p></li>
<li><p><strong>dtype</strong> – Optional scalar type for the tensor elements. </p></li>
<li><p><strong>layout</strong> – Optional memory layout (e.g., strided, sparse). </p></li>
<li><p><strong>device</strong> – Optional device to place the tensor on. </p></li>
<li><p><strong>pin_memory</strong> – Optional flag to use pinned memory (for CUDA tensors). </p></li>
<li><p><strong>memory_format</strong> – Optional memory format for the tensor. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new uninitialized tensor with the specified properties. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable10empty_likeERKN5torch6stable6TensorE">
<span id="_CPPv3N5torch6stable10empty_likeERKN5torch6stable6TensorE"></span><span id="_CPPv2N5torch6stable10empty_likeERKN5torch6stable6TensorE"></span><span id="torch::stable::empty_like__torch::stable::TensorCR"></span><span class="target" id="namespacetorch_1_1stable_1af8d64ca3c9d8b299b74d30015e9643e0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">empty_like</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable10empty_likeERKN5torch6stable6TensorE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the empty_like op. </p>
<p>Creates a new uninitialized tensor with the same size, dtype, layout, and device as the input tensor. This version does not support kwargs (device, dtype, layout, memory_format) - kwargs support may be added in the future.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self</strong> – The input tensor whose properties will be used for the new tensor. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new uninitialized tensor with the same properties as self. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable9new_emptyERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE">
<span id="_CPPv3N5torch6stable9new_emptyERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="_CPPv2N5torch6stable9new_emptyERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="torch::stable::new_empty__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef.std::optional:torch::headeronly::ScalarType:.std::optional:torch::headeronly::Layout:.std::optional:torch::stable::Device:.std::optional:b:"></span><span class="target" id="namespacetorch_1_1stable_1ab5669e5cbd9d9b7b7011d2f921910b8b"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">new_empty</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">size</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Layout</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">layout</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pin_memory</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable9new_emptyERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the new_empty op (2.10 version with full kwargs). </p>
<p>Creates a new uninitialized tensor with the specified size and options. This version supports all tensor creation kwargs. For versions &lt; 2.10, a simpler overload that only takes dtype is available.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor whose properties may be inherited if kwargs are not provided. </p></li>
<li><p><strong>size</strong> – The desired size of the output tensor. </p></li>
<li><p><strong>dtype</strong> – Optional scalar type for the tensor elements. </p></li>
<li><p><strong>layout</strong> – Optional memory layout (e.g., strided, sparse). </p></li>
<li><p><strong>device</strong> – Optional device to place the tensor on. </p></li>
<li><p><strong>pin_memory</strong> – Optional flag to use pinned memory (for CUDA tensors). </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new uninitialized tensor with the specified properties. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable9new_zerosERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE">
<span id="_CPPv3N5torch6stable9new_zerosERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="_CPPv2N5torch6stable9new_zerosERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="torch::stable::new_zeros__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef.std::optional:torch::headeronly::ScalarType:.std::optional:torch::headeronly::Layout:.std::optional:torch::stable::Device:.std::optional:b:"></span><span class="target" id="namespacetorch_1_1stable_1aa42d4a50487cf5ff9171695ad431e401"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">new_zeros</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">size</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Layout</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">layout</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pin_memory</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable9new_zerosERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the new_zeros op (2.10 version with full kwargs). </p>
<p>Creates a new zero-filled tensor with the specified size and options. This version supports all tensor creation kwargs. For versions &lt; 2.10, a simpler overload that only takes dtype is available.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor whose properties may be inherited if kwargs are not provided. </p></li>
<li><p><strong>size</strong> – The desired size of the output tensor. </p></li>
<li><p><strong>dtype</strong> – Optional scalar type for the tensor elements. </p></li>
<li><p><strong>layout</strong> – Optional memory layout (e.g., strided, sparse). </p></li>
<li><p><strong>device</strong> – Optional device to place the tensor on. </p></li>
<li><p><strong>pin_memory</strong> – Optional flag to use pinned memory (for CUDA tensors). </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new zero-filled tensor with the specified properties. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable4fullEN5torch10headeronly21IntHeaderOnlyArrayRefEdNSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE">
<span id="_CPPv3N5torch6stable4fullEN5torch10headeronly21IntHeaderOnlyArrayRefEdNSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="_CPPv2N5torch6stable4fullEN5torch10headeronly21IntHeaderOnlyArrayRefEdNSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"></span><span id="torch::stable::full__torch::headeronly::IntHeaderOnlyArrayRef.double.std::optional:torch::headeronly::ScalarType:.std::optional:torch::headeronly::Layout:.std::optional:torch::stable::Device:.std::optional:b:"></span><span class="target" id="namespacetorch_1_1stable_1a99327b3bbaee1b9b4d7e84f00b0d3dd1"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">full</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">size</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">fill_value</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Layout</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">layout</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pin_memory</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable4fullEN5torch10headeronly21IntHeaderOnlyArrayRefEdNSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the full.default op. </p>
<p>Creates a tensor of the specified size filled with the given value.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The fill_value parameter is typed C shim API uses double for the Scalar parameter.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> – The desired size of the output tensor. </p></li>
<li><p><strong>fill_value</strong> – The value to fill the tensor with. </p></li>
<li><p><strong>dtype</strong> – Optional scalar type for the tensor elements. </p></li>
<li><p><strong>layout</strong> – Optional memory layout. </p></li>
<li><p><strong>device</strong> – Optional device to place the tensor on. </p></li>
<li><p><strong>pin_memory</strong> – Optional flag to use pinned memory. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new tensor filled with the specified value. </p>
</dd>
</dl>
</dd></dl>

<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>doxygenfunction: Unable to resolve function “torch::stable::from_blob” with arguments None in doxygen xml output for project “PyTorch” from directory: /var/lib/jenkins/workspace/docs/cpp/build/xml.
Potential matches:
</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="o">-</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">from_blob</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">IntHeaderOnlyArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">IntHeaderOnlyArrayRef</span><span class="w"> </span><span class="n">strides</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">DeleterFnPtr</span><span class="w"> </span><span class="n">deleter</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">Layout</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">Layout</span><span class="o">::</span><span class="n">Strided</span><span class="p">)</span>
<span class="o">-</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">from_blob</span><span class="p">(</span><span class="kt">void</span><span class="w"> </span><span class="o">*</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">IntHeaderOnlyArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">IntHeaderOnlyArrayRef</span><span class="w"> </span><span class="n">strides</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">stable</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">Layout</span><span class="w"> </span><span class="n">layout</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">headeronly</span><span class="o">::</span><span class="n">Layout</span><span class="o">::</span><span class="n">Strided</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="tensor-manipulation">
<h3>Tensor Manipulation<a class="headerlink" href="#tensor-manipulation" title="Link to this heading">#</a></h3>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable5cloneERKN5torch6stable6TensorE">
<span id="_CPPv3N5torch6stable5cloneERKN5torch6stable6TensorE"></span><span id="_CPPv2N5torch6stable5cloneERKN5torch6stable6TensorE"></span><span id="torch::stable::clone__torch::stable::TensorCR"></span><span class="target" id="namespacetorch_1_1stable_1a7beb7c687293b8ff872a6eb3575c02d0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">clone</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable5cloneERKN5torch6stable6TensorE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the clone op. </p>
<p>Returns a copy of the input tensor. The returned tensor has the same data and type as the input, but is stored in a new memory location.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Optional memory_format kwarg support</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self</strong> – The input tensor to clone. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new tensor with copied data. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable10contiguousERKN5torch6stable6TensorEN5torch10headeronly12MemoryFormatE">
<span id="_CPPv3N5torch6stable10contiguousERKN5torch6stable6TensorEN5torch10headeronly12MemoryFormatE"></span><span id="_CPPv2N5torch6stable10contiguousERKN5torch6stable6TensorEN5torch10headeronly12MemoryFormatE"></span><span id="torch::stable::contiguous__torch::stable::TensorCR.torch::headeronly::MemoryFormat"></span><span class="target" id="namespacetorch_1_1stable_1abcbb8c6ad3beab6039d78671dd6cd2f0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">contiguous</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">MemoryFormat</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">memory_format</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">MemoryFormat</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Contiguous</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable10contiguousERKN5torch6stable6TensorEN5torch10headeronly12MemoryFormatE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the contiguous op. </p>
<p>Returns a contiguous in memory tensor containing the same data as the input tensor. If the input tensor is already contiguous in the specified memory format, the input tensor is returned.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>memory_format</strong> – The desired memory format. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A contiguous tensor. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable7reshapeERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE">
<span id="_CPPv3N5torch6stable7reshapeERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"></span><span id="_CPPv2N5torch6stable7reshapeERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"></span><span id="torch::stable::reshape__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef"></span><span class="target" id="namespacetorch_1_1stable_1a980a297706e77257e532926b4e3a1125"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">reshape</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">shape</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable7reshapeERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the reshape op. </p>
<p>Returns a tensor with the same data and number of elements as the input, but with the specified shape. When possible, the returned tensor will be a view of the input.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>shape</strong> – The desired output shape. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor with the specified shape. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable4viewERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE">
<span id="_CPPv3N5torch6stable4viewERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"></span><span id="_CPPv2N5torch6stable4viewERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"></span><span id="torch::stable::view__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef"></span><span class="target" id="namespacetorch_1_1stable_1a0a71bf9952cdfb0e2d498c808c69cd02"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">view</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">size</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable4viewERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the view op. </p>
<p>Returns a new tensor with the same data as the input tensor but with a different shape. The returned tensor shares the same data and must have the same number of elements.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>size</strong> – The desired output shape. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A view tensor with the specified shape. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable7flattenERKN5torch6stable6TensorE7int64_t7int64_t">
<span id="_CPPv3N5torch6stable7flattenERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="_CPPv2N5torch6stable7flattenERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="torch::stable::flatten__torch::stable::TensorCR.int64_t.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1ada3da8b1688c6c985bcf727fff5f4b76"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">flatten</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">start_dim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">end_dim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="o"><span class="pre">-</span></span><span class="m"><span class="pre">1</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable7flattenERKN5torch6stable6TensorE7int64_t7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the flatten.using_ints op. </p>
<p>Flattens the input tensor by reshaping it into a one-dimensional tensor. If start_dim or end_dim are specified, only dimensions starting from start_dim to end_dim are flattened.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor to flatten. </p></li>
<li><p><strong>start_dim</strong> – The first dimension to flatten. Defaults to 0. </p></li>
<li><p><strong>end_dim</strong> – The last dimension to flatten. Defaults to -1 (last dim). </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A flattened tensor. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable7squeezeERKN5torch6stable6TensorE7int64_t">
<span id="_CPPv3N5torch6stable7squeezeERKN5torch6stable6TensorE7int64_t"></span><span id="_CPPv2N5torch6stable7squeezeERKN5torch6stable6TensorE7int64_t"></span><span id="torch::stable::squeeze__torch::stable::TensorCR.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1a24d866d200d56124a13e40b631340b53"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">squeeze</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable7squeezeERKN5torch6stable6TensorE7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the squeeze.dim op. </p>
<p>Returns a tensor with the dimension of size one at the specified position removed. The returned tensor shares the same underlying data with the input tensor.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – The dimension to squeeze. the tensor is returned unchanged. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor with the specified dimension removed (if size was 1). </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable9unsqueezeERKN5torch6stable6TensorE7int64_t">
<span id="_CPPv3N5torch6stable9unsqueezeERKN5torch6stable6TensorE7int64_t"></span><span id="_CPPv2N5torch6stable9unsqueezeERKN5torch6stable6TensorE7int64_t"></span><span id="torch::stable::unsqueeze__torch::stable::TensorCR.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1adfb0a004b2c28c318f55996f933844db"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">unsqueeze</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable9unsqueezeERKN5torch6stable6TensorE7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the unsqueeze op. </p>
<p>Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with the input tensor.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – The index at which to insert values are supported. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor with an additional dimension. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable9transposeERKN5torch6stable6TensorE7int64_t7int64_t">
<span id="_CPPv3N5torch6stable9transposeERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="_CPPv2N5torch6stable9transposeERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="torch::stable::transpose__torch::stable::TensorCR.int64_t.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1a20fe87b7853288970cba3657b906ffa5"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">transpose</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim0</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim1</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable9transposeERKN5torch6stable6TensorE7int64_t7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the transpose.int op. </p>
<p>Returns a tensor that is a transposed version of the input, with dimensions dim0 and dim1 swapped. The returned tensor shares storage with the input.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim0</strong> – The first dimension to transpose. </p></li>
<li><p><strong>dim1</strong> – The second dimension to transpose. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A transposed view of the input tensor. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6selectERKN5torch6stable6TensorE7int64_t7int64_t">
<span id="_CPPv3N5torch6stable6selectERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="_CPPv2N5torch6stable6selectERKN5torch6stable6TensorE7int64_t7int64_t"></span><span id="torch::stable::select__torch::stable::TensorCR.int64_t.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1a483e6d91a1cab6b57d63e821fba8da2c"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">select</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">index</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6selectERKN5torch6stable6TensorE7int64_t7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the select.int op. </p>
<p>Slices the input tensor along the specified dimension at the given index. This function returns a view of the original tensor with the given dimension removed.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The index parameter is typed header-only.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – The dimension to slice. </p></li>
<li><p><strong>index</strong> – The index to select along the dimension. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor with one fewer dimension. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6narrowERN5torch6stable6TensorE7int64_t7int64_t7int64_t">
<span id="_CPPv3N5torch6stable6narrowERN5torch6stable6TensorE7int64_t7int64_t7int64_t"></span><span id="_CPPv2N5torch6stable6narrowERN5torch6stable6TensorE7int64_t7int64_t7int64_t"></span><span id="torch::stable::narrow__torch::stable::TensorR.int64_t.int64_t.int64_t"></span><span class="target" id="namespacetorch_1_1stable_1a3b8de956af86ca04ba1b8d80bab48ec8"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">narrow</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">start</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">length</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6narrowERN5torch6stable6TensorE7int64_t7int64_t7int64_t" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the narrow.default op. </p>
<p>Returns a new tensor that is a narrowed version of the input tensor. The dimension dim is narrowed from start to start + length.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The start and length parameters is not yet header-only.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor to narrow. </p></li>
<li><p><strong>dim</strong> – The dimension along which to narrow. </p></li>
<li><p><strong>start</strong> – The starting index for the narrowed dimension. </p></li>
<li><p><strong>length</strong> – The length of the narrowed dimension. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new tensor that is a narrowed view of the input. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable3padERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefERKNSt6stringEd">
<span id="_CPPv3N5torch6stable3padERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefERKNSt6stringEd"></span><span id="_CPPv2N5torch6stable3padERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefERKNSt6stringEd"></span><span id="torch::stable::pad__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef.ssCR.double"></span><span class="target" id="namespacetorch_1_1stable_1a80187ae6fe0a9014a17d6ed0ae8f4b56"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">pad</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pad</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">string</span></span><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">mode</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="s"><span class="pre">&quot;constant&quot;</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">value</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">0.0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable3padERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefERKNSt6stringEd" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the pad.default op. </p>
<p>Pads the input tensor according to the specified padding sizes. The padding is applied symmetrically to each dimension, with the padding sizes specified in reverse order (last dimension first).</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pad parameter is typed not yet header-only.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor to pad. </p></li>
<li><p><strong>pad</strong> – The padding sizes for each dimension (in pairs, starting from the last dimension). </p></li>
<li><p><strong>mode</strong> – The padding mode: “constant”, “reflect”, “replicate”, or “circular”. Defaults to “constant”. </p></li>
<li><p><strong>value</strong> – The fill value for constant padding. Defaults to 0.0. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new padded tensor. </p>
</dd>
</dl>
</dd></dl>

</section>
<section id="device-and-type-conversion">
<h3>Device and Type Conversion<a class="headerlink" href="#device-and-type-conversion" title="Link to this heading">#</a></h3>
<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable2toERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEEbbNSt8optionalIN5torch10headeronly12MemoryFormatEEE">
<span id="_CPPv3N5torch6stable2toERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEEbbNSt8optionalIN5torch10headeronly12MemoryFormatEEE"></span><span id="_CPPv2N5torch6stable2toERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEEbbNSt8optionalIN5torch10headeronly12MemoryFormatEEE"></span><span id="torch::stable::to__torch::stable::TensorCR.std::optional:torch::headeronly::ScalarType:.std::optional:torch::headeronly::Layout:.std::optional:torch::stable::Device:.std::optional:b:.b.b.std::optional:torch::headeronly::MemoryFormat:"></span><span class="target" id="namespacetorch_1_1stable_1a90b24ee3b37e7a81fdd86633fbe74ee9"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">to</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">Layout</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">layout</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">pin_memory</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">non_blocking</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">copy</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">MemoryFormat</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">memory_format</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable2toERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEEbbNSt8optionalIN5torch10headeronly12MemoryFormatEEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the to.dtype_layout op. </p>
<p>Converts a tensor to the specified dtype, layout, device, and/or memory format. Returns a new tensor with the specified properties.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dtype</strong> – Optional target scalar type. </p></li>
<li><p><strong>layout</strong> – Optional target memory layout. </p></li>
<li><p><strong>device</strong> – Optional target device. </p></li>
<li><p><strong>pin_memory</strong> – Optional flag to use pinned memory. </p></li>
<li><p><strong>non_blocking</strong> – If true, the operation may be asynchronous. Defaults to false. </p></li>
<li><p><strong>copy</strong> – If true, always create a copy. Defaults to false. </p></li>
<li><p><strong>memory_format</strong> – Optional target memory format. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor with the specified properties. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable2toERKN5torch6stable6TensorEN5torch6stable6DeviceEbb">
<span id="_CPPv3N5torch6stable2toERKN5torch6stable6TensorEN5torch6stable6DeviceEbb"></span><span id="_CPPv2N5torch6stable2toERKN5torch6stable6TensorEN5torch6stable6DeviceEbb"></span><span id="torch::stable::to__torch::stable::TensorCR.torch::stable::Device.b.b"></span><span class="target" id="namespacetorch_1_1stable_1ab51259cbfb068c6ef80ffb56333f0773"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">to</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_device.html#_CPPv4N5torch6stable6DeviceE" title="torch::stable::Device"><span class="n"><span class="pre">Device</span></span></a><span class="w"> </span><span class="n sig-param"><span class="pre">device</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">non_blocking</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">copy</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable2toERKN5torch6stable6TensorEN5torch6stable6DeviceEbb" title="Link to this definition">#</a><br /></dt>
<dd><p>Convenience overload for moving a tensor to a device. </p>
<p>Moves the tensor to the specified device. This is a convenience wrapper around the full <a class="reference internal" href="api/function_namespacetorch_1_1stable_1a90b24ee3b37e7a81fdd86633fbe74ee9.html#namespacetorch_1_1stable_1a90b24ee3b37e7a81fdd86633fbe74ee9"><span class="std std-ref">to()</span></a> function.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>device</strong> – The target device. </p></li>
<li><p><strong>non_blocking</strong> – If true, the operation may be asynchronous. Defaults to false. </p></li>
<li><p><strong>copy</strong> – If true, always create a copy. Defaults to false. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor on the specified device. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable5fill_ERKN5torch6stable6TensorEd">
<span id="_CPPv3N5torch6stable5fill_ERKN5torch6stable6TensorEd"></span><span id="_CPPv2N5torch6stable5fill_ERKN5torch6stable6TensorEd"></span><span id="torch::stable::fill___torch::stable::TensorCR.double"></span><span class="target" id="namespacetorch_1_1stable_1a9531469721e6888fc7fba8dc78106475"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">fill_</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">value</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable5fill_ERKN5torch6stable6TensorEd" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the fill_.Scalar op. </p>
<p>Fills the input tensor with the specified scalar value in-place and returns it. This has identical semantics to the existing fill_.Scalar op.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The value parameter is typed as double This is because Scalar.h is currently not header-only.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The tensor to fill. </p></li>
<li><p><strong>value</strong> – The scalar value to fill the tensor with. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The input tensor, now filled with the specified value. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable5zero_ERN5torch6stable6TensorE">
<span id="_CPPv3N5torch6stable5zero_ERN5torch6stable6TensorE"></span><span id="_CPPv2N5torch6stable5zero_ERN5torch6stable6TensorE"></span><span id="torch::stable::zero___torch::stable::TensorR"></span><span class="target" id="namespacetorch_1_1stable_1ad3f8981d0aff116ac32254fe4a27fa17"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">zero_</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable5zero_ERN5torch6stable6TensorE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the zero_ op. </p>
<p>Fills the input tensor with zeros in-place and returns it. Unlike the tensor method version (<a class="reference internal" href="api/function_namespacetorch_1_1stable_1ad3f8981d0aff116ac32254fe4a27fa17.html#namespacetorch_1_1stable_1ad3f8981d0aff116ac32254fe4a27fa17"><span class="std std-ref">t.zero_()</span></a>), this is called as a function: zero_(t).</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>self</strong> – The tensor to fill with zeros. </p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The input tensor, now filled with zeros. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable5copy_ERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIbEE">
<span id="_CPPv3N5torch6stable5copy_ERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIbEE"></span><span id="_CPPv2N5torch6stable5copy_ERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIbEE"></span><span id="torch::stable::copy___torch::stable::TensorR.torch::stable::TensorCR.std::optional:b:"></span><span class="target" id="namespacetorch_1_1stable_1ae383cb5ab4205135a7f25d5b81e03bc9"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">copy_</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">src</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="kt"><span class="pre">bool</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">non_blocking</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable5copy_ERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIbEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the copy_ op. </p>
<p>Copies the elements from the source tensor into the destination tensor in-place and returns the destination tensor. The tensors must be broadcastable.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The destination tensor (modified in-place). </p></li>
<li><p><strong>src</strong> – The source tensor to copy from. </p></li>
<li><p><strong>non_blocking</strong> – If true, the copy may occur asynchronously with respect to the host. Defaults to false. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The destination tensor with copied values. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable6matmulERKN5torch6stable6TensorERKN5torch6stable6TensorE">
<span id="_CPPv3N5torch6stable6matmulERKN5torch6stable6TensorERKN5torch6stable6TensorE"></span><span id="_CPPv2N5torch6stable6matmulERKN5torch6stable6TensorERKN5torch6stable6TensorE"></span><span id="torch::stable::matmul__torch::stable::TensorCR.torch::stable::TensorCR"></span><span class="target" id="namespacetorch_1_1stable_1a7460ebb5fac290af61c2b319c7a1c8a3"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">matmul</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">other</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable6matmulERKN5torch6stable6TensorERKN5torch6stable6TensorE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the matmul op. </p>
<p>Performs matrix multiplication between two tensors. The behavior depends on the dimensionality of the tensors (see PyTorch documentation for details on broadcasting rules for matmul).</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The first input tensor. </p></li>
<li><p><strong>other</strong> – The second input tensor. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The result of matrix multiplication. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorE7int64_tb">
<span id="_CPPv3N5torch6stable4amaxERKN5torch6stable6TensorE7int64_tb"></span><span id="_CPPv2N5torch6stable4amaxERKN5torch6stable6TensorE7int64_tb"></span><span id="torch::stable::amax__torch::stable::TensorCR.int64_t.b"></span><span class="target" id="namespacetorch_1_1stable_1ababb6017b8a214ae421a05c29adcf89c"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">amax</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">keepdim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorE7int64_tb" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the amax.default op (single dimension). </p>
<p>Computes the maximum value along the specified dimension. If keepdim is true, the output tensor has the same number of dimensions as the input, with the reduced dimension having size 1. Otherwise, the reduced dimension is removed.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – The dimension along which to compute the maximum. </p></li>
<li><p><strong>keepdim</strong> – Whether to retain </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor containing the maximum values along the specified dimension. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefEb">
<span id="_CPPv3N5torch6stable4amaxERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefEb"></span><span id="_CPPv2N5torch6stable4amaxERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefEb"></span><span id="torch::stable::amax__torch::stable::TensorCR.torch::headeronly::IntHeaderOnlyArrayRef.b"></span><span class="target" id="namespacetorch_1_1stable_1a8a9bf40e0ec4d4d05bab811cf5bcd730"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">amax</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dims</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">keepdim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefEb" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the amax.default op (multiple dimensions). </p>
<p>Computes the maximum value reducing over all the specified dimensions. If keepdim is true, the output tensor has the same number of dimensions as the input, with the reduced dimensions having size 1. Otherwise, the reduced dimensions are removed.</p>
<p>Minimum compatible version: PyTorch 2.9.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The dims parameter is typed is not yet header-only.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dims</strong> – The dimensions along which to compute the maximum. </p></li>
<li><p><strong>keepdim</strong> – Whether to retain the reduced dimensions. Defaults to false. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor containing the maximum values. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable3sumERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE">
<span id="_CPPv3N5torch6stable3sumERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"></span><span id="_CPPv2N5torch6stable3sumERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"></span><span id="torch::stable::sum__torch::stable::TensorCR.std::optional:torch::headeronly::IntHeaderOnlyArrayRef:.b.std::optional:torch::headeronly::ScalarType:"></span><span class="target" id="namespacetorch_1_1stable_1a895b49e556282e23d55459cc61bd23a0"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">sum</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">keepdim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable3sumERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the sum.dim_IntList op. </p>
<p>Computes the sum of the input tensor along the specified dimensions. If dim is not provided, sums over all dimensions.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – Optional dimensions to reduce. If not provided, reduces all dimensions. </p></li>
<li><p><strong>keepdim</strong> – Whether to retain the reduced dimensions. Defaults to false. </p></li>
<li><p><strong>dtype</strong> – Optional output dtype. If not provided, uses the input dtype. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor containing the sum. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable7sum_outERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE">
<span id="_CPPv3N5torch6stable7sum_outERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"></span><span id="_CPPv2N5torch6stable7sum_outERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"></span><span id="torch::stable::sum_out__torch::stable::TensorR.torch::stable::TensorCR.std::optional:torch::headeronly::IntHeaderOnlyArrayRef:.b.std::optional:torch::headeronly::ScalarType:"></span><span class="target" id="namespacetorch_1_1stable_1aa3c19bc0fd014c3b055a1cd05c49495e"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">sum_out</span></span></span><span class="sig-paren">(</span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">out</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">IntHeaderOnlyArrayRef</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span>, <span class="kt"><span class="pre">bool</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">keepdim</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="k"><span class="pre">false</span></span>, <span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">optional</span></span><span class="p"><span class="pre">&lt;</span></span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">headeronly</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">ScalarType</span></span><span class="p"><span class="pre">&gt;</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">dtype</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="n"><span class="pre">std</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">nullopt</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable7sum_outERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the sum.IntList_out op. </p>
<p>Computes the sum of the input tensor along the specified dimensions, storing the result in the provided output tensor. Following C++ convention, the out parameter comes first.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> – The output tensor (modified in-place). </p></li>
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>dim</strong> – Optional dimensions to reduce. </p></li>
<li><p><strong>keepdim</strong> – Whether to retain the reduced dimensions. Defaults to false. </p></li>
<li><p><strong>dtype</strong> – Optional output dtype. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Reference to the output tensor. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable8subtractERKN5torch6stable6TensorERKN5torch6stable6TensorEd">
<span id="_CPPv3N5torch6stable8subtractERKN5torch6stable6TensorERKN5torch6stable6TensorEd"></span><span id="_CPPv2N5torch6stable8subtractERKN5torch6stable6TensorERKN5torch6stable6TensorEd"></span><span id="torch::stable::subtract__torch::stable::TensorCR.torch::stable::TensorCR.double"></span><span class="target" id="namespacetorch_1_1stable_1a20b27efc1fc8021d202a3d353395784d"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">subtract</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">self</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span><a class="reference internal" href="api/classtorch_1_1stable_1_1_tensor.html#_CPPv4N5torch6stable6TensorE" title="torch::stable::Tensor"><span class="n"><span class="pre">Tensor</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">other</span></span>, <span class="kt"><span class="pre">double</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">alpha</span></span><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="m"><span class="pre">1.0</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable8subtractERKN5torch6stable6TensorERKN5torch6stable6TensorEd" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable version of the subtract.Tensor op. </p>
<p>Subtracts the other tensor from self, with an optional scaling factor alpha. Computes: self - alpha * other.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The alpha parameter is typed as double API uses double for the Scalar parameter.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong> – The input tensor. </p></li>
<li><p><strong>other</strong> – The tensor to subtract. </p></li>
<li><p><strong>alpha</strong> – The scaling factor for other. Defaults to 1.0. </p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The result of self - alpha * other. </p>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4I0EN5torch6stable12parallel_forEvK7int64_tK7int64_tK7int64_tRK1F">
<span id="_CPPv3I0EN5torch6stable12parallel_forEK7int64_tK7int64_tK7int64_tRK1F"></span><span id="_CPPv2I0EN5torch6stable12parallel_forEK7int64_tK7int64_tK7int64_tRK1F"></span><span class="k"><span class="pre">template</span></span><span class="p"><span class="pre">&lt;</span></span><span class="k"><span class="pre">class</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">F</span></span></span><span class="p"><span class="pre">&gt;</span></span><br /><span class="target" id="namespacetorch_1_1stable_1a37b8a1dda03bdfdcc294eb862bc39b43"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">parallel_for</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">begin</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">end</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">grain_size</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="#_CPPv4I0EN5torch6stable12parallel_forEvK7int64_tK7int64_tK7int64_tRK1F" title="torch::stable::parallel_for::F"><span class="n"><span class="pre">F</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">f</span></span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4I0EN5torch6stable12parallel_forEvK7int64_tK7int64_tK7int64_tRK1F" title="Link to this definition">#</a><br /></dt>
<dd><p>Stable parallel_for utility. </p>
<p>Provides a stable interface to at::parallel_for for parallel execution. The function f will be called with (begin, end) ranges to process in parallel. grain_size controls the minimum work size per thread for efficient parallelization.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Template Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>F</strong> – The callable type </p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>begin</strong> – The start of the iteration range. </p></li>
<li><p><strong>end</strong> – The end of the iteration range (exclusive). </p></li>
<li><p><strong>grain_size</strong> – The minimum number of iterations per thread. </p></li>
<li><p><strong>f</strong> – The function to execute in parallel. </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp" id="_CPPv4N5torch6stable15get_num_threadsEv">
<span id="_CPPv3N5torch6stable15get_num_threadsEv"></span><span id="_CPPv2N5torch6stable15get_num_threadsEv"></span><span id="torch::stable::get_num_threads"></span><span class="target" id="namespacetorch_1_1stable_1a1768ea5675df96693cd86d054471e106"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">uint32_t</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">get_num_threads</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#_CPPv4N5torch6stable15get_num_threadsEv" title="Link to this definition">#</a><br /></dt>
<dd><p>Gets the number of threads for the parallel backend. </p>
<p>Provides a stable interface to at::get_num_threads.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The number of threads </p>
</dd>
</dl>
</dd></dl>

</section>
<section id="parallelization-utilities">
<h3>Parallelization Utilities<a class="headerlink" href="#parallelization-utilities" title="Link to this heading">#</a></h3>
<dl class="cpp function">
<dt class="sig sig-object cpp">
<span class="k"><span class="pre">template</span></span><span class="p"><span class="pre">&lt;</span></span><span class="k"><span class="pre">class</span></span><span class="w"> </span><span class="sig-name descname"><span class="n"><span class="pre">F</span></span></span><span class="p"><span class="pre">&gt;</span></span><br /><span class="target" id="namespacetorch_1_1stable_1a37b8a1dda03bdfdcc294eb862bc39b43"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="kt"><span class="pre">void</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">parallel_for</span></span></span><span class="sig-paren">(</span><span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">begin</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">end</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><span class="n"><span class="pre">int64_t</span></span><span class="w"> </span><span class="n sig-param"><span class="pre">grain_size</span></span>, <span class="k"><span class="pre">const</span></span><span class="w"> </span><a class="reference internal" href="#_CPPv4I0EN5torch6stable12parallel_forEvK7int64_tK7int64_tK7int64_tRK1F" title="torch::stable::parallel_for::F"><span class="n"><span class="pre">F</span></span></a><span class="w"> </span><span class="p"><span class="pre">&amp;</span></span><span class="n sig-param"><span class="pre">f</span></span><span class="sig-paren">)</span><br /></dt>
<dd><p>Stable parallel_for utility. </p>
<p>Provides a stable interface to at::parallel_for for parallel execution. The function f will be called with (begin, end) ranges to process in parallel. grain_size controls the minimum work size per thread for efficient parallelization.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Template Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>F</strong> – The callable type </p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>begin</strong> – The start of the iteration range. </p></li>
<li><p><strong>end</strong> – The end of the iteration range (exclusive). </p></li>
<li><p><strong>grain_size</strong> – The minimum number of iterations per thread. </p></li>
<li><p><strong>f</strong> – The function to execute in parallel. </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="cpp function">
<dt class="sig sig-object cpp">
<span class="target" id="namespacetorch_1_1stable_1a1768ea5675df96693cd86d054471e106"></span><span class="k"><span class="pre">inline</span></span><span class="w"> </span><span class="n"><span class="pre">uint32_t</span></span><span class="w"> </span><span class="sig-prename descclassname"><span class="n"><span class="pre">torch</span></span><span class="p"><span class="pre">::</span></span><span class="n"><span class="pre">stable</span></span><span class="p"><span class="pre">::</span></span></span><span class="sig-name descname"><span class="n"><span class="pre">get_num_threads</span></span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><br /></dt>
<dd><p>Gets the number of threads for the parallel backend. </p>
<p>Provides a stable interface to at::get_num_threads.</p>
<p>Minimum compatible version: PyTorch 2.10.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The number of threads </p>
</dd>
</dl>
</dd></dl>

</section>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="frontend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The C++ Frontend</p>
      </div>
    </a>
    <a class="right-next"
       href="api/library_root.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Library API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="frontend.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The C++ Frontend</p>
      </div>
    </a>
    <a class="right-next"
       href="api/library_root.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Library API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#library-registration-macros">Library Registration Macros</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable-torch-library-ns-m"><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY(ns,</span> <span class="pre">m)</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable-torch-library-impl-ns-k-m"><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_IMPL(ns,</span> <span class="pre">k,</span> <span class="pre">m)</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable-torch-library-fragment-ns-m"><code class="docutils literal notranslate"><span class="pre">STABLE_TORCH_LIBRARY_FRAGMENT(ns,</span> <span class="pre">m)</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#torch-box-func"><code class="docutils literal notranslate"><span class="pre">TORCH_BOX(&amp;func)</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-class">Tensor Class</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6TensorE"><code class="docutils literal notranslate"><span class="pre">torch::stable::Tensor</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6Tensor6TensorEv"><code class="docutils literal notranslate"><span class="pre">Tensor()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6Tensor6TensorE16AtenTensorHandle"><code class="docutils literal notranslate"><span class="pre">Tensor()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor3getEv"><code class="docutils literal notranslate"><span class="pre">get()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor8data_ptrEv"><code class="docutils literal notranslate"><span class="pre">data_ptr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor16mutable_data_ptrEv"><code class="docutils literal notranslate"><span class="pre">mutable_data_ptr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor14const_data_ptrEv"><code class="docutils literal notranslate"><span class="pre">const_data_ptr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4I0ENK5torch6stable6Tensor16mutable_data_ptrEP1Tv"><code class="docutils literal notranslate"><span class="pre">mutable_data_ptr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4I0_NSt11enable_if_tIXntNSt10is_const_vI1TEEEiEEENK5torch6stable6Tensor14const_data_ptrEPK1Tv"><code class="docutils literal notranslate"><span class="pre">const_data_ptr()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor17set_requires_gradEb"><code class="docutils literal notranslate"><span class="pre">set_requires_grad()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor3dimEv"><code class="docutils literal notranslate"><span class="pre">dim()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor5numelEv"><code class="docutils literal notranslate"><span class="pre">numel()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor5sizesEv"><code class="docutils literal notranslate"><span class="pre">sizes()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor7stridesEv"><code class="docutils literal notranslate"><span class="pre">strides()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor13is_contiguousEv"><code class="docutils literal notranslate"><span class="pre">is_contiguous()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor6strideE7int64_t"><code class="docutils literal notranslate"><span class="pre">stride()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor16get_device_indexEv"><code class="docutils literal notranslate"><span class="pre">get_device_index()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor7is_cudaEv"><code class="docutils literal notranslate"><span class="pre">is_cuda()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor6is_cpuEv"><code class="docutils literal notranslate"><span class="pre">is_cpu()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor4sizeE7int64_t"><code class="docutils literal notranslate"><span class="pre">size()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor7definedEv"><code class="docutils literal notranslate"><span class="pre">defined()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor14storage_offsetEv"><code class="docutils literal notranslate"><span class="pre">storage_offset()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor12element_sizeEv"><code class="docutils literal notranslate"><span class="pre">element_size()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor11scalar_typeEv"><code class="docutils literal notranslate"><span class="pre">scalar_type()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor6deviceEv"><code class="docutils literal notranslate"><span class="pre">device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Tensor6layoutEv"><code class="docutils literal notranslate"><span class="pre">layout()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#device-class">Device Class</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6DeviceE"><code class="docutils literal notranslate"><span class="pre">torch::stable::Device</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6Device6DeviceE10DeviceType11DeviceIndex"><code class="docutils literal notranslate"><span class="pre">Device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6Device6DeviceERKNSt6stringE"><code class="docutils literal notranslate"><span class="pre">Device()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6DeviceeqERK6Device"><code class="docutils literal notranslate"><span class="pre">operator==()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6DeviceneERK6Device"><code class="docutils literal notranslate"><span class="pre">operator!=()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6Device9set_indexE11DeviceIndex"><code class="docutils literal notranslate"><span class="pre">set_index()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Device4typeEv"><code class="docutils literal notranslate"><span class="pre">type()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Device5indexEv"><code class="docutils literal notranslate"><span class="pre">index()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Device9has_indexEv"><code class="docutils literal notranslate"><span class="pre">has_index()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Device7is_cudaEv"><code class="docutils literal notranslate"><span class="pre">is_cuda()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4NK5torch6stable6Device6is_cpuEv"><code class="docutils literal notranslate"><span class="pre">is_cpu()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deviceguard-class">DeviceGuard Class</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable11accelerator11DeviceGuardE"><code class="docutils literal notranslate"><span class="pre">torch::stable::accelerator::DeviceGuard</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable11accelerator11DeviceGuard11DeviceGuardE11DeviceIndex"><code class="docutils literal notranslate"><span class="pre">DeviceGuard()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable11accelerator11DeviceGuard9set_indexE11DeviceIndex"><code class="docutils literal notranslate"><span class="pre">set_index()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable11accelerator21getCurrentDeviceIndexEv"><code class="docutils literal notranslate"><span class="pre">getCurrentDeviceIndex()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stream-utilities">Stream Utilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-current-cuda-stream">Getting the Current CUDA Stream</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-error-checking-macros">CUDA Error Checking Macros</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#std-cuda-check-expr"><code class="docutils literal notranslate"><span class="pre">STD_CUDA_CHECK(EXPR)</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#std-cuda-kernel-launch-check"><code class="docutils literal notranslate"><span class="pre">STD_CUDA_KERNEL_LAUNCH_CHECK()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#header-only-utilities">Header-Only Utilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#error-checking">Error Checking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-types">Core Types</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensoraccessor">TensorAccessor</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dispatch-macros">Dispatch Macros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-api-list">Full API List</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stable-operators">Stable Operators</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-creation">Tensor Creation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable5emptyEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEENSt8optionalIN5torch10headeronly12MemoryFormatEEE"><code class="docutils literal notranslate"><span class="pre">empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable10empty_likeERKN5torch6stable6TensorE"><code class="docutils literal notranslate"><span class="pre">empty_like()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable9new_emptyERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"><code class="docutils literal notranslate"><span class="pre">new_empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable9new_zerosERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"><code class="docutils literal notranslate"><span class="pre">new_zeros()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable4fullEN5torch10headeronly21IntHeaderOnlyArrayRefEdNSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEE"><code class="docutils literal notranslate"><span class="pre">full()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-manipulation">Tensor Manipulation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable5cloneERKN5torch6stable6TensorE"><code class="docutils literal notranslate"><span class="pre">clone()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable10contiguousERKN5torch6stable6TensorEN5torch10headeronly12MemoryFormatE"><code class="docutils literal notranslate"><span class="pre">contiguous()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable7reshapeERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"><code class="docutils literal notranslate"><span class="pre">reshape()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable4viewERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefE"><code class="docutils literal notranslate"><span class="pre">view()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable7flattenERKN5torch6stable6TensorE7int64_t7int64_t"><code class="docutils literal notranslate"><span class="pre">flatten()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable7squeezeERKN5torch6stable6TensorE7int64_t"><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable9unsqueezeERKN5torch6stable6TensorE7int64_t"><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable9transposeERKN5torch6stable6TensorE7int64_t7int64_t"><code class="docutils literal notranslate"><span class="pre">transpose()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6selectERKN5torch6stable6TensorE7int64_t7int64_t"><code class="docutils literal notranslate"><span class="pre">select()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6narrowERN5torch6stable6TensorE7int64_t7int64_t7int64_t"><code class="docutils literal notranslate"><span class="pre">narrow()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable3padERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefERKNSt6stringEd"><code class="docutils literal notranslate"><span class="pre">pad()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#device-and-type-conversion">Device and Type Conversion</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable2toERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly10ScalarTypeEEENSt8optionalIN5torch10headeronly6LayoutEEENSt8optionalIN5torch6stable6DeviceEEENSt8optionalIbEEbbNSt8optionalIN5torch10headeronly12MemoryFormatEEE"><code class="docutils literal notranslate"><span class="pre">to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable2toERKN5torch6stable6TensorEN5torch6stable6DeviceEbb"><code class="docutils literal notranslate"><span class="pre">to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable5fill_ERKN5torch6stable6TensorEd"><code class="docutils literal notranslate"><span class="pre">fill_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable5zero_ERN5torch6stable6TensorE"><code class="docutils literal notranslate"><span class="pre">zero_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable5copy_ERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIbEE"><code class="docutils literal notranslate"><span class="pre">copy_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable6matmulERKN5torch6stable6TensorERKN5torch6stable6TensorE"><code class="docutils literal notranslate"><span class="pre">matmul()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorE7int64_tb"><code class="docutils literal notranslate"><span class="pre">amax()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable4amaxERKN5torch6stable6TensorEN5torch10headeronly21IntHeaderOnlyArrayRefEb"><code class="docutils literal notranslate"><span class="pre">amax()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable3sumERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"><code class="docutils literal notranslate"><span class="pre">sum()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable7sum_outERN5torch6stable6TensorERKN5torch6stable6TensorENSt8optionalIN5torch10headeronly21IntHeaderOnlyArrayRefEEEbNSt8optionalIN5torch10headeronly10ScalarTypeEEE"><code class="docutils literal notranslate"><span class="pre">sum_out()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable8subtractERKN5torch6stable6TensorERKN5torch6stable6TensorEd"><code class="docutils literal notranslate"><span class="pre">subtract()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4I0EN5torch6stable12parallel_forEvK7int64_tK7int64_tK7int64_tRK1F"><code class="docutils literal notranslate"><span class="pre">parallel_for()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#_CPPv4N5torch6stable15get_num_threadsEv"><code class="docutils literal notranslate"><span class="pre">get_num_threads()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization-utilities">Parallelization Utilities</a></li>
</ul>
</li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="_sources/stable.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Torch Stable API",
       "headline": "Torch Stable API",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/stable.html",
       "articleBody": "Torch Stable API# The PyTorch Stable C++ API provides a convenient high level interface to call ABI-stable tensor operations and other utilities commonly used in custom operators. These functions are designed to maintain binary compatibility across PyTorch versions, making them suitable for use in ahead-of-time compiled code. For more information on the stable ABI, see the Stable ABI notes. Library Registration Macros# These macros provide stable ABI equivalents of the standard PyTorch operator registration macros (TORCH_LIBRARY, TORCH_LIBRARY_IMPL, etc.). Use these when building custom operators that need to maintain binary compatibility across PyTorch versions. STABLE_TORCH_LIBRARY(ns, m)# Defines a library of operators in a namespace using the stable ABI. This is the stable ABI equivalent of TORCH_LIBRARY. Use this macro to define operator schemas that will maintain binary compatibility across PyTorch versions. Only one STABLE_TORCH_LIBRARY block can exist per namespace; use STABLE_TORCH_LIBRARY_FRAGMENT for additional definitions in the same namespace from different translation units. Parameters: ns - The namespace in which to define operators (e.g., mylib). m - The name of the StableLibrary variable available in the block. Example: STABLE_TORCH_LIBRARY(mylib, m) { m.def(\"my_op(Tensor input, int size) -\u003e Tensor\"); m.def(\"another_op(Tensor a, Tensor b) -\u003e Tensor\"); } Minimum compatible version: PyTorch 2.9. STABLE_TORCH_LIBRARY_IMPL(ns, k, m)# Registers operator implementations for a specific dispatch key using the stable ABI. This is the stable ABI equivalent of TORCH_LIBRARY_IMPL. Use this macro to provide implementations of operators for a specific dispatch key (e.g., CPU, CUDA) while maintaining binary compatibility across PyTorch versions. Note All kernel functions registered with this macro must be boxed using the TORCH_BOX macro. Parameters: ns - The namespace in which the operators are defined. k - The dispatch key (e.g., CPU, CUDA). m - The name of the StableLibrary variable available in the block. Example: STABLE_TORCH_LIBRARY_IMPL(mylib, CPU, m) { m.impl(\"my_op\", TORCH_BOX(\u0026my_cpu_kernel)); } STABLE_TORCH_LIBRARY_IMPL(mylib, CUDA, m) { m.impl(\"my_op\", TORCH_BOX(\u0026my_cuda_kernel)); } Minimum compatible version: PyTorch 2.9. STABLE_TORCH_LIBRARY_FRAGMENT(ns, m)# Extends operator definitions in an existing namespace using the stable ABI. This is the stable ABI equivalent of TORCH_LIBRARY_FRAGMENT. Use this macro to add additional operator definitions to a namespace that was already created with STABLE_TORCH_LIBRARY. Parameters: ns - The namespace to extend. m - The name of the StableLibrary variable available in the block. Minimum compatible version: PyTorch 2.9. TORCH_BOX(\u0026func)# Wraps a function to conform to the stable boxed kernel calling convention. This macro takes an unboxed kernel function pointer and generates a boxed wrapper that can be registered with the stable library API. Parameters: func - The unboxed kernel function to wrap. Example: Tensor my_kernel(const Tensor\u0026 input, int64_t size) { return input.reshape({size}); } STABLE_TORCH_LIBRARY_IMPL(my_namespace, CPU, m) { m.impl(\"my_op\", TORCH_BOX(\u0026my_kernel)); } Minimum compatible version: PyTorch 2.9. Tensor Class# The torch::stable::Tensor class offers a user-friendly C++ interface similar to torch::Tensor while maintaining binary compatibility across PyTorch versions. class Tensor# An ABI stable wrapper around PyTorch tensors. This class is modeled after TensorBase, as custom op kernels primarily need to interact with Tensor metadata (sizes, strides, device, dtype). Other tensor operations (like empty_like) exist as standalone functions outside of this struct. Minimum compatible version: PyTorch 2.9. Public Functions inline Tensor()# Constructs a Tensor with an uninitialized AtenTensorHandle. Creates a new stable::Tensor by allocating an uninitialized tensor handle. The ownership of the handle is managed internally via shared_ptr. Minimum compatible version: PyTorch 2.9. inline explicit Tensor(AtenTensorHandle ath)# Constructs a Tensor from an existing AtenTensorHandle. Steals ownership of the provided AtenTensorHandle. Minimum compatible version: PyTorch 2.9. Parameters: ath \u2013 The AtenTensorHandle to wrap. Ownership is transferred to this Tensor. inline AtenTensorHandle get() const# Returns a borrowed reference to the underlying AtenTensorHandle. Minimum compatible version: PyTorch 2.9. Returns: The underlying AtenTensorHandle. inline void *data_ptr() const# Returns a pointer to the tensor\u2019s data. Minimum compatible version: PyTorch 2.9. Returns: A void pointer to the tensor\u2019s data storage. inline void *mutable_data_ptr() const# Returns a mutable pointer to the tensor\u2019s data. Minimum compatible version: PyTorch 2.10. Returns: A mutable void pointer to the tensor\u2019s data storage. inline const void *const_data_ptr() const# Returns a const pointer to the tensor\u2019s data. Minimum compatible version: PyTorch 2.10. Returns: A const void pointer to the tensor\u2019s data storage. template\u003ctypename T\u003eT *mutable_data_ptr() const# Returns a typed mutable pointer to the tensor\u2019s data. Minimum compatible version: PyTorch 2.10. Template Parameters: T \u2013 The type to cast the data pointer to. Returns: A mutable pointer to the tensor\u2019s data cast to type T*. EiEEENK5torch6stable6Tensor14const_data_ptrEv\"\u003etemplate\u003ctypename T, std::enable_if_t\u003c!std::is_const_v\u003cT\u003e, int\u003e = 0\u003econst T *const_data_ptr() const# Returns a typed const pointer to the tensor\u2019s data. Minimum compatible version: PyTorch 2.10. Template Parameters: T \u2013 The type to cast the data pointer to. Must not be const-qualified. Returns: A const pointer to the tensor\u2019s data cast to type const T*. inline const Tensor \u0026set_requires_grad(bool requires_grad) const# Sets whether this tensor requires gradient computation. Minimum compatible version: PyTorch 2.10. Parameters: requires_grad \u2013 If true, gradients will be computed for this tensor during backpropagation. Returns: A reference to this Tensor. inline int64_t dim() const# Returns the number of dimensions of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The number of dimensions (rank) of the tensor. inline int64_t numel() const# Returns the total number of elements in the tensor. Minimum compatible version: PyTorch 2.9. Returns: The total number of elements across all dimensions. inline IntHeaderOnlyArrayRef sizes() const# Returns the sizes (shape) of the tensor. Returns a borrowed reference of the dimension sizes of the tensor. Minimum compatible version: PyTorch 2.9. Returns: An IntHeaderOnlyArrayRef containing the size of each dimension. inline IntHeaderOnlyArrayRef strides() const# Returns the strides of the tensor. Returns a borrowed reference of the strides of the tensor. Minimum compatible version: PyTorch 2.9. Returns: An IntHeaderOnlyArrayRef containing the stride of each dimension. inline bool is_contiguous() const# Checks if the tensor is contiguous in memory. Minimum compatible version: PyTorch 2.9. Note This is a subset of the original TensorBase API. It takes no arguments whereas the original API takes a memory format argument. Here, we assume the default contiguous memory format. Returns: true if the tensor is contiguous, false otherwise. inline int64_t stride(int64_t dim) const# Returns the stride of a specific dimension. Minimum compatible version: PyTorch 2.9. Parameters: dim \u2013 The dimension index to query. Returns: The stride of the specified dimension. inline DeviceIndex get_device_index() const# Returns the device index of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The device index as DeviceIndex (int32_t). inline bool is_cuda() const# Checks if the tensor is on a CUDA device. Minimum compatible version: PyTorch 2.9. Returns: true if the tensor is on a CUDA device, false otherwise. inline bool is_cpu() const# Checks if the tensor is on the CPU. Minimum compatible version: PyTorch 2.9. Returns: true if the tensor is on the CPU, false otherwise. inline int64_t size(int64_t dim) const# Returns the size of a specific dimension. Minimum compatible version: PyTorch 2.9. Parameters: dim \u2013 The dimension index to query. Returns: The size of the specified dimension. inline bool defined() const# Checks if the tensor is defined (not null). Minimum compatible version: PyTorch 2.9. Returns: true if the tensor is defined, false otherwise. inline int64_t storage_offset() const# Returns the storage offset of the tensor. The storage offset is the number of elements from the beginning of the underlying storage to the first element of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The storage offset in number of elements. inline size_t element_size() const# Returns the size in bytes of each element in the tensor. Minimum compatible version: PyTorch 2.9. Returns: The element size in bytes. ScalarType scalar_type() const# Returns the scalar type (dtype) of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The ScalarType of the tensor. Device device() const# Returns the device of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The Device on which the tensor resides. Layout layout() const# Returns the layout of the tensor. Minimum compatible version: PyTorch 2.9. Returns: The Layout of the tensor (e.g., Strided, Sparse). Device Class# The torch::stable::Device class provides a user-friendly C++ interface similar to c10::Device while maintaining binary compatibility across PyTorch versions. It represents a compute device (CPU, CUDA, etc.) with an optional device index. class Device# A stable version of c10::Device. Minimum compatible version: PyTorch 2.9. Public Functions inline Device(DeviceType type, DeviceIndex index = -1)# Constructs a Device from a DeviceType and optional device index. Minimum compatible version: PyTorch 2.9. Parameters: type \u2013 The type of device (e.g., DeviceType::CPU, DeviceType::CUDA). index \u2013 The device index. Default is -1 (current device). Device(const std::string \u0026device_string)# Constructs a stable::Device from a string description. The string must follow the schema: (cpu|cuda|\u2026)[:\u003cdevice-index\u003e] Minimum compatible version: PyTorch 2.10. Parameters: device_string \u2013 A string describing the device (e.g., \u201ccuda:0\u201d, \u201ccpu\u201d). inline bool operator==(const Device \u0026other) const noexcept# Checks if two devices are equal. Minimum compatible version: PyTorch 2.9. Parameters: other \u2013 The device to compare with. Returns: true if both type and index match, false otherwise. inline bool operator!=(const Device \u0026other) const noexcept# Checks if two devices are not equal. Minimum compatible version: PyTorch 2.9. Parameters: other \u2013 The device to compare with. Returns: true if type or index differ, false otherwise. inline void set_index(DeviceIndex index)# Sets the device index. Minimum compatible version: PyTorch 2.9. Parameters: index \u2013 The new device index. inline DeviceType type() const noexcept# Returns the device type. Minimum compatible version: PyTorch 2.9. Returns: The DeviceType of this device. inline DeviceIndex index() const noexcept# Returns the device index. Minimum compatible version: PyTorch 2.9. Returns: The device index, or -1 if no specific index is set. inline bool has_index() const noexcept# Checks if this device has a specific index. Minimum compatible version: PyTorch 2.9. Returns: true if index is not -1, false otherwise. inline bool is_cuda() const noexcept# Checks if this is a CUDA device. Minimum compatible version: PyTorch 2.9. Returns: true if the device type is CUDA, false otherwise. inline bool is_cpu() const noexcept# Checks if this is a CPU device. Minimum compatible version: PyTorch 2.9. Returns: true if the device type is CPU, false otherwise. DeviceGuard Class# The torch::stable::accelerator::DeviceGuard provides a user-friendly C++ interface similar to c10::DeviceGuard while maintaining binary compatibility across PyTorch versions. class DeviceGuard# A stable ABI version of c10::DeviceGuard. RAII class that sets the current device to the specified device index on construction and restores the previous device on destruction. Minimum compatible version: PyTorch 2.9. Public Functions inline explicit DeviceGuard(DeviceIndex device_index)# Constructs a DeviceGuard that sets the current device. Minimum compatible version: PyTorch 2.9. Parameters: device_index \u2013 The device index to set as the current device. inline void set_index(DeviceIndex device_index)# Changes the current device to the specified device index. Minimum compatible version: PyTorch 2.9. Parameters: device_index \u2013 The new device index to set. inline DeviceIndex torch::stable::accelerator::getCurrentDeviceIndex()# Gets the current device index. Returns the index of the currently active device for the accelerator. Minimum compatible version: PyTorch 2.9. Returns: The current device index. Stream Utilities# For CUDA stream access, we currently recommend the ABI stable C shim API. This will be improved in a future release with a more ergonomic wrapper. Getting the Current CUDA Stream# To obtain the current cudaStream_t for use in CUDA kernels: #include \u003ctorch/csrc/inductor/aoti_torch/c/shim.h\u003e #include \u003ctorch/headeronly/util/shim_utils.h\u003e // For now, we rely on the ABI stable C shim API to get the current CUDA stream. // This will be improved in a future release. // When using a C shim API, we need to use TORCH_ERROR_CODE_CHECK to // check the error code and throw an appropriate runtime_error otherwise. void* stream_ptr = nullptr; TORCH_ERROR_CODE_CHECK( aoti_torch_get_current_cuda_stream(tensor.get_device_index(), \u0026stream_ptr)); cudaStream_t stream = static_cast\u003ccudaStream_t\u003e(stream_ptr); // Now you can use \u0027stream\u0027 in your CUDA kernel launches my_kernel\u003c\u003c\u003cblocks, threads, 0, stream\u003e\u003e\u003e(args...); Note The TORCH_ERROR_CODE_CHECK macro is required when using C shim APIs to properly check error codes and throw appropriate exceptions. CUDA Error Checking Macros# These macros provide stable ABI equivalents for CUDA error checking. They wrap CUDA API calls and kernel launches, providing detailed error messages using PyTorch\u2019s error formatting. STD_CUDA_CHECK(EXPR)# Checks the result of a CUDA API call and throws an exception on error. Users of this macro are expected to include cuda_runtime.h. Example: STD_CUDA_CHECK(cudaMalloc(\u0026ptr, size)); STD_CUDA_CHECK(cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost)); Minimum compatible version: PyTorch 2.10. STD_CUDA_KERNEL_LAUNCH_CHECK()# Checks for errors from the most recent CUDA kernel launch. Equivalent to STD_CUDA_CHECK(cudaGetLastError()). Example: my_kernel\u003c\u003c\u003cblocks, threads, 0, stream\u003e\u003e\u003e(args...); STD_CUDA_KERNEL_LAUNCH_CHECK(); Minimum compatible version: PyTorch 2.10. Header-Only Utilities# The torch::headeronly namespace provides header-only versions of common PyTorch types and utilities. These can be used without linking against libtorch, making them ideal for maintaining binary compatibility across PyTorch versions. Error Checking# STD_TORCH_CHECK is a header-only macro for runtime assertions: #include \u003ctorch/headeronly/util/Exception.h\u003e STD_TORCH_CHECK(condition, \"Error message with \", variable, \" interpolation\"); Core Types# The following c10:: types are available as header-only versions under torch::headeronly::: torch::headeronly::ScalarType - Tensor data types (Float, Double, Int, etc.) torch::headeronly::DeviceType - Device types (CPU, CUDA, etc.) torch::headeronly::MemoryFormat - Memory layout formats (Contiguous, ChannelsLast, etc.) torch::headeronly::Layout - Tensor layouts (Strided, Sparse, etc.) #include \u003ctorch/headeronly/core/ScalarType.h\u003e #include \u003ctorch/headeronly/core/DeviceType.h\u003e #include \u003ctorch/headeronly/core/MemoryFormat.h\u003e #include \u003ctorch/headeronly/core/Layout.h\u003e auto dtype = torch::headeronly::ScalarType::Float; auto device_type = torch::headeronly::DeviceType::CUDA; auto memory_format = torch::headeronly::MemoryFormat::Contiguous; auto layout = torch::headeronly::Layout::Strided; TensorAccessor# TensorAccessor provides efficient, bounds-checked access to tensor data. You can construct one from a stable tensor\u2019s data pointer, sizes, and strides: #include \u003ctorch/headeronly/core/TensorAccessor.h\u003e // Create a TensorAccessor for a 2D float tensor auto sizes = tensor.sizes(); auto strides = tensor.strides(); torch::headeronly::TensorAccessor\u003cfloat, 2\u003e accessor( static_cast\u003cfloat*\u003e(tensor.mutable_data_ptr()), sizes.data(), strides.data()); // Access elements float value = accessor[i][j]; Dispatch Macros# Header-only dispatch macros (THO = Torch Header Only) are available for dtype and device dispatching: #include \u003ctorch/headeronly/core/Dispatch.h\u003e THO_DISPATCH_FLOATING_TYPES(tensor.scalar_type(), \"my_kernel\", [\u0026] { // scalar_t is the resolved type auto* data = tensor.data_ptr\u003cscalar_t\u003e(); }); Full API List# For the complete list of header-only APIs, see torch/header_only_apis.txt in the PyTorch source tree. Stable Operators# Tensor Creation# inline torch::stable::Tensor torch::stable::empty(torch::headeronly::IntHeaderOnlyArrayRef size, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt, std::optional\u003ctorch::headeronly::Layout\u003e layout = std::nullopt, std::optional\u003ctorch::stable::Device\u003e device = std::nullopt, std::optional\u003cbool\u003e pin_memory = std::nullopt, std::optional\u003ctorch::headeronly::MemoryFormat\u003e memory_format = std::nullopt)# Stable version of the empty.memory_format op. Creates a new uninitialized tensor with the specified size and options. This function supports full tensor creation options including device, dtype, layout, and memory format. Minimum compatible version: PyTorch 2.10. Parameters: size \u2013 The desired size of the output tensor. dtype \u2013 Optional scalar type for the tensor elements. layout \u2013 Optional memory layout (e.g., strided, sparse). device \u2013 Optional device to place the tensor on. pin_memory \u2013 Optional flag to use pinned memory (for CUDA tensors). memory_format \u2013 Optional memory format for the tensor. Returns: A new uninitialized tensor with the specified properties. inline torch::stable::Tensor torch::stable::empty_like(const torch::stable::Tensor \u0026self)# Stable version of the empty_like op. Creates a new uninitialized tensor with the same size, dtype, layout, and device as the input tensor. This version does not support kwargs (device, dtype, layout, memory_format) - kwargs support may be added in the future. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor whose properties will be used for the new tensor. Returns: A new uninitialized tensor with the same properties as self. inline torch::stable::Tensor torch::stable::new_empty(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef size, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt, std::optional\u003ctorch::headeronly::Layout\u003e layout = std::nullopt, std::optional\u003ctorch::stable::Device\u003e device = std::nullopt, std::optional\u003cbool\u003e pin_memory = std::nullopt)# Stable version of the new_empty op (2.10 version with full kwargs). Creates a new uninitialized tensor with the specified size and options. This version supports all tensor creation kwargs. For versions \u003c 2.10, a simpler overload that only takes dtype is available. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor whose properties may be inherited if kwargs are not provided. size \u2013 The desired size of the output tensor. dtype \u2013 Optional scalar type for the tensor elements. layout \u2013 Optional memory layout (e.g., strided, sparse). device \u2013 Optional device to place the tensor on. pin_memory \u2013 Optional flag to use pinned memory (for CUDA tensors). Returns: A new uninitialized tensor with the specified properties. inline torch::stable::Tensor torch::stable::new_zeros(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef size, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt, std::optional\u003ctorch::headeronly::Layout\u003e layout = std::nullopt, std::optional\u003ctorch::stable::Device\u003e device = std::nullopt, std::optional\u003cbool\u003e pin_memory = std::nullopt)# Stable version of the new_zeros op (2.10 version with full kwargs). Creates a new zero-filled tensor with the specified size and options. This version supports all tensor creation kwargs. For versions \u003c 2.10, a simpler overload that only takes dtype is available. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor whose properties may be inherited if kwargs are not provided. size \u2013 The desired size of the output tensor. dtype \u2013 Optional scalar type for the tensor elements. layout \u2013 Optional memory layout (e.g., strided, sparse). device \u2013 Optional device to place the tensor on. pin_memory \u2013 Optional flag to use pinned memory (for CUDA tensors). Returns: A new zero-filled tensor with the specified properties. inline torch::stable::Tensor torch::stable::full(torch::headeronly::IntHeaderOnlyArrayRef size, double fill_value, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt, std::optional\u003ctorch::headeronly::Layout\u003e layout = std::nullopt, std::optional\u003ctorch::stable::Device\u003e device = std::nullopt, std::optional\u003cbool\u003e pin_memory = std::nullopt)# Stable version of the full.default op. Creates a tensor of the specified size filled with the given value. Minimum compatible version: PyTorch 2.10. Note The fill_value parameter is typed C shim API uses double for the Scalar parameter. Parameters: size \u2013 The desired size of the output tensor. fill_value \u2013 The value to fill the tensor with. dtype \u2013 Optional scalar type for the tensor elements. layout \u2013 Optional memory layout. device \u2013 Optional device to place the tensor on. pin_memory \u2013 Optional flag to use pinned memory. Returns: A new tensor filled with the specified value. Warning doxygenfunction: Unable to resolve function \u201ctorch::stable::from_blob\u201d with arguments None in doxygen xml output for project \u201cPyTorch\u201d from directory: /var/lib/jenkins/workspace/docs/cpp/build/xml. Potential matches: - torch::stable::Tensor from_blob(void *data, torch::headeronly::IntHeaderOnlyArrayRef sizes, torch::headeronly::IntHeaderOnlyArrayRef strides, torch::stable::Device device, torch::headeronly::ScalarType dtype, DeleterFnPtr deleter, int64_t storage_offset = 0, torch::headeronly::Layout layout = torch::headeronly::Layout::Strided) - torch::stable::Tensor from_blob(void *data, torch::headeronly::IntHeaderOnlyArrayRef sizes, torch::headeronly::IntHeaderOnlyArrayRef strides, torch::stable::Device device, torch::headeronly::ScalarType dtype, int64_t storage_offset = 0, torch::headeronly::Layout layout = torch::headeronly::Layout::Strided) Tensor Manipulation# inline torch::stable::Tensor torch::stable::clone(const torch::stable::Tensor \u0026self)# Stable version of the clone op. Returns a copy of the input tensor. The returned tensor has the same data and type as the input, but is stored in a new memory location. Minimum compatible version: PyTorch 2.9. Note Optional memory_format kwarg support Parameters: self \u2013 The input tensor to clone. Returns: A new tensor with copied data. inline torch::stable::Tensor torch::stable::contiguous(const torch::stable::Tensor \u0026self, torch::headeronly::MemoryFormat memory_format = torch::headeronly::MemoryFormat::Contiguous)# Stable version of the contiguous op. Returns a contiguous in memory tensor containing the same data as the input tensor. If the input tensor is already contiguous in the specified memory format, the input tensor is returned. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. memory_format \u2013 The desired memory format. Returns: A contiguous tensor. inline torch::stable::Tensor torch::stable::reshape(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef shape)# Stable version of the reshape op. Returns a tensor with the same data and number of elements as the input, but with the specified shape. When possible, the returned tensor will be a view of the input. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. shape \u2013 The desired output shape. Returns: A tensor with the specified shape. inline torch::stable::Tensor torch::stable::view(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef size)# Stable version of the view op. Returns a new tensor with the same data as the input tensor but with a different shape. The returned tensor shares the same data and must have the same number of elements. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. size \u2013 The desired output shape. Returns: A view tensor with the specified shape. inline torch::stable::Tensor torch::stable::flatten(const torch::stable::Tensor \u0026self, int64_t start_dim = 0, int64_t end_dim = -1)# Stable version of the flatten.using_ints op. Flattens the input tensor by reshaping it into a one-dimensional tensor. If start_dim or end_dim are specified, only dimensions starting from start_dim to end_dim are flattened. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor to flatten. start_dim \u2013 The first dimension to flatten. Defaults to 0. end_dim \u2013 The last dimension to flatten. Defaults to -1 (last dim). Returns: A flattened tensor. inline torch::stable::Tensor torch::stable::squeeze(const torch::stable::Tensor \u0026self, int64_t dim)# Stable version of the squeeze.dim op. Returns a tensor with the dimension of size one at the specified position removed. The returned tensor shares the same underlying data with the input tensor. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor. dim \u2013 The dimension to squeeze. the tensor is returned unchanged. Returns: A tensor with the specified dimension removed (if size was 1). inline torch::stable::Tensor torch::stable::unsqueeze(const torch::stable::Tensor \u0026self, int64_t dim)# Stable version of the unsqueeze op. Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with the input tensor. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor. dim \u2013 The index at which to insert values are supported. Returns: A tensor with an additional dimension. inline torch::stable::Tensor torch::stable::transpose(const torch::stable::Tensor \u0026self, int64_t dim0, int64_t dim1)# Stable version of the transpose.int op. Returns a tensor that is a transposed version of the input, with dimensions dim0 and dim1 swapped. The returned tensor shares storage with the input. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor. dim0 \u2013 The first dimension to transpose. dim1 \u2013 The second dimension to transpose. Returns: A transposed view of the input tensor. inline torch::stable::Tensor torch::stable::select(const torch::stable::Tensor \u0026self, int64_t dim, int64_t index)# Stable version of the select.int op. Slices the input tensor along the specified dimension at the given index. This function returns a view of the original tensor with the given dimension removed. Minimum compatible version: PyTorch 2.9. Note The index parameter is typed header-only. Parameters: self \u2013 The input tensor. dim \u2013 The dimension to slice. index \u2013 The index to select along the dimension. Returns: A tensor with one fewer dimension. inline torch::stable::Tensor torch::stable::narrow(torch::stable::Tensor \u0026self, int64_t dim, int64_t start, int64_t length)# Stable version of the narrow.default op. Returns a new tensor that is a narrowed version of the input tensor. The dimension dim is narrowed from start to start + length. Minimum compatible version: PyTorch 2.9. Note The start and length parameters is not yet header-only. Parameters: self \u2013 The input tensor to narrow. dim \u2013 The dimension along which to narrow. start \u2013 The starting index for the narrowed dimension. length \u2013 The length of the narrowed dimension. Returns: A new tensor that is a narrowed view of the input. inline torch::stable::Tensor torch::stable::pad(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef pad, const std::string \u0026mode = \"constant\", double value = 0.0)# Stable version of the pad.default op. Pads the input tensor according to the specified padding sizes. The padding is applied symmetrically to each dimension, with the padding sizes specified in reverse order (last dimension first). Minimum compatible version: PyTorch 2.9. Note The pad parameter is typed not yet header-only. Parameters: self \u2013 The input tensor to pad. pad \u2013 The padding sizes for each dimension (in pairs, starting from the last dimension). mode \u2013 The padding mode: \u201cconstant\u201d, \u201creflect\u201d, \u201creplicate\u201d, or \u201ccircular\u201d. Defaults to \u201cconstant\u201d. value \u2013 The fill value for constant padding. Defaults to 0.0. Returns: A new padded tensor. Device and Type Conversion# inline torch::stable::Tensor torch::stable::to(const torch::stable::Tensor \u0026self, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt, std::optional\u003ctorch::headeronly::Layout\u003e layout = std::nullopt, std::optional\u003ctorch::stable::Device\u003e device = std::nullopt, std::optional\u003cbool\u003e pin_memory = std::nullopt, bool non_blocking = false, bool copy = false, std::optional\u003ctorch::headeronly::MemoryFormat\u003e memory_format = std::nullopt)# Stable version of the to.dtype_layout op. Converts a tensor to the specified dtype, layout, device, and/or memory format. Returns a new tensor with the specified properties. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. dtype \u2013 Optional target scalar type. layout \u2013 Optional target memory layout. device \u2013 Optional target device. pin_memory \u2013 Optional flag to use pinned memory. non_blocking \u2013 If true, the operation may be asynchronous. Defaults to false. copy \u2013 If true, always create a copy. Defaults to false. memory_format \u2013 Optional target memory format. Returns: A tensor with the specified properties. inline torch::stable::Tensor torch::stable::to(const torch::stable::Tensor \u0026self, torch::stable::Device device, bool non_blocking = false, bool copy = false)# Convenience overload for moving a tensor to a device. Moves the tensor to the specified device. This is a convenience wrapper around the full to() function. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. device \u2013 The target device. non_blocking \u2013 If true, the operation may be asynchronous. Defaults to false. copy \u2013 If true, always create a copy. Defaults to false. Returns: A tensor on the specified device. inline torch::stable::Tensor torch::stable::fill_(const torch::stable::Tensor \u0026self, double value)# Stable version of the fill_.Scalar op. Fills the input tensor with the specified scalar value in-place and returns it. This has identical semantics to the existing fill_.Scalar op. Minimum compatible version: PyTorch 2.9. Note The value parameter is typed as double This is because Scalar.h is currently not header-only. Parameters: self \u2013 The tensor to fill. value \u2013 The scalar value to fill the tensor with. Returns: The input tensor, now filled with the specified value. inline torch::stable::Tensor torch::stable::zero_(torch::stable::Tensor \u0026self)# Stable version of the zero_ op. Fills the input tensor with zeros in-place and returns it. Unlike the tensor method version (t.zero_()), this is called as a function: zero_(t). Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The tensor to fill with zeros. Returns: The input tensor, now filled with zeros. inline torch::stable::Tensor torch::stable::copy_(torch::stable::Tensor \u0026self, const torch::stable::Tensor \u0026src, std::optional\u003cbool\u003e non_blocking = std::nullopt)# Stable version of the copy_ op. Copies the elements from the source tensor into the destination tensor in-place and returns the destination tensor. The tensors must be broadcastable. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The destination tensor (modified in-place). src \u2013 The source tensor to copy from. non_blocking \u2013 If true, the copy may occur asynchronously with respect to the host. Defaults to false. Returns: The destination tensor with copied values. inline torch::stable::Tensor torch::stable::matmul(const torch::stable::Tensor \u0026self, const torch::stable::Tensor \u0026other)# Stable version of the matmul op. Performs matrix multiplication between two tensors. The behavior depends on the dimensionality of the tensors (see PyTorch documentation for details on broadcasting rules for matmul). Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The first input tensor. other \u2013 The second input tensor. Returns: The result of matrix multiplication. inline torch::stable::Tensor torch::stable::amax(const torch::stable::Tensor \u0026self, int64_t dim, bool keepdim = false)# Stable version of the amax.default op (single dimension). Computes the maximum value along the specified dimension. If keepdim is true, the output tensor has the same number of dimensions as the input, with the reduced dimension having size 1. Otherwise, the reduced dimension is removed. Minimum compatible version: PyTorch 2.9. Parameters: self \u2013 The input tensor. dim \u2013 The dimension along which to compute the maximum. keepdim \u2013 Whether to retain Returns: A tensor containing the maximum values along the specified dimension. inline torch::stable::Tensor torch::stable::amax(const torch::stable::Tensor \u0026self, torch::headeronly::IntHeaderOnlyArrayRef dims, bool keepdim = false)# Stable version of the amax.default op (multiple dimensions). Computes the maximum value reducing over all the specified dimensions. If keepdim is true, the output tensor has the same number of dimensions as the input, with the reduced dimensions having size 1. Otherwise, the reduced dimensions are removed. Minimum compatible version: PyTorch 2.9. Note The dims parameter is typed is not yet header-only. Parameters: self \u2013 The input tensor. dims \u2013 The dimensions along which to compute the maximum. keepdim \u2013 Whether to retain the reduced dimensions. Defaults to false. Returns: A tensor containing the maximum values. inline torch::stable::Tensor torch::stable::sum(const torch::stable::Tensor \u0026self, std::optional\u003ctorch::headeronly::IntHeaderOnlyArrayRef\u003e dim = std::nullopt, bool keepdim = false, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt)# Stable version of the sum.dim_IntList op. Computes the sum of the input tensor along the specified dimensions. If dim is not provided, sums over all dimensions. Minimum compatible version: PyTorch 2.10. Parameters: self \u2013 The input tensor. dim \u2013 Optional dimensions to reduce. If not provided, reduces all dimensions. keepdim \u2013 Whether to retain the reduced dimensions. Defaults to false. dtype \u2013 Optional output dtype. If not provided, uses the input dtype. Returns: A tensor containing the sum. inline torch::stable::Tensor \u0026torch::stable::sum_out(torch::stable::Tensor \u0026out, const torch::stable::Tensor \u0026self, std::optional\u003ctorch::headeronly::IntHeaderOnlyArrayRef\u003e dim = std::nullopt, bool keepdim = false, std::optional\u003ctorch::headeronly::ScalarType\u003e dtype = std::nullopt)# Stable version of the sum.IntList_out op. Computes the sum of the input tensor along the specified dimensions, storing the result in the provided output tensor. Following C++ convention, the out parameter comes first. Minimum compatible version: PyTorch 2.10. Parameters: out \u2013 The output tensor (modified in-place). self \u2013 The input tensor. dim \u2013 Optional dimensions to reduce. keepdim \u2013 Whether to retain the reduced dimensions. Defaults to false. dtype \u2013 Optional output dtype. Returns: Reference to the output tensor. inline torch::stable::Tensor torch::stable::subtract(const torch::stable::Tensor \u0026self, const torch::stable::Tensor \u0026other, double alpha = 1.0)# Stable version of the subtract.Tensor op. Subtracts the other tensor from self, with an optional scaling factor alpha. Computes: self - alpha * other. Minimum compatible version: PyTorch 2.10. Note The alpha parameter is typed as double API uses double for the Scalar parameter. Parameters: self \u2013 The input tensor. other \u2013 The tensor to subtract. alpha \u2013 The scaling factor for other. Defaults to 1.0. Returns: The result of self - alpha * other. template\u003cclass F\u003einline void torch::stable::parallel_for(const int64_t begin, const int64_t end, const int64_t grain_size, const F \u0026f)# Stable parallel_for utility. Provides a stable interface to at::parallel_for for parallel execution. The function f will be called with (begin, end) ranges to process in parallel. grain_size controls the minimum work size per thread for efficient parallelization. Minimum compatible version: PyTorch 2.10. Template Parameters: F \u2013 The callable type Parameters: begin \u2013 The start of the iteration range. end \u2013 The end of the iteration range (exclusive). grain_size \u2013 The minimum number of iterations per thread. f \u2013 The function to execute in parallel. inline uint32_t torch::stable::get_num_threads()# Gets the number of threads for the parallel backend. Provides a stable interface to at::get_num_threads. Minimum compatible version: PyTorch 2.10. Returns: The number of threads Parallelization Utilities# template\u003cclass F\u003einline void torch::stable::parallel_for(const int64_t begin, const int64_t end, const int64_t grain_size, const F \u0026f) Stable parallel_for utility. Provides a stable interface to at::parallel_for for parallel execution. The function f will be called with (begin, end) ranges to process in parallel. grain_size controls the minimum work size per thread for efficient parallelization. Minimum compatible version: PyTorch 2.10. Template Parameters: F \u2013 The callable type Parameters: begin \u2013 The start of the iteration range. end \u2013 The end of the iteration range (exclusive). grain_size \u2013 The minimum number of iterations per thread. f \u2013 The function to execute in parallel. inline uint32_t torch::stable::get_num_threads() Gets the number of threads for the parallel backend. Provides a stable interface to at::get_num_threads. Minimum compatible version: PyTorch 2.10. Returns: The number of threads",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/stable.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Tensor CUDA Stream API &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css?v=a885cde7" />
    <link rel="stylesheet" type="text/css" href="../_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js?v=73120307"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js?v=660e4f45"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/tensor_cuda_stream';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tensor Indexing API" href="tensor_indexing.html" />
    <link rel="prev" title="Tensor Creation API" href="tensor_creation.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="inference_mode.html">
    Inference Mode
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" current active">
  <a class="nav-link dropdown-item nav-internal" href="#">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="#">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"></div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Tensor CUDA...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Tensor CUDA Stream API">
        <meta itemprop="position" content="1">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="tensor-cuda-stream-api">
<h1>Tensor CUDA Stream API<a class="headerlink" href="#tensor-cuda-stream-api" title="Link to this heading">#</a></h1>
<p>A <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams">CUDA Stream</a> is a linear sequence of execution that belongs to a specific CUDA device.
The PyTorch C++ API supports CUDA streams with the CUDAStream class and useful helper functions to make streaming operations easy.
You can find them in <a class="reference external" href="https://pytorch.org/cppdocs/api/file_c10_cuda_CUDAStream.h.html#file-c10-cuda-cudastream-h">CUDAStream.h</a>. This note provides more details on how to use Pytorch C++ CUDA Stream APIs.</p>
<section id="acquiring-cuda-stream">
<h2>Acquiring CUDA stream<a class="headerlink" href="#acquiring-cuda-stream" title="Link to this heading">#</a></h2>
<p>Pytorch’s C++ API provides the following ways to acquire CUDA stream:</p>
<ol class="arabic simple">
<li><p>Acquire a new stream from the CUDA stream pool, streams are preallocated from the pool and returned in a round-robin fashion.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getStreamFromPool</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">isHighPriority</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can request a stream from the high priority pool by setting isHighPriority to true, or a stream for a specific device
by setting device index (defaulting to the current CUDA stream’s device index).</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Acquire the default CUDA stream for the passed CUDA device, or for the current device if no device index is passed.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getDefaultCUDAStream</span><span class="p">(</span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The default stream is where most computation occurs when you aren’t explicitly using streams.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Acquire the current CUDA stream, for the CUDA device with index <code class="docutils literal notranslate"><span class="pre">device_index</span></code>, or for the current device if no device index is passed.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getCurrentCUDAStream</span><span class="p">(</span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The current CUDA stream will usually be the default CUDA stream for the device, but it may be different if someone
called <code class="docutils literal notranslate"><span class="pre">setCurrentCUDAStream</span></code> or used <code class="docutils literal notranslate"><span class="pre">StreamGuard</span></code> or <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code>.</p>
</div>
</section>
<section id="set-cuda-stream">
<h2>Set CUDA stream<a class="headerlink" href="#set-cuda-stream" title="Link to this heading">#</a></h2>
<p>Pytorch’s C++ API provides the following ways to set CUDA stream:</p>
<ol class="arabic simple">
<li><p>Set the current stream on the device of the passed in stream to be the passed in stream.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">setCurrentCUDAStream</span><span class="p">(</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>This function may have nothing to do with the current device. It only changes the current stream on the stream’s device.
We recommend using <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code>, instead, since it switches to the stream’s device and makes it the current stream on that device.
<code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code> will also restore the current device and stream when it’s destroyed</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code> to switch to a CUDA stream within a scope, it is defined in <a class="reference external" href="https://pytorch.org/cppdocs/api/structc10_1_1cuda_1_1_c_u_d_a_stream_guard.html">CUDAStreamGuard.h</a></p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">CUDAMultiStreamGuard</span></code> if you need to set streams on multiple CUDA devices.</p>
</div>
</section>
<section id="cuda-stream-usage-examples">
<h2>CUDA Stream Usage Examples<a class="headerlink" href="#cuda-stream-usage-examples" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Acquiring and setting CUDA stream on the same device</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to acquire and set CUDA stream on the same device.</span>
<span class="c1">// `at::cuda::setCurrentCUDAStream` is used to set current CUDA stream</span>

<span class="c1">// create a tensor on device 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span>
<span class="c1">// get a new CUDA stream from CUDA stream pool on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">();</span>
<span class="c1">// set current CUDA stream from default stream to `myStream` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream</span><span class="p">);</span>
<span class="c1">// sum() on tensor0 uses `myStream` as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>

<span class="c1">// get the default CUDA stream on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">defaultStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">();</span>
<span class="c1">// set current CUDA stream back to default CUDA stream on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">defaultStream</span><span class="p">);</span>
<span class="c1">// sum() on tensor0 uses `defaultStream` as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example is the same as previous example, but explicitly specify device</span>
<span class="c1">// index and use CUDA stream guard to set current CUDA stream</span>

<span class="c1">// create a tensor on device 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span>
<span class="c1">// get a new stream from CUDA stream pool on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="c1">// set the current CUDA stream to `myStream` within the scope using CUDA stream guard</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">guard</span><span class="p">(</span><span class="n">myStream</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// current CUDA stream is `myStream` from here till the end of bracket.</span>
<span class="w">  </span><span class="c1">// sum() on tensor0 uses `myStream` as current CUDA stream</span>
<span class="w">  </span><span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="p">}</span>
<span class="c1">// current CUDA stream is reset to default CUDA stream after CUDA stream guard is destroyed</span>
<span class="c1">// sum() on tensor0 uses default CUDA stream on device 0 as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Above code is running on the same CUDA device. <cite>setCurrentCUDAStream</cite> will always set current CUDA stream on current device,
but note that <cite>setCurrentCUDAStream</cite> actually set current stream on the device of passed in CUDA stream.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Acquiring and setting CUDA streams on multiple devices.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to acquire and set CUDA stream on two devices.</span>

<span class="c1">// acquire new CUDA streams from CUDA stream pool on device 0 and device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="c1">// set current CUDA stream to `myStream0` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream0</span><span class="p">);</span>
<span class="c1">// set current CUDA stream to `myStream1` on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream1</span><span class="p">);</span>

<span class="c1">// create a tensor on device 0, no need to specify device index since</span>
<span class="c1">// current device index is 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span>
<span class="c1">// sum() on tensor0 use `myStream0` as current CUDA stream on device 0</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>

<span class="c1">// change the current device index to 1 by using CUDA device guard within a bracket scope</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="n">device_guard</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span>
<span class="w">  </span><span class="c1">// create a tensor on device 1</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span>
<span class="w">  </span><span class="c1">// sum() on tensor 1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// current device is reset to device 0 after device_guard is destroyed</span>

<span class="c1">// acquire a new CUDA stream on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>
<span class="c1">// create a new tensor on device 1</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">}));</span>

<span class="c1">// change the current device index to 1 and current CUDA stream on device 1</span>
<span class="c1">// to `myStream1_1` using CUDA stream guard within a scope</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">stream_guard</span><span class="p">(</span><span class="n">myStream1_1</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// sum() on tensor1 use `myStream1_1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// current device is reset to device 0 and current CUDA stream on device 1 is</span>
<span class="c1">// reset to `myStream1`</span>

<span class="c1">// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Working with CUDA multistream guard</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to use CUDA multistream guard to set</span>
<span class="c1">// two streams on two devices at the same time.</span>

<span class="c1">// create two tensor, one on device 0, one on device 1</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">}));</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">}));</span>

<span class="c1">// acquire new CUDA streams from CUDA stream pool on device 0 and device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span>

<span class="c1">// set current CUDA stream on device 0 to `myStream0` and</span>
<span class="c1">// set current CUDA stream on device 1 to `myStream1` CUDA using multistream guard</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAMultiStreamGuard</span><span class="w"> </span><span class="nf">multi_guard</span><span class="p">({</span><span class="n">myStream0</span><span class="p">,</span><span class="w"> </span><span class="n">myStream1</span><span class="p">});</span>

<span class="w">  </span><span class="c1">// sum() on tensor0 uses `myStream0` as current CUDA stream on device 0</span>
<span class="w">  </span><span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="w">  </span><span class="c1">// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// current CUDA stream on device 0 is reset to default CUDA stream on device 0</span>
<span class="c1">// current CUDA stream on device 1 is reset to default CUDA stream on device 1</span>

<span class="c1">// sum() on tensor0 uses default CUDA stream as current CUDA stream on device 0</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
<span class="c1">// sum() on tensor1 uses default CUDA stream as current CUDA stream on device 1</span>
<span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p><code class="docutils literal notranslate"><span class="pre">CUDAMultiStreamGuard</span></code> does not change current device index, it only changes the stream on
each passed in stream’s device. Other than scope controlling, this guard is equivalent to
calling <code class="docutils literal notranslate"><span class="pre">setCurrentCUDAStream</span></code> on each passed in stream.</p>
</div>
<ol class="arabic simple" start="4">
<li><p>A skeleton example for handling CUDA streams on multiple devices</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This is a skeleton example that shows how to handle CUDA streams on multiple devices</span>
<span class="c1">// Suppose you want to do work on the non-default stream on two devices simultaneously, and we</span>
<span class="c1">// already have streams on both devices in two vectors. The following code shows three ways</span>
<span class="c1">// of acquiring and setting the streams.</span>

<span class="c1">// Usage 0: acquire CUDA stream and set current CUDA stream with `setCurrentCUDAStream`</span>
<span class="c1">// Create a CUDA stream vector `streams0` on device 0</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streams0</span><span class="w"> </span><span class="o">=</span>
<span class="w">  </span><span class="p">{</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">(),</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">()};</span>
<span class="c1">// set current stream as `streams0[0]` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">streams0</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>

<span class="c1">// create a CUDA stream vector `streams1` on device using CUDA device guard</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streams1</span><span class="p">;</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// device index is set to 1 within this scope</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="nf">device_guard</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="n">streams1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">());</span>
<span class="w">  </span><span class="n">streams1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">());</span>
<span class="p">}</span>
<span class="c1">// device index is reset to 0 after device_guard is destroyed</span>

<span class="c1">// set current stream as `streams1[0]` on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">streams1</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>


<span class="c1">// Usage 1: use CUDA device guard to change the current device index only</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="nf">device_guard</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// current device index is changed to 1 within scope</span>
<span class="w">  </span><span class="c1">// current CUDA stream is still `streams1[0]` on device 1, no change</span>
<span class="p">}</span>
<span class="c1">// current device index is reset to 0 after `device_guard` is destroyed</span>


<span class="c1">// Usage 2: use CUDA stream guard to change both current device index and current CUDA stream.</span>
<span class="p">{</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">stream_guard</span><span class="p">(</span><span class="n">streams1</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span>

<span class="w">  </span><span class="c1">// current device index and current CUDA stream are set to 1 and `streams1[1]` within scope</span>
<span class="p">}</span>
<span class="c1">// current device index and current CUDA stream are reset to 0 and `streams0[0]` after</span>
<span class="c1">// stream_guard is destroyed</span>


<span class="c1">// Usage 3: use CUDA multi-stream guard to change multiple streams on multiple devices</span>
<span class="p">{</span>
<span class="w">  </span><span class="c1">// This is the same as calling `torch::cuda::setCurrentCUDAStream` on both streams</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAMultiStreamGuard</span><span class="w"> </span><span class="nf">multi_guard</span><span class="p">({</span><span class="n">streams0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">streams1</span><span class="p">[</span><span class="mi">1</span><span class="p">]});</span>

<span class="w">  </span><span class="c1">// current device index is not change, still 0</span>
<span class="w">  </span><span class="c1">// current CUDA stream on device 0 and device 1 are set to `streams0[1]` and `streams1[1]`</span>
<span class="p">}</span>
<span class="c1">// current CUDA stream on device 0 and device 1 are reset to `streams0[0]` and `streams1[0]`</span>
<span class="c1">// after `multi_guard` is destroyed.</span>
</pre></div>
</div>
</section>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4">★</span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5">★</span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="tensor_creation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tensor Creation API</p>
      </div>
    </a>
    <a class="right-next"
       href="tensor_indexing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tensor Indexing API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="tensor_creation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Tensor Creation API</p>
      </div>
    </a>
    <a class="right-next"
       href="tensor_indexing.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tensor Indexing API</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acquiring-cuda-stream">Acquiring CUDA stream</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-cuda-stream">Set CUDA stream</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-stream-usage-examples">CUDA Stream Usage Examples</a></li>
</ul>
  </nav></div>
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/notes/tensor_cuda_stream.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright © The Linux Foundation®. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Tensor CUDA Stream API",
       "headline": "Tensor CUDA Stream API",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/notes/tensor_cuda_stream.html",
       "articleBody": "Tensor CUDA Stream API# A CUDA Stream is a linear sequence of execution that belongs to a specific CUDA device. The PyTorch C++ API supports CUDA streams with the CUDAStream class and useful helper functions to make streaming operations easy. You can find them in CUDAStream.h. This note provides more details on how to use Pytorch C++ CUDA Stream APIs. Acquiring CUDA stream# Pytorch\u2019s C++ API provides the following ways to acquire CUDA stream: Acquire a new stream from the CUDA stream pool, streams are preallocated from the pool and returned in a round-robin fashion. CUDAStream getStreamFromPool(const bool isHighPriority = false, DeviceIndex device = -1); Tip You can request a stream from the high priority pool by setting isHighPriority to true, or a stream for a specific device by setting device index (defaulting to the current CUDA stream\u2019s device index). Acquire the default CUDA stream for the passed CUDA device, or for the current device if no device index is passed. CUDAStream getDefaultCUDAStream(DeviceIndex device_index = -1); Tip The default stream is where most computation occurs when you aren\u2019t explicitly using streams. Acquire the current CUDA stream, for the CUDA device with index device_index, or for the current device if no device index is passed. CUDAStream getCurrentCUDAStream(DeviceIndex device_index = -1); Tip The current CUDA stream will usually be the default CUDA stream for the device, but it may be different if someone called setCurrentCUDAStream or used StreamGuard or CUDAStreamGuard. Set CUDA stream# Pytorch\u2019s C++ API provides the following ways to set CUDA stream: Set the current stream on the device of the passed in stream to be the passed in stream. void setCurrentCUDAStream(CUDAStream stream); Attention This function may have nothing to do with the current device. It only changes the current stream on the stream\u2019s device. We recommend using CUDAStreamGuard, instead, since it switches to the stream\u2019s device and makes it the current stream on that device. CUDAStreamGuard will also restore the current device and stream when it\u2019s destroyed Use CUDAStreamGuard to switch to a CUDA stream within a scope, it is defined in CUDAStreamGuard.h Tip Use CUDAMultiStreamGuard if you need to set streams on multiple CUDA devices. CUDA Stream Usage Examples# Acquiring and setting CUDA stream on the same device // This example shows how to acquire and set CUDA stream on the same device. // `at::cuda::setCurrentCUDAStream` is used to set current CUDA stream // create a tensor on device 0 torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA)); // get a new CUDA stream from CUDA stream pool on device 0 at::cuda::CUDAStream myStream = at::cuda::getStreamFromPool(); // set current CUDA stream from default stream to `myStream` on device 0 at::cuda::setCurrentCUDAStream(myStream); // sum() on tensor0 uses `myStream` as current CUDA stream tensor0.sum(); // get the default CUDA stream on device 0 at::cuda::CUDAStream defaultStream = at::cuda::getDefaultCUDAStream(); // set current CUDA stream back to default CUDA stream on device 0 at::cuda::setCurrentCUDAStream(defaultStream); // sum() on tensor0 uses `defaultStream` as current CUDA stream tensor0.sum(); // This example is the same as previous example, but explicitly specify device // index and use CUDA stream guard to set current CUDA stream // create a tensor on device 0 torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(torch::kCUDA)); // get a new stream from CUDA stream pool on device 0 at::cuda::CUDAStream myStream = at::cuda::getStreamFromPool(false, 0); // set the current CUDA stream to `myStream` within the scope using CUDA stream guard { at::cuda::CUDAStreamGuard guard(myStream); // current CUDA stream is `myStream` from here till the end of bracket. // sum() on tensor0 uses `myStream` as current CUDA stream tensor0.sum(); } // current CUDA stream is reset to default CUDA stream after CUDA stream guard is destroyed // sum() on tensor0 uses default CUDA stream on device 0 as current CUDA stream tensor0.sum(); Attention Above code is running on the same CUDA device. setCurrentCUDAStream will always set current CUDA stream on current device, but note that setCurrentCUDAStream actually set current stream on the device of passed in CUDA stream. Acquiring and setting CUDA streams on multiple devices. // This example shows how to acquire and set CUDA stream on two devices. // acquire new CUDA streams from CUDA stream pool on device 0 and device 1 at::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0); at::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1); // set current CUDA stream to `myStream0` on device 0 at::cuda::setCurrentCUDAStream(myStream0); // set current CUDA stream to `myStream1` on device 1 at::cuda::setCurrentCUDAStream(myStream1); // create a tensor on device 0, no need to specify device index since // current device index is 0 torch::Tensor tensor0 = torch::ones({2, 2}, torch::device(at::kCUDA)); // sum() on tensor0 use `myStream0` as current CUDA stream on device 0 tensor0.sum(); // change the current device index to 1 by using CUDA device guard within a bracket scope { at::cuda::CUDAGuard device_guard{1}; // create a tensor on device 1 torch::Tensor tensor1 = torch::ones({2, 2}, torch::device(at::kCUDA)); // sum() on tensor 1 uses `myStream1` as current CUDA stream on device 1 tensor1.sum(); } // current device is reset to device 0 after device_guard is destroyed // acquire a new CUDA stream on device 1 at::cuda::CUDAStream myStream1_1 = at::cuda::getStreamFromPool(false, 1); // create a new tensor on device 1 torch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1})); // change the current device index to 1 and current CUDA stream on device 1 // to `myStream1_1` using CUDA stream guard within a scope { at::cuda::CUDAStreamGuard stream_guard(myStream1_1); // sum() on tensor1 use `myStream1_1` as current CUDA stream on device 1 tensor1.sum(); } // current device is reset to device 0 and current CUDA stream on device 1 is // reset to `myStream1` // sum() on tensor1 uses `myStream1` as current CUDA stream on device 1 tensor1.sum(); Working with CUDA multistream guard // This example shows how to use CUDA multistream guard to set // two streams on two devices at the same time. // create two tensor, one on device 0, one on device 1 torch::Tensor tensor0 = torch::ones({2, 2}, torch::device({torch::kCUDA, 0})); torch::Tensor tensor1 = torch::ones({2, 2}, torch::device({torch::kCUDA, 1})); // acquire new CUDA streams from CUDA stream pool on device 0 and device 1 at::cuda::CUDAStream myStream0 = at::cuda::getStreamFromPool(false, 0); at::cuda::CUDAStream myStream1 = at::cuda::getStreamFromPool(false, 1); // set current CUDA stream on device 0 to `myStream0` and // set current CUDA stream on device 1 to `myStream1` CUDA using multistream guard { at::cuda::CUDAMultiStreamGuard multi_guard({myStream0, myStream1}); // sum() on tensor0 uses `myStream0` as current CUDA stream on device 0 tensor0.sum(); // sum() on tensor1 uses `myStream1` as current CUDA stream on device 1 tensor1.sum(); } // current CUDA stream on device 0 is reset to default CUDA stream on device 0 // current CUDA stream on device 1 is reset to default CUDA stream on device 1 // sum() on tensor0 uses default CUDA stream as current CUDA stream on device 0 tensor0.sum(); // sum() on tensor1 uses default CUDA stream as current CUDA stream on device 1 tensor1.sum(); Attention CUDAMultiStreamGuard does not change current device index, it only changes the stream on each passed in stream\u2019s device. Other than scope controlling, this guard is equivalent to calling setCurrentCUDAStream on each passed in stream. A skeleton example for handling CUDA streams on multiple devices // This is a skeleton example that shows how to handle CUDA streams on multiple devices // Suppose you want to do work on the non-default stream on two devices simultaneously, and we // already have streams on both devices in two vectors. The following code shows three ways // of acquiring and setting the streams. // Usage 0: acquire CUDA stream and set current CUDA stream with `setCurrentCUDAStream` // Create a CUDA stream vector `streams0` on device 0 std::vector\u003cat::cuda::CUDAStream\u003e streams0 = {at::cuda::getDefaultCUDAStream(), at::cuda::getStreamFromPool()}; // set current stream as `streams0[0]` on device 0 at::cuda::setCurrentCUDAStream(streams0[0]); // create a CUDA stream vector `streams1` on device using CUDA device guard std::vector\u003cat::cuda::CUDAStream\u003e streams1; { // device index is set to 1 within this scope at::cuda::CUDAGuard device_guard(1); streams1.push_back(at::cuda::getDefaultCUDAStream()); streams1.push_back(at::cuda::getStreamFromPool()); } // device index is reset to 0 after device_guard is destroyed // set current stream as `streams1[0]` on device 1 at::cuda::setCurrentCUDAStream(streams1[0]); // Usage 1: use CUDA device guard to change the current device index only { at::cuda::CUDAGuard device_guard(1); // current device index is changed to 1 within scope // current CUDA stream is still `streams1[0]` on device 1, no change } // current device index is reset to 0 after `device_guard` is destroyed // Usage 2: use CUDA stream guard to change both current device index and current CUDA stream. { at::cuda::CUDAStreamGuard stream_guard(streams1[1]); // current device index and current CUDA stream are set to 1 and `streams1[1]` within scope } // current device index and current CUDA stream are reset to 0 and `streams0[0]` after // stream_guard is destroyed // Usage 3: use CUDA multi-stream guard to change multiple streams on multiple devices { // This is the same as calling `torch::cuda::setCurrentCUDAStream` on both streams at::cuda::CUDAMultiStreamGuard multi_guard({streams0[1], streams1[1]}); // current device index is not change, still 0 // current CUDA stream on device 0 and device 1 are set to `streams0[1]` and `streams1[1]` } // current CUDA stream on device 0 and device 1 are reset to `streams0[0]` and `streams1[0]` // after `multi_guard` is destroyed.",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/notes/tensor_cuda_stream.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>
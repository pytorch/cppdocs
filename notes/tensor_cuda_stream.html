


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tensor CUDA Stream API &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/notes/tensor_cuda_stream.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/cpp_theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tensor Indexing API" href="tensor_indexing.html" />
    <link rel="prev" title="Tensor Creation API" href="tensor_creation.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="inference_mode.html">Inference Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="maybe_owned.html">MaybeOwned&lt;Tensor&gt;</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_creation.html">Tensor Creation API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tensor CUDA Stream API</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_indexing.html">Tensor Indexing API</a></li>
<li class="toctree-l1"><a class="reference internal" href="versioning.html">Library Versioning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Tensor CUDA Stream API</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notes/tensor_cuda_stream.rst.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="tensor-cuda-stream-api">
<h1>Tensor CUDA Stream API<a class="headerlink" href="#tensor-cuda-stream-api" title="Permalink to this headline">Â¶</a></h1>
<p>A <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams">CUDA Stream</a> is a linear sequence of execution that belongs to a specific CUDA device.
The PyTorch C++ API supports CUDA streams with the CUDAStream class and useful helper functions to make streaming operations easy.
You can find them in <a class="reference external" href="https://pytorch.org/cppdocs/api/file_c10_cuda_CUDAStream.h.html#file-c10-cuda-cudastream-h">CUDAStream.h</a>. This note provides more details on how to use Pytorch C++ CUDA Stream APIs.</p>
<div class="section" id="acquiring-cuda-stream">
<h2>Acquiring CUDA stream<a class="headerlink" href="#acquiring-cuda-stream" title="Permalink to this headline">Â¶</a></h2>
<p>Pytorchâ€™s C++ API provides the following ways to acquire CUDA stream:</p>
<ol class="arabic simple">
<li><p>Acquire a new stream from the CUDA stream pool, streams are preallocated from the pool and returned in a round-robin fashion.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getStreamFromPool</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">isHighPriority</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can request a stream from the high priority pool by setting isHighPriority to true, or a stream for a specific device
by setting device index (defaulting to the current CUDA streamâ€™s device index).</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Acquire the default CUDA stream for the passed CUDA device, or for the current device if no device index is passed.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getDefaultCUDAStream</span><span class="p">(</span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The default stream is where most computation occurs when you arenâ€™t explicitly using streams.</p>
</div>
<ol class="arabic simple" start="3">
<li><p>Acquire the current CUDA stream, for the CUDA device with index <code class="docutils literal notranslate"><span class="pre">device_index</span></code>, or for the current device if no device index is passed.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">CUDAStream</span><span class="w"> </span><span class="nf">getCurrentCUDAStream</span><span class="p">(</span><span class="n">DeviceIndex</span><span class="w"> </span><span class="n">device_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-1</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The current CUDA stream will usually be the default CUDA stream for the device, but it may be different if someone
called <code class="docutils literal notranslate"><span class="pre">setCurrentCUDAStream</span></code> or used <code class="docutils literal notranslate"><span class="pre">StreamGuard</span></code> or <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code>.</p>
</div>
</div>
<div class="section" id="set-cuda-stream">
<h2>Set CUDA stream<a class="headerlink" href="#set-cuda-stream" title="Permalink to this headline">Â¶</a></h2>
<p>Pytorchâ€™s C++ API provides the following ways to set CUDA stream:</p>
<ol class="arabic simple">
<li><p>Set the current stream on the device of the passed in stream to be the passed in stream.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">setCurrentCUDAStream</span><span class="p">(</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>This function may have nothing to do with the current device. It only changes the current stream on the streamâ€™s device.
We recommend using <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code>, instead, since it switches to the streamâ€™s device and makes it the current stream on that device.
<code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code> will also restore the current device and stream when itâ€™s destroyed</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">CUDAStreamGuard</span></code> to switch to a CUDA stream within a scope, it is defined in <a class="reference external" href="https://pytorch.org/cppdocs/api/structc10_1_1cuda_1_1_c_u_d_a_stream_guard.html">CUDAStreamGuard.h</a></p></li>
</ol>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">CUDAMultiStreamGuard</span></code> if you need to set streams on multiple CUDA devices.</p>
</div>
</div>
<div class="section" id="cuda-stream-usage-examples">
<h2>CUDA Stream Usage Examples<a class="headerlink" href="#cuda-stream-usage-examples" title="Permalink to this headline">Â¶</a></h2>
<ol class="arabic simple">
<li><p>Acquiring and setting CUDA stream on the same device</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to acquire and set CUDA stream on the same device.</span>
<span class="c1">// `at::cuda::setCurrentCUDAStream` is used to set current CUDA stream</span>

<span class="c1">// create a tensor on device 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span><span class="w"></span>
<span class="c1">// get a new CUDA stream from CUDA stream pool on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">();</span><span class="w"></span>
<span class="c1">// set current CUDA stream from default stream to `myStream` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream</span><span class="p">);</span><span class="w"></span>
<span class="c1">// sum() on tensor0 uses `myStream` as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>

<span class="c1">// get the default CUDA stream on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">defaultStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">();</span><span class="w"></span>
<span class="c1">// set current CUDA stream back to default CUDA stream on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">defaultStream</span><span class="p">);</span><span class="w"></span>
<span class="c1">// sum() on tensor0 uses `defaultStream` as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example is the same as previous example, but explicitly specify device</span>
<span class="c1">// index and use CUDA stream guard to set current CUDA stream</span>

<span class="c1">// create a tensor on device 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span><span class="w"></span>
<span class="c1">// get a new stream from CUDA stream pool on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="c1">// set the current CUDA stream to `myStream` within the scope using CUDA stream guard</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">guard</span><span class="p">(</span><span class="n">myStream</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="c1">// current CUDA stream is `myStream` from here till the end of bracket.</span>
<span class="w">  </span><span class="c1">// sum() on tensor0 uses `myStream` as current CUDA stream</span>
<span class="w">  </span><span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// current CUDA stream is reset to default CUDA stream after CUDA stream guard is destroyed</span>
<span class="c1">// sum() on tensor0 uses default CUDA stream on device 0 as current CUDA stream</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Above code is running on the same CUDA device. <cite>setCurrentCUDAStream</cite> will always set current CUDA stream on current device,
but note that <cite>setCurrentCUDAStream</cite> actually set current stream on the device of passed in CUDA stream.</p>
</div>
<ol class="arabic simple" start="2">
<li><p>Acquiring and setting CUDA streams on multiple devices.</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to acquire and set CUDA stream on two devices.</span>

<span class="c1">// acquire new CUDA streams from CUDA stream pool on device 0 and device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>

<span class="c1">// set current CUDA stream to `myStream0` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream0</span><span class="p">);</span><span class="w"></span>
<span class="c1">// set current CUDA stream to `myStream1` on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">myStream1</span><span class="p">);</span><span class="w"></span>

<span class="c1">// create a tensor on device 0, no need to specify device index since</span>
<span class="c1">// current device index is 0</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span><span class="w"></span>
<span class="c1">// sum() on tensor0 use `myStream0` as current CUDA stream on device 0</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>

<span class="c1">// change the current device index to 1 by using CUDA device guard within a braket scope</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="n">device_guard</span><span class="p">{</span><span class="mi">1</span><span class="p">};</span><span class="w"></span>
<span class="w">  </span><span class="c1">// create a tensor on device 1</span>
<span class="w">  </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="c1">// sum() on tensor 1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// current device is reset to device 0 after device_guard is destroyed</span>

<span class="c1">// acquire a new CUDA stream on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="c1">// create a new tensor on device 1</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">}));</span><span class="w"></span>

<span class="c1">// change the current device index to 1 and current CUDA stream on device 1</span>
<span class="c1">// to `myStream1_1` using CUDA stream guard within a scope</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">stream_guard</span><span class="p">(</span><span class="n">myStream1_1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="c1">// sum() on tensor1 use `myStream1_1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// current device is reset to device 0 and current CUDA stream on device 1 is</span>
<span class="c1">// reset to `myStream1`</span>

<span class="c1">// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Working with CUDA multistream guard</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This example shows how to use CUDA multistream guard to set</span>
<span class="c1">// two streams on two devices at the same time.</span>

<span class="c1">// create two tensor, one on device 0, one on device 1</span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">}));</span><span class="w"></span>
<span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">ones</span><span class="p">({</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">},</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">device</span><span class="p">({</span><span class="n">torch</span><span class="o">::</span><span class="n">kCUDA</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">}));</span><span class="w"></span>

<span class="c1">// acquire new CUDA streams from CUDA stream pool on device 0 and device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="w"> </span><span class="n">myStream1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">);</span><span class="w"></span>

<span class="c1">// set current CUDA stream on device 0 to `myStream0` and</span>
<span class="c1">// set current CUDA stream on device 1 to `myStream1` CUDA using multistream guard</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAMultiStreamGuard</span><span class="w"> </span><span class="nf">multi_guard</span><span class="p">({</span><span class="n">myStream0</span><span class="p">,</span><span class="w"> </span><span class="n">myStream1</span><span class="p">});</span><span class="w"></span>

<span class="w">  </span><span class="c1">// sum() on tensor0 uses `myStream0` as current CUDA stream on device 0</span>
<span class="w">  </span><span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="c1">// sum() on tensor1 uses `myStream1` as current CUDA stream on device 1</span>
<span class="w">  </span><span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// current CUDA stream on device 0 is reset to default CUDA stream on device 0</span>
<span class="c1">// current CUDA stream on device 1 is reset to default CUDA stream on device 1</span>

<span class="c1">// sum() on tensor0 uses default CUDA stream as current CUDA stream on device 0</span>
<span class="n">tensor0</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
<span class="c1">// sum() on tensor1 uses defualt CUDA stream as current CUDA stream on device 1</span>
<span class="n">tensor1</span><span class="p">.</span><span class="n">sum</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p><code class="docutils literal notranslate"><span class="pre">CUDAMultiStreamGuard</span></code> does not change current device index, it only changes the stream on
each passed in streamâ€™s device. Other than scope controlling, this guard is equivalent to
calling <code class="docutils literal notranslate"><span class="pre">setCurrentCUDAStream</span></code> on each passed in stream.</p>
</div>
<ol class="arabic simple" start="4">
<li><p>A skeleton example for handling CUDA streams on multiple devices</p></li>
</ol>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// This is a skeleton example that shows how to handle CUDA streams on multiple devices</span>
<span class="c1">// Suppose you want to do work on the non-default stream on two devices simultaneously, and we</span>
<span class="c1">// already have streams on both devices in two vectors. The following code shows three ways</span>
<span class="c1">// of acquiring and setting the streams.</span>

<span class="c1">// Usage 0: acquire CUDA stream and set current CUDA stream with `setCurrentCUDAStream`</span>
<span class="c1">// Create a CUDA stream vector `streams0` on device 0</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streams0</span><span class="w"> </span><span class="o">=</span><span class="w"></span>
<span class="w">  </span><span class="p">{</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">(),</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">()};</span><span class="w"></span>
<span class="c1">// set current stream as `streams0[0]` on device 0</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">streams0</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>

<span class="c1">// create a CUDA stream vector `streams1` on device using CUDA device guard</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&gt;</span><span class="w"> </span><span class="n">streams1</span><span class="p">;</span><span class="w"></span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// device index is set to 1 within this scope</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="nf">device_guard</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="n">streams1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getDefaultCUDAStream</span><span class="p">());</span><span class="w"></span>
<span class="w">  </span><span class="n">streams1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getStreamFromPool</span><span class="p">());</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// device index is reset to 0 after device_guard is destroyed</span>

<span class="c1">// set current stream as `streams1[0]` on device 1</span>
<span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">setCurrentCUDAStream</span><span class="p">(</span><span class="n">streams1</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>


<span class="c1">// Usage 1: use CUDA device guard to change the current device index only</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAGuard</span><span class="w"> </span><span class="nf">device_guard</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>

<span class="w">  </span><span class="c1">// current device index is changed to 1 within scope</span>
<span class="w">  </span><span class="c1">// current CUDA stream is still `streams1[0]` on device 1, no change</span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// current device index is reset to 0 after `device_guard` is destroyed</span>


<span class="c1">// Usage 2: use CUDA stream guard to change both current device index and current CUDA stream.</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStreamGuard</span><span class="w"> </span><span class="nf">stream_guard</span><span class="p">(</span><span class="n">streams1</span><span class="p">[</span><span class="mi">1</span><span class="p">]);</span><span class="w"></span>

<span class="w">  </span><span class="c1">// current device index and current CUDA stream are set to 1 and `streams1[1]` within scope</span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// current device index and current CUDA stream are reset to 0 and `streams0[0]` after</span>
<span class="c1">// stream_guard is destroyed</span>


<span class="c1">// Usage 3: use CUDA multi-stream guard to change multiple streams on multiple devices</span>
<span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="c1">// This is the same as calling `torch::cuda::setCurrentCUDAStream` on both streams</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAMultiStreamGuard</span><span class="w"> </span><span class="nf">multi_guard</span><span class="p">({</span><span class="n">streams0</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">streams1</span><span class="p">[</span><span class="mi">1</span><span class="p">]});</span><span class="w"></span>

<span class="w">  </span><span class="c1">// current device index is not change, still 0</span>
<span class="w">  </span><span class="c1">// current CUDA stream on device 0 and device 1 are set to `streams0[1]` and `streams1[1]`</span>
<span class="p">}</span><span class="w"></span>
<span class="c1">// current CUDA stream on device 0 and device 1 are reset to `streams0[0]` and `streams1[0]`</span>
<span class="c1">// after `multi_guard` is destroyed.</span>
</pre></div>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensor_indexing.html" class="btn btn-neutral float-right" title="Tensor Indexing API" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="tensor_creation.html" class="btn btn-neutral" title="Tensor Creation API" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Tensor CUDA Stream API</a><ul>
<li><a class="reference internal" href="#acquiring-cuda-stream">Acquiring CUDA stream</a></li>
<li><a class="reference internal" href="#set-cuda-stream">Set CUDA stream</a></li>
<li><a class="reference internal" href="#cuda-stream-usage-examples">CUDA Stream Usage Examples</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>Â© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>

<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Program Listing for File TensorBody.h &#8212; PyTorch main documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=047068a3" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css?v=a885cde7" />
    <link rel="stylesheet" type="text/css" href="../_static/cpp_theme.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=a8da1a53"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js?v=73120307"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js?v=660e4f45"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'api/program_listing_file_build_aten_src_ATen_core_TensorBody.h';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="File TensorOptions.h" href="file_aten_src_ATen_TensorOptions.h.html" />
    <link rel="prev" title="File TensorBody.h" href="file_build_aten_src_ATen_core_TensorBody.h.html" />

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
<script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/2.3.1/list.min.js"></script>
<script>
  if (window.location.hostname === 'docs.pytorch.org' || window.location.hostname === 'docs-preview.pytorch.org') {
    const script = document.createElement('script');
    script.src = 'https://cmp.osano.com/16A0DbT9yDNIaQkvZ/31b1b91a-e0b6-47ea-bde2-7f2bd13dbe5c/osano.js?variant=one';
    document.head.appendChild(script);
  }
</script>
<script>
  // Cookie banner for non-LF projects
  document.addEventListener('DOMContentLoaded', function () {
    // Hide cookie banner on local environments and LF owned docs
    if (window.location.hostname === 'localhost' ||
      window.location.hostname === '0.0.0.0' ||
      window.location.hostname === '127.0.0.1' ||
      window.location.hostname === 'docs.pytorch.org' ||
      window.location.hostname === 'docs-preview.pytorch.org' ||
      window.location.hostname.startsWith('192.168.')) {
      const banner = document.querySelector('.cookie-banner-wrapper');
      if (banner) {
        banner.style.display = 'none';
      }
    }
  });
</script>
<!-- Conditional CSS for header and footer height adjustment -->


<link rel="stylesheet" type="text/css" href="../_static/css/theme.css" crossorigin="anonymous">
<script type="text/javascript" src="../_static/js/theme.js"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400&display=swap" rel="stylesheet">
<meta property="og:image" content="https://docs.pytorch.org/docs/stable/_static/img/pytorch_seo.png" />
<link rel="stylesheet" href="../_static/webfonts/all.min.css" crossorigin="anonymous">
<meta http-equiv="Content-Security-Policy"
  content="default-src * 'unsafe-inline' 'unsafe-eval' data: blob:; style-src * 'unsafe-inline'; script-src * 'unsafe-inline' 'unsafe-eval' blob:;">
<meta name="pytorch_project" content="">
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS" height="0" width="0"
    style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<!-- Google Tag Manager -->
<script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    j.onload = function () {
      window.dispatchEvent(new Event('gtm_loaded'));
      console.log('GTM loaded successfully');
    };
  })(window, document, 'script', 'dataLayer', 'GTM-T8XT4PS');
</script>
<!-- End Google Tag Manager -->
<!-- Facebook Pixel Code -->
<script>
  !function (f, b, e, v, n, t, s) {
    if (f.fbq) return; n = f.fbq = function () {
      n.callMethod ?
        n.callMethod.apply(n, arguments) : n.queue.push(arguments)
    };
    if (!f._fbq) f._fbq = n; n.push = n; n.loaded = !0; n.version = '2.0';
    n.queue = []; t = b.createElement(e); t.async = !0;
    t.src = v; s = b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t, s)
  }(window, document, 'script',
    'https://connect.facebook.net/en_US/fbevents.js');
  fbq('init', '243028289693773');
  fbq('track', 'PageView');
</script>
<script>
  document.documentElement.setAttribute('data-version', 'main');
</script>
<noscript>
  <img height="1" width="1" src="https://www.facebook.com/tr?id=243028289693773&ev=PageView&noscript=1" />
</noscript>
<script>
  function gtag() {
    window.dataLayer.push(arguments);
  }
</script>
<!-- End Facebook Pixel Code -->
<!-- Repository configuration for tutorials -->

<!-- Script to Fix scrolling -->
<script>
  document.addEventListener('DOMContentLoaded', function () {
    // Fix anchor scrolling
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetElement = document.getElementById(targetId);

        if (targetElement) {
          const headerHeight =
            (document.querySelector('.header-holder') ? document.querySelector('.header-holder').offsetHeight : 0) +
            (document.querySelector('.bd-header') ? document.querySelector('.bd-header').offsetHeight : 0) + 20;

          const targetPosition = targetElement.getBoundingClientRect().top + window.pageYOffset - headerHeight;
          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update URL hash without scrolling
          history.pushState(null, null, '#' + targetId);
        }
      });
    });
  });
</script>

<script async src="https://cse.google.com/cse.js?cx=e65585f8c3ea1440e"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  </head>

<body data-feedback-url="https://github.com/pytorch/pytorch" class="pytorch-body">
  
    <div class="container-fluid header-holder tutorials-header" id="header-holder">
   <div class="header-container-wrapper">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                <span>Learn</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started/locally">
                  <span class=dropdown-title>Get Started</span>
                </a>
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/webinars/">
                  <span class="dropdown-title">Webinars</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Community</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://landscape.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Landscape</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/join-ecosystem">
                  <span class="dropdown-title">Join the Ecosystem</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-hub/">
                  <span class="dropdown-title">Community Hub</span>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/">
                  <span class="dropdown-title">Forums</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contributor-awards/">
                  <span class="dropdown-title">Contributor Awards</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-events/">
                  <span class="dropdown-title">Community Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/programs/ambassadors/">
                  <span class="dropdown-title">PyTorch Ambassadors</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Projects</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/pytorch/">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/vllm/">
                  <span class="dropdown-title">vLLM</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/deepspeed/">
                  <span class="dropdown-title">DeepSpeed</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/host-your-project/">
                  <span class="dropdown-title">Host Your Project</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/projects/ray/">
                  <span class="dropdown-title">RAY</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span> Docs</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://docs.pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/domains">
                  <span class="dropdown-title">Domains</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>Blogs & News</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">Blog</span>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/announcements">
                  <span class="dropdown-title">Announcements</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/case-studies/">
                  <span class="dropdown-title">Case Studies</span>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                </a>
            </div>
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
              <span>About</span>
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/members">
                  <span class="dropdown-title">Members</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact">
                  <span class="dropdown-title">Contact</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/wp-content/uploads/2025/09/pytorch_brand_guide_091925a.pdf">
                  <span class="dropdown-title">Brand Guidelines</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown main-menu-button">
              <a href="https://pytorch.org/join" data-cta="join">
                JOIN
              </a>
            </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu">
        <i class="fa-solid fa-ellipsis"></i>
      </a>
    </div>
  </div>
 </div>

 <!-- Begin Mobile Menu -->

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="header-container-wrapper">
      <div class="mobile-main-menu-header-container">
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu">
        </a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">
    <div class="main-menu">
      <ul>
         <li class="resources-mobile-menu-title">
           <a>Learn</a>
         </li>
         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/get-started/locally">Get Started</a>
           </li>
           <li>
             <a href="https://docs.pytorch.org/tutorials">Tutorials</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
           </li>
           <li>
             <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
           </li>
           <li>
            <a href="https://pytorch.org/webinars/">Webinars</a>
          </li>
        </ul>
         <li class="resources-mobile-menu-title">
           <a>Community</a>
         </li>
         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://landscape.pytorch.org/">Landscape</a>
          </li>
          <li>
             <a href="https://pytorch.org/join-ecosystem">Join the Ecosystem</a>
           </li>
           <li>
             <a href="https://pytorch.org/community-hub/">Community Hub</a>
           </li>
           <li>
             <a href="https://discuss.pytorch.org/">Forums</a>
           </li>
           <li>
             <a href="https://pytorch.org/resources">Developer Resources</a>
           </li>
           <li>
             <a href="https://pytorch.org/contributor-awards/">Contributor Awards</a>
           </li>
           <li>
            <a href="https://pytorch.org/community-events/">Community Events</a>
          </li>
          <li>
            <a href="https://pytorch.org/programs/ambassadors/">PyTorch Ambassadors</a>
          </li>
       </ul>

         <li class="resources-mobile-menu-title">
           <a>Projects</a>
         </li>

         <ul class="resources-mobile-menu-items">
           <li>
             <a href="https://pytorch.org/projects/pytorch/">PyTorch</a>
           </li>

           <li>
             <a href="https://pytorch.org/projects/vllm/">vLLM</a>
           </li>
           <li>
            <a href="https://pytorch.org/projects/deepspeed/">DeepSpeed</a>
          </li>
          <li>
             <a href="https://pytorch.org/projects/host-your-project/">Host Your Project</a>
           </li>
         </ul>

         <li class="resources-mobile-menu-title">
           <a>Docs</a>
         </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://docs.pytorch.org/docs/stable/index.html">PyTorch</a>
          </li>

          <li>
            <a href="https://pytorch.org/domains">Domains</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Blog & News</a>
        </li>

         <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>
          <li>
            <a href="https://pytorch.org/announcements">Announcements</a>
          </li>

          <li>
            <a href="https://pytorch.org/case-studies/">Case Studies</a>
          </li>
          <li>
            <a href="https://pytorch.org/events">Events</a>
          </li>
          <li>
             <a href="https://pytorch.org/newsletter">Newsletter</a>
           </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>About</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
          </li>
          <li>
            <a href="https://pytorch.org/members">Members</a>
          </li>
          <li>
            <a href="https://pytorch.org/governing-board">Governing Board</a>
          </li>
          <li>
            <a href="https://pytorch.org/tac">Technical Advisory Council</a>
         </li>
         <li>
             <a href="https://pytorch.org/credits">Cloud Credit Program</a>
          </li>
          <li>
             <a href="https://pytorch.org/staff">Staff</a>
          </li>
          <li>
             <a href="https://pytorch.org/contact">Contact</a>
          </li>
        </ul>
      </ul>
    </div>
  </div>
</div>

<!-- End Mobile Menu -->
  
  
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  <a href="../index.html" class="version">main</a>
</div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../stable.html">
    Torch Stable API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
      
        <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installing.html">
    Installing C++ Distributions of PyTorch
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../frontend.html">
    The C++ Frontend
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../stable.html">
    Torch Stable API
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="library_root.html">
    Library API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/inference_mode.html">
    Inference Mode
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/maybe_owned.html">
    MaybeOwned<Tensor>
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_basics.html">
    Tensor Basics
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_creation.html">
    Tensor Creation API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_cuda_stream.html">
    Tensor CUDA Stream API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/tensor_indexing.html">
    Tensor Indexing API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../notes/versioning.html">
    Library Versioning
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
  
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
</div>
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://x.com/PyTorch" title="X" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-x-twitter fa-lg" aria-hidden="true"></i>
            <span class="sr-only">X</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/pytorch/pytorch" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discuss.pytorch.org/" title="PyTorch Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyTorch Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/torch/" title="PyPi" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPi</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_at.html">Namespace at</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_at__detail.html">Namespace at::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_at__indexing.html">Namespace at::indexing</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_at__native.html">Namespace at::native</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_at__symint.html">Namespace at::symint</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10.html">Namespace c10</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__cuda.html">Namespace c10::cuda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__detail.html">Namespace c10::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__detail_.html">Namespace c10::detail_</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__ivalue.html">Namespace c10::ivalue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__WarningUtils.html">Namespace c10::WarningUtils</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_c10__xpu.html">Namespace c10::xpu</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_caffe2.html">Namespace caffe2</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_std.html">Namespace std</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch.html">Namespace torch</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__autograd.html">Namespace torch::autograd</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__autograd__detail.html">Namespace torch::autograd::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__autograd__forward_ad.html">Namespace torch::autograd::forward_ad</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__cuda.html">Namespace torch::cuda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data.html">Namespace torch::data</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__datasets.html">Namespace torch::data::datasets</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__datasets__detail.html">Namespace torch::data::datasets::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__detail.html">Namespace torch::data::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__detail__sequencers.html">Namespace torch::data::detail::sequencers</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__detail__sequencers__detail.html">Namespace torch::data::detail::sequencers::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__example.html">Namespace torch::data::example</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__samplers.html">Namespace torch::data::samplers</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__data__transforms.html">Namespace torch::data::transforms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__detail.html">Namespace torch::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__enumtype.html">Namespace torch::enumtype</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__fft.html">Namespace torch::fft</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__jit.html">Namespace torch::jit</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__jit__detail.html">Namespace torch::jit::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__jit__script.html">Namespace torch::jit::script</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__mps.html">Namespace torch::mps</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nativert.html">Namespace torch::nativert</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nested.html">Namespace torch::nested</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn.html">Namespace torch::nn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__%40150.html">Namespace torch::nn::@150</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__detail.html">Namespace torch::nn::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__functional.html">Namespace torch::nn::functional</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__functions.html">Namespace torch::nn::functions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__init.html">Namespace torch::nn::init</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__modules.html">Namespace torch::nn::modules</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__modules__utils.html">Namespace torch::nn::modules::utils</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__parallel.html">Namespace torch::nn::parallel</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__utils.html">Namespace torch::nn::utils</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__nn__utils__rnn.html">Namespace torch::nn::utils::rnn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__optim.html">Namespace torch::optim</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__optim__detail.html">Namespace torch::optim::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__python.html">Namespace torch::python</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__python__detail.html">Namespace torch::python::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__serialize.html">Namespace torch::serialize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__special.html">Namespace torch::special</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__stable.html">Namespace torch::stable</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__stable__accelerator.html">Namespace torch::stable::accelerator</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__stable__accelerator__%40196.html">Namespace torch::stable::accelerator::@196</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__stable__detail.html">Namespace torch::stable::detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="namespace_torch__xpu.html">Namespace torch::xpu</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_activation_descriptor.html">Struct ActivationDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_convolution_descriptor.html">Struct ConvolutionDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_c_t_c_loss_descriptor.html">Struct CTCLossDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_descriptor_deleter.html">Template Struct DescriptorDeleter</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_dfti_descriptor_deleter.html">Struct DftiDescriptorDeleter</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_dropout_descriptor.html">Struct DropoutDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_r_n_n_descriptor.html">Struct RNNDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structat_1_1native_1_1_spatial_transformer_descriptor.html">Struct SpatialTransformerDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_capsule.html">Struct Capsule</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1cuda_1_1_c_u_d_a_guard.html">Struct CUDAGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1cuda_1_1_c_u_d_a_multi_stream_guard.html">Struct CUDAMultiStreamGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1cuda_1_1_c_u_d_a_stream_guard.html">Struct CUDAStreamGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1cuda_1_1_optional_c_u_d_a_guard.html">Struct OptionalCUDAGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1cuda_1_1_optional_c_u_d_a_stream_guard.html">Struct OptionalCUDAStreamGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_device.html">Struct Device</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_exclusively_owned_traits_3_01at_1_1_tensor_01_4.html">Template Struct ExclusivelyOwnedTraits&lt; at::Tensor &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value.html">Struct IValue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value_1_1_comp_aliased_i_values.html">Struct IValue::CompAliasedIValues</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value_1_1_comp_identity_i_values.html">Struct IValue::CompIdentityIValues</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1ivalue_1_1_complex_holder.html">Struct ComplexHolder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value_1_1_hash_aliased_i_value.html">Struct IValue::HashAliasedIValue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value_1_1_hash_identity_i_value.html">Struct IValue::HashIdentityIValue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1ivalue_1_1_stream_data3_holder.html">Struct StreamData3Holder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_i_value_1_1_tag_type.html">Template Struct IValue::TagType</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_maybe_owned_traits_3_01at_1_1_tensor_01_4.html">Template Struct MaybeOwnedTraits&lt; at::Tensor &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_optional_array.html">Template Struct OptionalArray</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_strong_type_ptr.html">Struct StrongTypePtr</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_warning_utils_1_1_warn_always.html">Struct WarnAlways</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_weak_i_value.html">Struct WeakIValue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_weak_or_strong_compilation_unit.html">Struct WeakOrStrongCompilationUnit</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_weak_or_strong_type_ptr.html">Struct WeakOrStrongTypePtr</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structc10_1_1_weak_type_ptr.html">Struct WeakTypePtr</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_autograd_context.html">Struct AutogradContext</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_cpp_node.html">Template Struct CppNode</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1detail_1_1_make_next_function_list.html">Struct MakeNextFunctionList</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_extract_variables.html">Struct ExtractVariables</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_function.html">Template Struct Function</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_node.html">Struct Node</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_node_1_1undefined__input.html">Struct Node::undefined_input</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_traceable_function.html">Struct TraceableFunction</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1autograd_1_1_type_and_size.html">Struct TypeAndSize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_data_loader_base_1_1_job.html">Struct DataLoaderBase::Job</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_data_loader_base_1_1_quit_worker.html">Struct DataLoaderBase::QuitWorker</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_data_loader_base_1_1_result.html">Struct DataLoaderBase::Result</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_data_loader_base_1_1_sequenced.html">Struct DataLoaderBase::Sequenced</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_data_loader_options.html">Struct DataLoaderOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1datasets_1_1_chunk_dataset_options.html">Struct ChunkDatasetOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1datasets_1_1detail_1_1_batch_data_buffer_1_1_unwrapped_batch_data.html">Struct BatchDataBuffer::UnwrappedBatchData</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1datasets_1_1detail_1_1is__optional.html">Template Struct is_optional</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1datasets_1_1_tensor_dataset.html">Struct TensorDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1_iterator_impl.html">Template Struct IteratorImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1_sentinel_iterator.html">Template Struct SentinelIterator</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1sequencers_1_1_no_sequencer.html">Template Struct NoSequencer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1sequencers_1_1_ordered_sequencer.html">Template Struct OrderedSequencer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1sequencers_1_1_sequencer.html">Template Struct Sequencer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1detail_1_1_valid_iterator.html">Template Struct ValidIterator</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_example.html">Template Struct Example</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_example_3_01_data_00_01example_1_1_no_target_01_4.html">Template Struct Example&lt; Data, example::NoTarget &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_full_data_loader_options.html">Struct FullDataLoaderOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1samplers_1_1_batch_size.html">Struct BatchSize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1samplers_1_1_custom_batch_request.html">Struct CustomBatchRequest</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1transforms_1_1_normalize.html">Template Struct Normalize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1transforms_1_1_stack.html">Template Struct Stack</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1transforms_1_1_stack_3_01_example_3_4_01_4.html">Template Struct Stack&lt; Example&lt;  &gt; &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1transforms_1_1_stack_3_01_tensor_example_01_4.html">Template Struct Stack&lt; TensorExample &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1data_1_1_worker_exception.html">Struct WorkerException</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1__compute__enum__name.html">Struct _compute_enum_name</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_area.html">Struct kArea</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_batch_mean.html">Struct kBatchMean</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_bicubic.html">Struct kBicubic</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_bilinear.html">Struct kBilinear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_border.html">Struct kBorder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_circular.html">Struct kCircular</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_constant.html">Struct kConstant</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv1_d.html">Struct kConv1D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv2_d.html">Struct kConv2D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv3_d.html">Struct kConv3D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv_transpose1_d.html">Struct kConvTranspose1D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv_transpose2_d.html">Struct kConvTranspose2D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_conv_transpose3_d.html">Struct kConvTranspose3D</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_fan_in.html">Struct kFanIn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_fan_out.html">Struct kFanOut</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_g_e_l_u.html">Struct kGELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_g_r_u.html">Struct kGRU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_leaky_re_l_u.html">Struct kLeakyReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_linear.html">Struct kLinear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_l_s_t_m.html">Struct kLSTM</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_max.html">Struct kMax</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_mean.html">Struct kMean</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_mish.html">Struct kMish</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_nearest.html">Struct kNearest</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_nearest_exact.html">Struct kNearestExact</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_none.html">Struct kNone</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_reflect.html">Struct kReflect</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_reflection.html">Struct kReflection</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_re_l_u.html">Struct kReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_replicate.html">Struct kReplicate</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_r_n_n___r_e_l_u.html">Struct kRNN_RELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_r_n_n___t_a_n_h.html">Struct kRNN_TANH</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_same.html">Struct kSame</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_sigmoid.html">Struct kSigmoid</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_si_l_u.html">Struct kSiLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_sum.html">Struct kSum</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_tanh.html">Struct kTanh</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_trilinear.html">Struct kTrilinear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_valid.html">Struct kValid</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1enumtype_1_1k_zeros.html">Struct kZeros</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1_init_lambda.html">Template Struct InitLambda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_attribute_policy.html">Struct AttributePolicy</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_buffer_policy.html">Struct BufferPolicy</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_module_policy.html">Struct ModulePolicy</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_named_policy.html">Template Struct NamedPolicy</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_parameter_policy.html">Struct ParameterPolicy</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1detail_1_1_slot_cursor.html">Struct SlotCursor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1_module.html">Struct Module</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1_named.html">Template Struct Named</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1_register_operators.html">Struct RegisterOperators</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1slot__iterator__impl.html">Template Struct slot_iterator_impl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1jit_1_1slot__list__impl.html">Template Struct slot_list_impl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_adaptive_avg_pool_options.html">Template Struct AdaptiveAvgPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_adaptive_log_softmax_with_loss_options.html">Struct AdaptiveLogSoftmaxWithLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_adaptive_max_pool_options.html">Template Struct AdaptiveMaxPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_module_holder.html">Template Struct AnyModuleHolder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_module_holder_1_1_checked_getter.html">Struct AnyModuleHolder::CheckedGetter</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_module_holder_1_1_invoke_forward.html">Struct AnyModuleHolder::InvokeForward</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_module_placeholder.html">Struct AnyModulePlaceholder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_value_1_1_holder.html">Template Struct AnyValue::Holder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_any_value_1_1_placeholder.html">Struct AnyValue::Placeholder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_a_s_moutput.html">Struct ASMoutput</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_avg_pool_options.html">Template Struct AvgPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_batch_norm_options.html">Struct BatchNormOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_b_c_e_loss_impl.html">Struct BCELossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_b_c_e_loss_options.html">Struct BCELossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_b_c_e_with_logits_loss_impl.html">Struct BCEWithLogitsLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_b_c_e_with_logits_loss_options.html">Struct BCEWithLogitsLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_bilinear_options.html">Struct BilinearOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_c_e_l_u_options.html">Struct CELUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_constant_pad_options.html">Template Struct ConstantPadOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_conv_options.html">Template Struct ConvOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_conv_transpose_options.html">Template Struct ConvTransposeOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cosine_embedding_loss_impl.html">Struct CosineEmbeddingLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cosine_embedding_loss_options.html">Struct CosineEmbeddingLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cosine_similarity_options.html">Struct CosineSimilarityOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cross_entropy_loss_impl.html">Struct CrossEntropyLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cross_entropy_loss_options.html">Struct CrossEntropyLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_cross_map_l_r_n2d_options.html">Struct CrossMapLRN2dOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_c_t_c_loss_impl.html">Struct CTCLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_c_t_c_loss_options.html">Struct CTCLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1detail_1_1_conv_nd_options.html">Template Struct ConvNdOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1detail_1_1_r_n_n_cell_options_base.html">Struct RNNCellOptionsBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1detail_1_1_r_n_n_options_base.html">Struct RNNOptionsBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_dropout_options.html">Struct DropoutOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_e_l_u_options.html">Struct ELUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_embedding_bag_from_pretrained_options.html">Struct EmbeddingBagFromPretrainedOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_embedding_bag_options.html">Struct EmbeddingBagOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_embedding_from_pretrained_options.html">Struct EmbeddingFromPretrainedOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_embedding_options.html">Struct EmbeddingOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_flatten_options.html">Struct FlattenOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_fold_options.html">Struct FoldOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_fractional_max_pool_options.html">Template Struct FractionalMaxPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_alpha_dropout_func_options.html">Struct AlphaDropoutFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_batch_norm_func_options.html">Struct BatchNormFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_conv_func_options.html">Template Struct ConvFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_conv_transpose_func_options.html">Template Struct ConvTransposeFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_dropout_func_options.html">Struct DropoutFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_embedding_bag_func_options.html">Struct EmbeddingBagFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_embedding_func_options.html">Struct EmbeddingFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_feature_alpha_dropout_func_options.html">Struct FeatureAlphaDropoutFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_grid_sample_func_options.html">Struct GridSampleFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_group_norm_func_options.html">Struct GroupNormFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_gumbel_softmax_func_options.html">Struct GumbelSoftmaxFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_instance_norm_func_options.html">Struct InstanceNormFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_interpolate_func_options.html">Struct InterpolateFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_layer_norm_func_options.html">Struct LayerNormFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_log_softmax_func_options.html">Struct LogSoftmaxFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_max_unpool_func_options.html">Template Struct MaxUnpoolFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_multihead_attention_forward_func_options.html">Struct MultiheadAttentionForwardFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_normalize_func_options.html">Struct NormalizeFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_pad_func_options.html">Struct PadFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_r_re_l_u_func_options.html">Struct RReLUFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_softmax_func_options.html">Struct SoftmaxFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1functional_1_1_softmin_func_options.html">Struct SoftminFuncOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_g_e_l_u_options.html">Struct GELUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_g_l_u_options.html">Struct GLUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_group_norm_options.html">Struct GroupNormOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_g_r_u_cell_options.html">Struct GRUCellOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_g_r_u_options.html">Struct GRUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_hardshrink_options.html">Struct HardshrinkOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_hardtanh_options.html">Struct HardtanhOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_hinge_embedding_loss_impl.html">Struct HingeEmbeddingLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_hinge_embedding_loss_options.html">Struct HingeEmbeddingLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_huber_loss_impl.html">Struct HuberLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_huber_loss_options.html">Struct HuberLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_instance_norm_options.html">Struct InstanceNormOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_k_l_div_loss_impl.html">Struct KLDivLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_k_l_div_loss_options.html">Struct KLDivLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_l1_loss_impl.html">Struct L1LossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_l1_loss_options.html">Struct L1LossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_layer_norm_options.html">Struct LayerNormOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_leaky_re_l_u_options.html">Struct LeakyReLUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_linear_options.html">Struct LinearOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_local_response_norm_options.html">Struct LocalResponseNormOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_log_softmax_options.html">Struct LogSoftmaxOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_l_p_pool_options.html">Template Struct LPPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_l_s_t_m_cell_options.html">Struct LSTMCellOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_l_s_t_m_options.html">Struct LSTMOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_margin_ranking_loss_impl.html">Struct MarginRankingLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_margin_ranking_loss_options.html">Struct MarginRankingLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_max_pool_options.html">Template Struct MaxPoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_max_unpool_options.html">Template Struct MaxUnpoolOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_m_s_e_loss_impl.html">Struct MSELossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_m_s_e_loss_options.html">Struct MSELossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multihead_attention_options.html">Struct MultiheadAttentionOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_label_margin_loss_impl.html">Struct MultiLabelMarginLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_label_margin_loss_options.html">Struct MultiLabelMarginLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_label_soft_margin_loss_impl.html">Struct MultiLabelSoftMarginLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_label_soft_margin_loss_options.html">Struct MultiLabelSoftMarginLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_margin_loss_impl.html">Struct MultiMarginLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_multi_margin_loss_options.html">Struct MultiMarginLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_n_l_l_loss_impl.html">Struct NLLLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_n_l_l_loss_options.html">Struct NLLLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_pairwise_distance_options.html">Struct PairwiseDistanceOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_pixel_shuffle_impl.html">Struct PixelShuffleImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_pixel_shuffle_options.html">Struct PixelShuffleOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_pixel_unshuffle_impl.html">Struct PixelUnshuffleImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_pixel_unshuffle_options.html">Struct PixelUnshuffleOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_poisson_n_l_l_loss_impl.html">Struct PoissonNLLLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_poisson_n_l_l_loss_options.html">Struct PoissonNLLLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_p_re_l_u_options.html">Struct PReLUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_reflection_pad_options.html">Template Struct ReflectionPadOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_re_l_u6_options.html">Struct ReLU6Options</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_re_l_u_options.html">Struct ReLUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_replication_pad_options.html">Template Struct ReplicationPadOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_r_n_n_cell_options.html">Struct RNNCellOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_r_n_n_options.html">Struct RNNOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_r_re_l_u_options.html">Struct RReLUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_s_e_l_u_options.html">Struct SELUOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_smooth_l1_loss_impl.html">Struct SmoothL1LossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_smooth_l1_loss_options.html">Struct SmoothL1LossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_soft_margin_loss_impl.html">Struct SoftMarginLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_soft_margin_loss_options.html">Struct SoftMarginLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_softmax_options.html">Struct SoftmaxOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_softmin_options.html">Struct SoftminOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_softplus_options.html">Struct SoftplusOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_softshrink_options.html">Struct SoftshrinkOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_threshold_options.html">Struct ThresholdOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_transformer_decoder_layer_options.html">Struct TransformerDecoderLayerOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_transformer_decoder_options.html">Struct TransformerDecoderOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_transformer_encoder_layer_options.html">Struct TransformerEncoderLayerOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_transformer_encoder_options.html">Struct TransformerEncoderOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_transformer_options.html">Struct TransformerOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_triplet_margin_loss_impl.html">Struct TripletMarginLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_triplet_margin_loss_options.html">Struct TripletMarginLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_triplet_margin_with_distance_loss_impl.html">Struct TripletMarginWithDistanceLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_triplet_margin_with_distance_loss_options.html">Struct TripletMarginWithDistanceLossOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_unflatten_options.html">Struct UnflattenOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_unfold_options.html">Struct UnfoldOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_upsample_options.html">Struct UpsampleOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1nn_1_1_zero_pad_options.html">Template Struct ZeroPadOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adagrad_options.html">Struct AdagradOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adagrad_param_state.html">Struct AdagradParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adam_options.html">Struct AdamOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adam_param_state.html">Struct AdamParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adam_w_options.html">Struct AdamWOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_adam_w_param_state.html">Struct AdamWParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_l_b_f_g_s_options.html">Struct LBFGSOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_l_b_f_g_s_param_state.html">Struct LBFGSParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_r_m_sprop_options.html">Struct RMSpropOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_r_m_sprop_param_state.html">Struct RMSpropParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_s_g_d_options.html">Struct SGDOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1optim_1_1_s_g_d_param_state.html">Struct SGDParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1boxer.html">Template Struct boxer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1boxer__impl.html">Template Struct boxer_impl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1boxer__impl_3_01_return_type_00_01torch_1_1headeronly_1_1gutsa0940d39d9dd89f80a80e036730770c0.html">Template Struct boxer_impl&lt; ReturnType, torch::headeronly::guts::typelist::typelist&lt; ParameterTypes &gt;, FuncT, func &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1boxer__impl_3_01void_00_01torch_1_1headeronly_1_1guts_1_1type99181efc5fb2ad5a7b9ab621d41cde72.html">Template Struct boxer_impl&lt; void, torch::headeronly::guts::typelist::typelist&lt; ParameterTypes &gt;, FuncT, func &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1_unbox_type.html">Template Struct UnboxType</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="structtorch_1_1stable_1_1detail_1_1_unbox_type_3_01torch_1_1headeronly_1_1_header_only_array_ref_3_01_t_01_4_01_4.html">Template Struct UnboxType&lt; torch::headeronly::HeaderOnlyArrayRef&lt; T &gt; &gt;</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1native_1_1_descriptor.html">Template Class Descriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1native_1_1_dfti_descriptor.html">Class DftiDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1native_1_1_filter_descriptor.html">Class FilterDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1native_1_1_r_n_n_data_descriptor.html">Class RNNDataDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1native_1_1_tensor_descriptor.html">Class TensorDescriptor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1_optional_tensor_ref.html">Class OptionalTensorRef</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1_tensor.html">Class Tensor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classat_1_1_tensor_ref.html">Class TensorRef</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_accelerator_error.html">Class AcceleratorError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_array_ref.html">Template Class ArrayRef</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_buffer_error.html">Class BufferError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1cuda_1_1_c_u_d_a_stream.html">Class CUDAStream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dict.html">Template Class Dict</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dist_backend_error.html">Class DistBackendError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dist_error.html">Class DistError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dist_network_error.html">Class DistNetworkError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dist_queue_empty_error.html">Class DistQueueEmptyError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_dist_store_error.html">Class DistStoreError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_enforce_finite_error.html">Class EnforceFiniteError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_error.html">Class Error</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_error_always_show_cpp_stacktrace.html">Class ErrorAlwaysShowCppStacktrace</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_i_list_ref.html">Template Class IListRef</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_index_error.html">Class IndexError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_lin_alg_error.html">Class LinAlgError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_list.html">Template Class List</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_not_implemented_error.html">Class NotImplementedError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_onnxfi_backend_system_error.html">Class OnnxfiBackendSystemError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_optional_array_ref.html">Template Class OptionalArrayRef</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_out_of_memory_error.html">Class OutOfMemoryError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_syntax_error.html">Class SyntaxError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_type_error.html">Class TypeError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_value_error.html">Class ValueError</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_warning.html">Class Warning</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_warning_1_1_deprecation_warning.html">Class Warning::DeprecationWarning</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_warning_1_1_user_warning.html">Class Warning::UserWarning</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_warning_handler.html">Class WarningHandler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1_warning_utils_1_1_warning_handler_guard.html">Class WarningHandlerGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classc10_1_1xpu_1_1_x_p_u_stream.html">Class XPUStream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1autograd_1_1_node_guard.html">Class NodeGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1class__.html">Template Class class_</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_cpp_function.html">Class CppFunction</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_custom_class_holder.html">Class CustomClassHolder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1_data_loader_base.html">Template Class DataLoaderBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_batch_dataset.html">Template Class BatchDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_chunk_data_reader.html">Template Class ChunkDataReader</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_chunk_dataset.html">Template Class ChunkDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_dataset.html">Template Class Dataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1detail_1_1_batch_data_buffer.html">Template Class BatchDataBuffer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_map_dataset.html">Template Class MapDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_m_n_i_s_t.html">Class MNIST</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_shared_batch_dataset.html">Template Class SharedBatchDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1datasets_1_1_stateful_dataset.html">Template Class StatefulDataset</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1detail_1_1_data_shuttle.html">Template Class DataShuttle</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1detail_1_1_queue.html">Template Class Queue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1_iterator.html">Template Class Iterator</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_distributed_random_sampler.html">Class DistributedRandomSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_distributed_sampler.html">Template Class DistributedSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_distributed_sequential_sampler.html">Class DistributedSequentialSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_random_sampler.html">Class RandomSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_sampler.html">Template Class Sampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_sequential_sampler.html">Class SequentialSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1samplers_1_1_stream_sampler.html">Class StreamSampler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1_stateful_data_loader.html">Template Class StatefulDataLoader</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1_stateless_data_loader.html">Template Class StatelessDataLoader</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_batch_lambda.html">Template Class BatchLambda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_batch_transform.html">Template Class BatchTransform</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_lambda.html">Template Class Lambda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_tensor_lambda.html">Template Class TensorLambda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_tensor_transform.html">Template Class TensorTransform</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1data_1_1transforms_1_1_transform.html">Template Class Transform</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1detail_1_1_class_not_selected.html">Class ClassNotSelected</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1detail_1_1_selective_str.html">Template Class SelectiveStr</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1detail_1_1_torch_library_init.html">Class TorchLibraryInit</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_expanding_array.html">Template Class ExpandingArray</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_expanding_array_with_optional_elem.html">Template Class ExpandingArrayWithOptionalElem</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_i_method.html">Class IMethod</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_library.html">Class Library</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nativert_1_1_model_runner_handle.html">Class ModelRunnerHandle</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool1d.html">Class AdaptiveAvgPool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool1d_impl.html">Class AdaptiveAvgPool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool2d.html">Class AdaptiveAvgPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool2d_impl.html">Class AdaptiveAvgPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool3d.html">Class AdaptiveAvgPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool3d_impl.html">Class AdaptiveAvgPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_avg_pool_impl.html">Template Class AdaptiveAvgPoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_log_softmax_with_loss.html">Class AdaptiveLogSoftmaxWithLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_log_softmax_with_loss_impl.html">Class AdaptiveLogSoftmaxWithLossImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool1d.html">Class AdaptiveMaxPool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool1d_impl.html">Class AdaptiveMaxPool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool2d.html">Class AdaptiveMaxPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool2d_impl.html">Class AdaptiveMaxPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool3d.html">Class AdaptiveMaxPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool3d_impl.html">Class AdaptiveMaxPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_adaptive_max_pool_impl.html">Template Class AdaptiveMaxPoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_alpha_dropout.html">Class AlphaDropout</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_alpha_dropout_impl.html">Class AlphaDropoutImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_any_module.html">Class AnyModule</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_any_value.html">Class AnyValue</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool1d.html">Class AvgPool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool1d_impl.html">Class AvgPool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool2d.html">Class AvgPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool2d_impl.html">Class AvgPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool3d.html">Class AvgPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool3d_impl.html">Class AvgPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_avg_pool_impl.html">Template Class AvgPoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm1d.html">Class BatchNorm1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm1d_impl.html">Class BatchNorm1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm2d.html">Class BatchNorm2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm2d_impl.html">Class BatchNorm2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm3d.html">Class BatchNorm3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm3d_impl.html">Class BatchNorm3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_batch_norm_impl_base.html">Template Class BatchNormImplBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_b_c_e_loss.html">Class BCELoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_b_c_e_with_logits_loss.html">Class BCEWithLogitsLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_bilinear.html">Class Bilinear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_bilinear_impl.html">Class BilinearImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_c_e_l_u.html">Class CELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_c_e_l_u_impl.html">Class CELUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cloneable.html">Template Class Cloneable</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad1d.html">Class ConstantPad1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad1d_impl.html">Class ConstantPad1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad2d.html">Class ConstantPad2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad2d_impl.html">Class ConstantPad2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad3d.html">Class ConstantPad3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad3d_impl.html">Class ConstantPad3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_constant_pad_impl.html">Template Class ConstantPadImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv1d.html">Class Conv1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv1d_impl.html">Class Conv1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv2d.html">Class Conv2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv2d_impl.html">Class Conv2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv3d.html">Class Conv3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv3d_impl.html">Class Conv3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_nd_impl.html">Template Class ConvNdImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose1d.html">Class ConvTranspose1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose1d_impl.html">Class ConvTranspose1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose2d.html">Class ConvTranspose2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose2d_impl.html">Class ConvTranspose2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose3d.html">Class ConvTranspose3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose3d_impl.html">Class ConvTranspose3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_conv_transpose_nd_impl.html">Template Class ConvTransposeNdImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cosine_embedding_loss.html">Class CosineEmbeddingLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cosine_similarity.html">Class CosineSimilarity</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cosine_similarity_impl.html">Class CosineSimilarityImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cross_entropy_loss.html">Class CrossEntropyLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cross_map_l_r_n2d.html">Class CrossMapLRN2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_cross_map_l_r_n2d_impl.html">Class CrossMapLRN2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_c_t_c_loss.html">Class CTCLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1detail_1_1___dropout_nd.html">Template Class _DropoutNd</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_cell_impl_base.html">Template Class RNNCellImplBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1detail_1_1_r_n_n_impl_base.html">Template Class RNNImplBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout.html">Class Dropout</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout2d.html">Class Dropout2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout2d_impl.html">Class Dropout2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout3d.html">Class Dropout3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout3d_impl.html">Class Dropout3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_dropout_impl.html">Class DropoutImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_e_l_u.html">Class ELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_e_l_u_impl.html">Class ELUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_embedding.html">Class Embedding</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_embedding_bag.html">Class EmbeddingBag</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_embedding_bag_impl.html">Class EmbeddingBagImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_embedding_impl.html">Class EmbeddingImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_feature_alpha_dropout.html">Class FeatureAlphaDropout</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_feature_alpha_dropout_impl.html">Class FeatureAlphaDropoutImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_flatten.html">Class Flatten</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_flatten_impl.html">Class FlattenImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fold.html">Class Fold</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fold_impl.html">Class FoldImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fractional_max_pool2d.html">Class FractionalMaxPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fractional_max_pool2d_impl.html">Class FractionalMaxPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fractional_max_pool3d.html">Class FractionalMaxPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_fractional_max_pool3d_impl.html">Class FractionalMaxPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_functional.html">Class Functional</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_functional_impl.html">Class FunctionalImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1functions_1_1_cross_map_l_r_n2d.html">Class CrossMapLRN2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_e_l_u.html">Class GELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_e_l_u_impl.html">Class GELUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_l_u.html">Class GLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_l_u_impl.html">Class GLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_group_norm.html">Class GroupNorm</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_group_norm_impl.html">Class GroupNormImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_r_u.html">Class GRU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_r_u_cell.html">Class GRUCell</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_r_u_cell_impl.html">Class GRUCellImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_g_r_u_impl.html">Class GRUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_hardshrink.html">Class Hardshrink</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_hardshrink_impl.html">Class HardshrinkImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_hardtanh.html">Class Hardtanh</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_hardtanh_impl.html">Class HardtanhImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_hinge_embedding_loss.html">Class HingeEmbeddingLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_huber_loss.html">Class HuberLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_identity.html">Class Identity</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_identity_impl.html">Class IdentityImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm1d.html">Class InstanceNorm1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm1d_impl.html">Class InstanceNorm1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm2d.html">Class InstanceNorm2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm2d_impl.html">Class InstanceNorm2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm3d.html">Class InstanceNorm3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm3d_impl.html">Class InstanceNorm3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_instance_norm_impl.html">Template Class InstanceNormImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_k_l_div_loss.html">Class KLDivLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l1_loss.html">Class L1Loss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_layer_norm.html">Class LayerNorm</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_layer_norm_impl.html">Class LayerNormImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_leaky_re_l_u.html">Class LeakyReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_leaky_re_l_u_impl.html">Class LeakyReLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_linear.html">Class Linear</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_linear_impl.html">Class LinearImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_local_response_norm.html">Class LocalResponseNorm</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_local_response_norm_impl.html">Class LocalResponseNormImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_log_sigmoid.html">Class LogSigmoid</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_log_sigmoid_impl.html">Class LogSigmoidImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_log_softmax.html">Class LogSoftmax</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_log_softmax_impl.html">Class LogSoftmaxImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool1d.html">Class LPPool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool1d_impl.html">Class LPPool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool2d.html">Class LPPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool2d_impl.html">Class LPPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool3d.html">Class LPPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool3d_impl.html">Class LPPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_p_pool_impl.html">Template Class LPPoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_s_t_m.html">Class LSTM</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_s_t_m_cell.html">Class LSTMCell</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_s_t_m_cell_impl.html">Class LSTMCellImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_l_s_t_m_impl.html">Class LSTMImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_margin_ranking_loss.html">Class MarginRankingLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool1d.html">Class MaxPool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool1d_impl.html">Class MaxPool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool2d.html">Class MaxPool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool2d_impl.html">Class MaxPool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool3d.html">Class MaxPool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool3d_impl.html">Class MaxPool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_pool_impl.html">Template Class MaxPoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool1d.html">Class MaxUnpool1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool1d_impl.html">Class MaxUnpool1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool2d.html">Class MaxUnpool2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool2d_impl.html">Class MaxUnpool2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool3d.html">Class MaxUnpool3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool3d_impl.html">Class MaxUnpool3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_max_unpool_impl.html">Template Class MaxUnpoolImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_mish.html">Class Mish</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_mish_impl.html">Class MishImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module.html">Class Module</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module_dict.html">Class ModuleDict</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module_dict_impl.html">Class ModuleDictImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module_holder.html">Template Class ModuleHolder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module_list.html">Class ModuleList</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_module_list_impl.html">Class ModuleListImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_m_s_e_loss.html">Class MSELoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_multihead_attention.html">Class MultiheadAttention</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_multihead_attention_impl.html">Class MultiheadAttentionImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_multi_label_margin_loss.html">Class MultiLabelMarginLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_multi_label_soft_margin_loss.html">Class MultiLabelSoftMarginLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_multi_margin_loss.html">Class MultiMarginLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_named_any_module.html">Class NamedAnyModule</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_n_l_l_loss.html">Class NLLLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_norm_impl_base.html">Template Class NormImplBase</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_pairwise_distance.html">Class PairwiseDistance</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_pairwise_distance_impl.html">Class PairwiseDistanceImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_parameter_dict.html">Class ParameterDict</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_parameter_dict_impl.html">Class ParameterDictImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_parameter_list.html">Class ParameterList</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_parameter_list_impl.html">Class ParameterListImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_pixel_shuffle.html">Class PixelShuffle</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_pixel_unshuffle.html">Class PixelUnshuffle</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_poisson_n_l_l_loss.html">Class PoissonNLLLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_p_re_l_u.html">Class PReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_p_re_l_u_impl.html">Class PReLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad1d.html">Class ReflectionPad1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad1d_impl.html">Class ReflectionPad1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad2d.html">Class ReflectionPad2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad2d_impl.html">Class ReflectionPad2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad3d.html">Class ReflectionPad3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad3d_impl.html">Class ReflectionPad3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_reflection_pad_impl.html">Template Class ReflectionPadImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_re_l_u.html">Class ReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_re_l_u6.html">Class ReLU6</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_re_l_u6_impl.html">Class ReLU6Impl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_re_l_u_impl.html">Class ReLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad1d.html">Class ReplicationPad1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad1d_impl.html">Class ReplicationPad1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad2d.html">Class ReplicationPad2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad2d_impl.html">Class ReplicationPad2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad3d.html">Class ReplicationPad3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad3d_impl.html">Class ReplicationPad3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_replication_pad_impl.html">Template Class ReplicationPadImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_n_n.html">Class RNN</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_n_n_cell.html">Class RNNCell</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_n_n_cell_impl.html">Class RNNCellImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_n_n_impl.html">Class RNNImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_re_l_u.html">Class RReLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_r_re_l_u_impl.html">Class RReLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_s_e_l_u.html">Class SELU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_s_e_l_u_impl.html">Class SELUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_sequential.html">Class Sequential</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_sequential_impl.html">Class SequentialImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_sigmoid.html">Class Sigmoid</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_sigmoid_impl.html">Class SigmoidImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_si_l_u.html">Class SiLU</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_si_l_u_impl.html">Class SiLUImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_smooth_l1_loss.html">Class SmoothL1Loss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_soft_margin_loss.html">Class SoftMarginLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmax.html">Class Softmax</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmax2d.html">Class Softmax2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmax2d_impl.html">Class Softmax2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmax_impl.html">Class SoftmaxImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmin.html">Class Softmin</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softmin_impl.html">Class SoftminImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softplus.html">Class Softplus</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softplus_impl.html">Class SoftplusImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softshrink.html">Class Softshrink</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softshrink_impl.html">Class SoftshrinkImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softsign.html">Class Softsign</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_softsign_impl.html">Class SoftsignImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_tanh.html">Class Tanh</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_tanh_impl.html">Class TanhImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_tanhshrink.html">Class Tanhshrink</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_tanhshrink_impl.html">Class TanhshrinkImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_threshold.html">Class Threshold</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_threshold_impl.html">Class ThresholdImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer.html">Class Transformer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_decoder.html">Class TransformerDecoder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_decoder_impl.html">Class TransformerDecoderImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_decoder_layer.html">Class TransformerDecoderLayer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_decoder_layer_impl.html">Class TransformerDecoderLayerImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_encoder.html">Class TransformerEncoder</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_encoder_impl.html">Class TransformerEncoderImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_encoder_layer.html">Class TransformerEncoderLayer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_encoder_layer_impl.html">Class TransformerEncoderLayerImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_transformer_impl.html">Class TransformerImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_triplet_margin_loss.html">Class TripletMarginLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_triplet_margin_with_distance_loss.html">Class TripletMarginWithDistanceLoss</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_unflatten.html">Class Unflatten</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_unflatten_impl.html">Class UnflattenImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_unfold.html">Class Unfold</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_unfold_impl.html">Class UnfoldImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_upsample.html">Class Upsample</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_upsample_impl.html">Class UpsampleImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1utils_1_1rnn_1_1_packed_sequence.html">Class PackedSequence</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad1d.html">Class ZeroPad1d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad1d_impl.html">Class ZeroPad1dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad2d.html">Class ZeroPad2d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad2d_impl.html">Class ZeroPad2dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad3d.html">Class ZeroPad3d</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad3d_impl.html">Class ZeroPad3dImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1nn_1_1_zero_pad_impl.html">Template Class ZeroPadImpl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_adagrad.html">Class Adagrad</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_adam.html">Class Adam</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_adam_w.html">Class AdamW</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_l_b_f_g_s.html">Class LBFGS</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_l_r_scheduler.html">Class LRScheduler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer.html">Class Optimizer</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer_cloneable_options.html">Template Class OptimizerCloneableOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer_cloneable_param_state.html">Template Class OptimizerCloneableParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer_options.html">Class OptimizerOptions</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer_param_group.html">Class OptimizerParamGroup</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_optimizer_param_state.html">Class OptimizerParamState</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_reduce_l_r_on_plateau_scheduler.html">Class ReduceLROnPlateauScheduler</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_r_m_sprop.html">Class RMSprop</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_s_g_d.html">Class SGD</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1optim_1_1_step_l_r.html">Class StepLR</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_ordered_dict.html">Template Class OrderedDict</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1_ordered_dict_1_1_item.html">Class OrderedDict::Item</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1serialize_1_1_input_archive.html">Class InputArchive</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1serialize_1_1_output_archive.html">Class OutputArchive</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1accelerator_1_1_device_guard.html">Class DeviceGuard</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1accelerator_1_1_stream.html">Class Stream</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1detail_1_1_stable_library.html">Class StableLibrary</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1detail_1_1_stable_torch_library_init.html">Class StableTorchLibraryInit</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1_device.html">Class Device</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="classtorch_1_1stable_1_1_tensor.html">Class Tensor</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unionat_1_1native_1_1_constant.html">Union Constant</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unionc10_1_1_i_value_1_1_payload.html">Union IValue::Payload</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="unionc10_1_1_i_value_1_1_payload_1_1_trivially_copyable_payload.html">Union TriviallyCopyablePayload</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten.html">Directory aten</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src.html">Directory src</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen.html">Directory ATen</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen_core.html">Directory core</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen_cuda.html">Directory cuda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen_cudnn.html">Directory cudnn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen_mkl.html">Directory mkl</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_aten_src_ATen_native.html">Directory native</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_build.html">Directory build</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_build_aten.html">Directory aten</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_build_aten_src.html">Directory src</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_build_aten_src_ATen.html">Directory ATen</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_build_aten_src_ATen_core.html">Directory core</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_c10.html">Directory c10</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_c10_core.html">Directory core</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_c10_cuda.html">Directory cuda</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_c10_util.html">Directory util</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_c10_xpu.html">Directory xpu</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch.html">Directory torch</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc.html">Directory csrc</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api.html">Directory api</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include.html">Directory include</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch.html">Directory torch</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data.html">Directory data</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data_dataloader.html">Directory dataloader</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data_datasets.html">Directory datasets</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data_detail.html">Directory detail</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data_samplers.html">Directory samplers</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_data_transforms.html">Directory transforms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nativert.html">Directory nativert</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn.html">Directory nn</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_functional.html">Directory functional</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_modules.html">Directory modules</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_modules_container.html">Directory container</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_options.html">Directory options</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_parallel.html">Directory parallel</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_nn_utils.html">Directory utils</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_optim.html">Directory optim</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_optim_schedulers.html">Directory schedulers</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_python.html">Directory python</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_api_include_torch_serialize.html">Directory serialize</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_autograd.html">Directory autograd</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_autograd_generated.html">Directory generated</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_jit.html">Directory jit</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_jit_api.html">Directory api</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_jit_runtime.html">Directory runtime</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_jit_serialization.html">Directory serialization</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dir_torch_csrc_stable.html">Directory stable</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules__functions.h.html">File _functions.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules__functions.h.html">Program Listing for File _functions.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_accelerator.h.html">File accelerator.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_accelerator.h.html">Program Listing for File accelerator.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_activation.h.html">File activation.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_activation.h.html">Program Listing for File activation.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_activation.h.html">File activation.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_activation.h.html">Program Listing for File activation.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_activation.h.html">File activation.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_activation.h.html">Program Listing for File activation.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_adagrad.h.html">File adagrad.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_adagrad.h.html">Program Listing for File adagrad.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_adam.h.html">File adam.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_adam.h.html">Program Listing for File adam.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_adamw.h.html">File adamw.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_adamw.h.html">Program Listing for File adamw.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_adaptive.h.html">File adaptive.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_adaptive.h.html">Program Listing for File adaptive.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_adaptive.h.html">File adaptive.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_adaptive.h.html">Program Listing for File adaptive.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_all.h.html">File all.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_all.h.html">Program Listing for File all.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_any.h.html">File any.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_any.h.html">Program Listing for File any.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_any_module_holder.h.html">File any_module_holder.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_any_module_holder.h.html">Program Listing for File any_module_holder.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_any_value.h.html">File any_value.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_any_value.h.html">Program Listing for File any_value.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_serialize_archive.h.html">File archive.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_serialize_archive.h.html">Program Listing for File archive.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_arg.h.html">File arg.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_arg.h.html">Program Listing for File arg.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_util_ArrayRef.h.html">File ArrayRef.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_util_ArrayRef.h.html">Program Listing for File ArrayRef.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_ATen.h.html">File ATen.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_ATen.h.html">Program Listing for File ATen.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_autograd.h.html">File autograd.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_autograd.h.html">Program Listing for File autograd.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_autograd_autograd.h.html">File autograd.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_autograd_autograd.h.html">Program Listing for File autograd.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_Backend.h.html">File Backend.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_Backend.h.html">Program Listing for File Backend.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_dataloader_base.h.html">File base.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_dataloader_base.h.html">Program Listing for File base.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_base.h.html">File base.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_base.h.html">Program Listing for File base.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_base.h.html">File base.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_base.h.html">Program Listing for File base.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms_base.h.html">File base.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms_base.h.html">Program Listing for File base.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_batchnorm.h.html">File batchnorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_batchnorm.h.html">Program Listing for File batchnorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_batchnorm.h.html">File batchnorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_batchnorm.h.html">Program Listing for File batchnorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_batchnorm.h.html">File batchnorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_batchnorm.h.html">Program Listing for File batchnorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_chunk.h.html">File chunk.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_chunk.h.html">Program Listing for File chunk.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_utils_clip_grad.h.html">File clip_grad.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_utils_clip_grad.h.html">Program Listing for File clip_grad.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_cloneable.h.html">File cloneable.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_cloneable.h.html">Program Listing for File cloneable.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms_collate.h.html">File collate.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms_collate.h.html">Program Listing for File collate.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_common.h.html">File common.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_common.h.html">Program Listing for File common.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_conv.h.html">File conv.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_conv.h.html">Program Listing for File conv.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_conv.h.html">File conv.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_conv.h.html">Program Listing for File conv.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_conv.h.html">File conv.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_conv.h.html">Program Listing for File conv.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_utils_convert_parameters.h.html">File convert_parameters.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_utils_convert_parameters.h.html">Program Listing for File convert_parameters.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_cuda.h.html">File cuda.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_cuda.h.html">Program Listing for File cuda.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_cuda_CUDAContext.h.html">File CUDAContext.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_cuda_CUDAContext.h.html">Program Listing for File CUDAContext.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_cuda_CUDAGuard.h.html">File CUDAGuard.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_cuda_CUDAGuard.h.html">Program Listing for File CUDAGuard.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_cuda_CUDAStream.h.html">File CUDAStream.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_cuda_CUDAStream.h.html">Program Listing for File CUDAStream.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_custom_batch_request.h.html">File custom_batch_request.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_custom_batch_request.h.html">Program Listing for File custom_batch_request.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_custom_class.h.html">File custom_class.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_custom_class.h.html">Program Listing for File custom_class.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_autograd_custom_function.h.html">File custom_function.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_autograd_custom_function.h.html">Program Listing for File custom_function.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_jit_runtime_custom_operator.h.html">File custom_operator.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_jit_runtime_custom_operator.h.html">Program Listing for File custom_operator.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data.h.html">File data.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data.h.html">Program Listing for File data.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_parallel_data_parallel.h.html">File data_parallel.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_parallel_data_parallel.h.html">Program Listing for File data_parallel.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_detail_data_shuttle.h.html">File data_shuttle.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_detail_data_shuttle.h.html">Program Listing for File data_shuttle.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_dataloader.h.html">File dataloader.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_dataloader.h.html">Program Listing for File dataloader.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_dataloader_options.h.html">File dataloader_options.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_dataloader_options.h.html">Program Listing for File dataloader_options.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets.h.html">File datasets.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets.h.html">Program Listing for File datasets.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_cudnn_Descriptors.h.html">File Descriptors.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_cudnn_Descriptors.h.html">Program Listing for File Descriptors.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_mkl_Descriptors.h.html">File Descriptors.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_mkl_Descriptors.h.html">Program Listing for File Descriptors.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_core_Device.h.html">File Device.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_core_Device.h.html">Program Listing for File Device.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_device_struct.h.html">File device_struct.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_device_struct.h.html">Program Listing for File device_struct.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_DeviceGuard.h.html">File DeviceGuard.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_DeviceGuard.h.html">Program Listing for File DeviceGuard.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_core_DeviceType.h.html">File DeviceType.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_core_DeviceType.h.html">Program Listing for File DeviceType.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_distance.h.html">File distance.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_distance.h.html">Program Listing for File distance.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_distance.h.html">File distance.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_distance.h.html">Program Listing for File distance.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_distance.h.html">File distance.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_distance.h.html">Program Listing for File distance.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_distributed.h.html">File distributed.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_distributed.h.html">Program Listing for File distributed.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_dropout.h.html">File dropout.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_dropout.h.html">Program Listing for File dropout.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_dropout.h.html">File dropout.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_dropout.h.html">Program Listing for File dropout.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_dropout.h.html">File dropout.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_dropout.h.html">Program Listing for File dropout.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_embedding.h.html">File embedding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_embedding.h.html">Program Listing for File embedding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_embedding.h.html">File embedding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_embedding.h.html">Program Listing for File embedding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_embedding.h.html">File embedding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_embedding.h.html">Program Listing for File embedding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_enum.h.html">File enum.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_enum.h.html">Program Listing for File enum.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_example.h.html">File example.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_example.h.html">Program Listing for File example.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_util_Exception.h.html">File Exception.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_util_Exception.h.html">Program Listing for File Exception.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_expanding_array.h.html">File expanding_array.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_expanding_array.h.html">Program Listing for File expanding_array.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_fft.h.html">File fft.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_fft.h.html">Program Listing for File fft.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_fold.h.html">File fold.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_fold.h.html">Program Listing for File fold.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_fold.h.html">File fold.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_fold.h.html">Program Listing for File fold.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_fold.h.html">File fold.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_fold.h.html">Program Listing for File fold.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_autograd_function.h.html">File function.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_autograd_function.h.html">Program Listing for File function.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional.h.html">File functional.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional.h.html">Program Listing for File functional.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_functional.h.html">File functional.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_functional.h.html">Program Listing for File functional.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_build_aten_src_ATen_Functions.h.html">File Functions.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_build_aten_src_ATen_Functions.h.html">Program Listing for File Functions.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_util_Half.h.html">File Half.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_util_Half.h.html">Program Listing for File Half.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_cudnn_Handles.h.html">File Handles.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_cudnn_Handles.h.html">Program Listing for File Handles.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_imethod.h.html">File imethod.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_imethod.h.html">Program Listing for File imethod.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_jit_serialization_import.h.html">File import.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_jit_serialization_import.h.html">Program Listing for File import.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_init.h.html">File init.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_init.h.html">Program Listing for File init.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_python_init.h.html">File init.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_python_init.h.html">Program Listing for File init.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_serialize_input-archive.h.html">File input-archive.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_serialize_input-archive.h.html">Program Listing for File input-archive.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_instancenorm.h.html">File instancenorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_instancenorm.h.html">Program Listing for File instancenorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_instancenorm.h.html">File instancenorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_instancenorm.h.html">Program Listing for File instancenorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_instancenorm.h.html">File instancenorm.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_instancenorm.h.html">Program Listing for File instancenorm.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_iterator.h.html">File iterator.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_iterator.h.html">Program Listing for File iterator.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_core_ivalue.h.html">File ivalue.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_core_ivalue.h.html">Program Listing for File ivalue.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_jit.h.html">File jit.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_jit.h.html">Program Listing for File jit.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms_lambda.h.html">File lambda.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms_lambda.h.html">Program Listing for File lambda.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_Layout.h.html">File Layout.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_Layout.h.html">Program Listing for File Layout.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_lbfgs.h.html">File lbfgs.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_lbfgs.h.html">Program Listing for File lbfgs.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_library.h.html">File library.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_library.h.html">Program Listing for File library.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_library.h.html">File library.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_library.h.html">Program Listing for File library.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_linear.h.html">File linear.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_linear.h.html">Program Listing for File linear.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_linear.h.html">File linear.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_linear.h.html">Program Listing for File linear.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_linear.h.html">File linear.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_linear.h.html">Program Listing for File linear.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_loss.h.html">File loss.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_loss.h.html">Program Listing for File loss.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_loss.h.html">File loss.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_loss.h.html">Program Listing for File loss.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_loss.h.html">File loss.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_loss.h.html">Program Listing for File loss.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_schedulers_lr_scheduler.h.html">File lr_scheduler.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_schedulers_lr_scheduler.h.html">Program Listing for File lr_scheduler.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_macros.h.html">File macros.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_macros.h.html">Program Listing for File macros.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_map.h.html">File map.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_map.h.html">Program Listing for File map.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_mnist.h.html">File mnist.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_mnist.h.html">Program Listing for File mnist.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nativert_ModelRunnerHandle.h.html">File ModelRunnerHandle.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nativert_ModelRunnerHandle.h.html">Program Listing for File ModelRunnerHandle.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_module.h.html">File module.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_module.h.html">Program Listing for File module.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_jit_api_module.h.html">File module.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_jit_api_module.h.html">Program Listing for File module.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_moduledict.h.html">File moduledict.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_moduledict.h.html">Program Listing for File moduledict.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_modulelist.h.html">File modulelist.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_modulelist.h.html">Program Listing for File modulelist.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules.h.html">File modules.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules.h.html">Program Listing for File modules.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_mps.h.html">File mps.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_mps.h.html">Program Listing for File mps.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_named_any.h.html">File named_any.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_named_any.h.html">Program Listing for File named_any.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nested.h.html">File nested.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nested.h.html">Program Listing for File nested.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn.h.html">File nn.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn.h.html">Program Listing for File nn.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_normalization.h.html">File normalization.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_normalization.h.html">Program Listing for File normalization.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_normalization.h.html">File normalization.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_normalization.h.html">Program Listing for File normalization.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_normalization.h.html">File normalization.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_normalization.h.html">Program Listing for File normalization.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_ops.h.html">File ops.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_ops.h.html">Program Listing for File ops.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim.h.html">File optim.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim.h.html">Program Listing for File optim.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_optimizer.h.html">File optimizer.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_optimizer.h.html">Program Listing for File optimizer.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_util_Optional.h.html">File Optional.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_util_Optional.h.html">Program Listing for File Optional.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_util_OptionalArrayRef.h.html">File OptionalArrayRef.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_util_OptionalArrayRef.h.html">Program Listing for File OptionalArrayRef.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options.h.html">File options.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options.h.html">Program Listing for File options.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_ordered_dict.h.html">File ordered_dict.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_ordered_dict.h.html">Program Listing for File ordered_dict.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_serialize_output-archive.h.html">File output-archive.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_serialize_output-archive.h.html">Program Listing for File output-archive.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_padding.h.html">File padding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_padding.h.html">Program Listing for File padding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_padding.h.html">File padding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_padding.h.html">Program Listing for File padding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_padding.h.html">File padding.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_padding.h.html">Program Listing for File padding.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_parameterdict.h.html">File parameterdict.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_parameterdict.h.html">Program Listing for File parameterdict.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_parameterlist.h.html">File parameterlist.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_parameterlist.h.html">Program Listing for File parameterlist.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_pimpl.h.html">File pimpl.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_pimpl.h.html">Program Listing for File pimpl.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_pixelshuffle.h.html">File pixelshuffle.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_pixelshuffle.h.html">Program Listing for File pixelshuffle.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_pixelshuffle.h.html">File pixelshuffle.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_pixelshuffle.h.html">Program Listing for File pixelshuffle.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_pixelshuffle.h.html">File pixelshuffle.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_pixelshuffle.h.html">Program Listing for File pixelshuffle.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_pooling.h.html">File pooling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_pooling.h.html">Program Listing for File pooling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_pooling.h.html">File pooling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_pooling.h.html">Program Listing for File pooling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_pooling.h.html">File pooling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_pooling.h.html">Program Listing for File pooling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_python.h.html">File python.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_python.h.html">Program Listing for File python.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_detail_queue.h.html">File queue.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_detail_queue.h.html">Program Listing for File queue.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_random.h.html">File random.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_random.h.html">Program Listing for File random.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_schedulers_reduce_on_plateau_scheduler.h.html">File reduce_on_plateau_scheduler.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_schedulers_reduce_on_plateau_scheduler.h.html">Program Listing for File reduce_on_plateau_scheduler.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_rmsprop.h.html">File rmsprop.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_rmsprop.h.html">Program Listing for File rmsprop.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_rnn.h.html">File rnn.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_rnn.h.html">Program Listing for File rnn.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_rnn.h.html">File rnn.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_rnn.h.html">Program Listing for File rnn.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_utils_rnn.h.html">File rnn.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_utils_rnn.h.html">Program Listing for File rnn.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers.h.html">File samplers.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers.h.html">Program Listing for File samplers.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_Scalar.h.html">File Scalar.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_Scalar.h.html">Program Listing for File Scalar.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_core_ScalarType.h.html">File ScalarType.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_core_ScalarType.h.html">Program Listing for File ScalarType.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_detail_sequencers.h.html">File sequencers.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_detail_sequencers.h.html">Program Listing for File sequencers.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_sequential.h.html">File sequential.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_sequential.h.html">Program Listing for File sequential.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_container_sequential.h.html">File sequential.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_container_sequential.h.html">Program Listing for File sequential.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_serialize.h.html">File serialize.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_serialize.h.html">Program Listing for File serialize.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_serialize.h.html">File serialize.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_serialize.h.html">Program Listing for File serialize.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_serialize.h.html">File serialize.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_serialize.h.html">Program Listing for File serialize.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_sgd.h.html">File sgd.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_sgd.h.html">Program Listing for File sgd.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_shared.h.html">File shared.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_shared.h.html">Program Listing for File shared.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_sparse.h.html">File sparse.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_sparse.h.html">Program Listing for File sparse.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_special.h.html">File special.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_special.h.html">Program Listing for File special.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms_stack.h.html">File stack.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms_stack.h.html">Program Listing for File stack.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_dataloader_stateful.h.html">File stateful.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_dataloader_stateful.h.html">Program Listing for File stateful.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_stateful.h.html">File stateful.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_stateful.h.html">Program Listing for File stateful.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_dataloader_stateless.h.html">File stateless.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_dataloader_stateless.h.html">Program Listing for File stateless.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_optim_schedulers_step_lr.h.html">File step_lr.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_optim_schedulers_step_lr.h.html">Program Listing for File step_lr.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_samplers_stream.h.html">File stream.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_samplers_stream.h.html">Program Listing for File stream.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_core_Tensor.h.html">File Tensor.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_core_Tensor.h.html">Program Listing for File Tensor.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_datasets_tensor.h.html">File tensor.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_datasets_tensor.h.html">Program Listing for File tensor.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms_tensor.h.html">File tensor.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms_tensor.h.html">Program Listing for File tensor.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_serialize_tensor.h.html">File tensor.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_serialize_tensor.h.html">Program Listing for File tensor.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_stable_tensor_struct.h.html">File tensor_struct.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_stable_tensor_struct.h.html">Program Listing for File tensor_struct.h</a></li>
</ul>
</details></li>
</ul>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="file_build_aten_src_ATen_core_TensorBody.h.html">File TensorBody.h</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Program Listing for File TensorBody.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_TensorOptions.h.html">File TensorOptions.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_TensorOptions.h.html">Program Listing for File TensorOptions.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_native_TensorShape.h.html">File TensorShape.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_native_TensorShape.h.html">Program Listing for File TensorShape.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_torch.h.html">File torch.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_torch.h.html">Program Listing for File torch.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_transformer.h.html">File transformer.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_transformer.h.html">Program Listing for File transformer.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_transformer.h.html">File transformer.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_transformer.h.html">Program Listing for File transformer.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_transformercoder.h.html">File transformercoder.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_transformercoder.h.html">Program Listing for File transformercoder.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_transformercoder.h.html">File transformercoder.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_transformercoder.h.html">Program Listing for File transformercoder.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_transformerlayer.h.html">File transformerlayer.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_transformerlayer.h.html">Program Listing for File transformerlayer.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_transformerlayer.h.html">File transformerlayer.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_transformerlayer.h.html">Program Listing for File transformerlayer.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_transforms.h.html">File transforms.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_transforms.h.html">Program Listing for File transforms.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_cudnn_Types.h.html">File Types.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_cudnn_Types.h.html">Program Listing for File Types.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_types.h.html">File types.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_types.h.html">Program Listing for File types.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_upsampling.h.html">File upsampling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_upsampling.h.html">Program Listing for File upsampling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_upsampling.h.html">File upsampling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_upsampling.h.html">Program Listing for File upsampling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_upsampling.h.html">File upsampling.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_upsampling.h.html">Program Listing for File upsampling.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_aten_src_ATen_cudnn_Utils.h.html">File Utils.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_aten_src_ATen_cudnn_Utils.h.html">Program Listing for File Utils.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_modules_utils.h.html">File utils.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_modules_utils.h.html">Program Listing for File utils.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_utils.h.html">File utils.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_utils.h.html">Program Listing for File utils.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_utils.h.html">File utils.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_utils.h.html">Program Listing for File utils.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_autograd_variable.h.html">File variable.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_autograd_variable.h.html">Program Listing for File variable.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_autograd_generated_variable_factories.h.html">File variable_factories.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_autograd_generated_variable_factories.h.html">Program Listing for File variable_factories.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_version.h.html">File version.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_version.h.html">Program Listing for File version.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_functional_vision.h.html">File vision.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_functional_vision.h.html">Program Listing for File vision.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_nn_options_vision.h.html">File vision.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_nn_options_vision.h.html">Program Listing for File vision.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_data_worker_exception.h.html">File worker_exception.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_data_worker_exception.h.html">Program Listing for File worker_exception.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_torch_csrc_api_include_torch_xpu.h.html">File xpu.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_torch_csrc_api_include_torch_xpu.h.html">Program Listing for File xpu.h</a></li>
</ul>
</details></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="file_c10_xpu_XPUStream.h.html">File XPUStream.h</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="program_listing_file_c10_xpu_XPUStream.h.html">Program Listing for File XPUStream.h</a></li>
</ul>
</details></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="library_root.html" class="nav-link">Library API</a></li>
    
    
    <li class="breadcrumb-item"><a href="file_build_aten_src_ATen_core_TensorBody.h.html" class="nav-link">File TensorBody.h</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Program...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>
</div>
      
    </div>
  
</div>
</div>
              
              
  
<div id="searchbox"></div>
  <article class="bd-article" id="pytorch-article">
    <!-- Hidden breadcrumb schema for SEO only -->
    <div style="display:none;" itemscope itemtype="https://schema.org/BreadcrumbList">
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="library_root.html">
        <meta itemprop="name" content="Library API">
        <meta itemprop="position" content="1">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <link itemprop="item" href="file_build_aten_src_ATen_core_TensorBody.h.html">
        <meta itemprop="name" content="File TensorBody.h">
        <meta itemprop="position" content="2">
      </div>
      
      <div itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
        <meta itemprop="name" content="Program Listing for File TensorBody.h">
        <meta itemprop="position" content="3">
      </div>
    </div>

    
    

    
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="program-listing-for-file-tensorbody-h">
<span id="program-listing-file-build-aten-src-aten-core-tensorbody-h"></span><h1>Program Listing for File TensorBody.h<a class="headerlink" href="#program-listing-for-file-tensorbody-h" title="Link to this heading">#</a></h1>
<p> <a class="reference internal" href="file_build_aten_src_ATen_core_TensorBody.h.html#file-build-aten-src-aten-core-tensorbody-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">build/aten/src/ATen/core/TensorBody.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#ifdef TORCH_ASSERT_NO_OPERATORS</span>
<span class="cp">#error This change adds a dependency on native_functions.yaml,            \</span>
<span class="cp">  meaning the file will need to be re-compiled every time an operator     \</span>
<span class="cp">  is changed or added. Consider if your change would be better placed in  \</span>
<span class="cp">  another file, or if a more specific header might achieve the same goal. \</span>
<span class="cp">  See NOTE: [Tensor vs. TensorBase]</span>
<span class="cp">#endif</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/Device.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/Layout.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/MemoryFormat.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/QScheme.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/Stream.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/Scalar.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/ScalarType.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/ScalarTypeToTypeMeta.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/Storage.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/TensorImpl.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/UndefinedTensorImpl.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/WrapDimMinimal.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/Exception.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/ExclusivelyOwned.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/Deprecated.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/MaybeOwned.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;optional&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/OptionalArrayRef.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/intrusive_ptr.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/macros/Export.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/macros/Macros.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/CheckMemoryFormat.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/DeprecatedTypePropertiesRegistry.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/DeprecatedTypeProperties.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/NamedTensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/QuantizerBase.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/core/SymInt.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/TensorAccessor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/TensorBase.h&gt;</span>


<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/MethodOperators.h&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">c10</span><span class="p">{</span>
<span class="k">template</span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">List</span><span class="p">;</span>
<span class="k">template</span><span class="o">&lt;</span><span class="k">class</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">IListRef</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="w"> </span><span class="p">{</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Generator</span><span class="p">;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">Type</span><span class="p">;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DeprecatedTypeProperties</span><span class="p">;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace at</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">indexing</span><span class="w"> </span><span class="p">{</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">TensorIndex</span><span class="p">;</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace indexing</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace at</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Node</span><span class="p">;</span>

<span class="p">}}</span><span class="w"> </span><span class="c1">// namespace torch::autograd</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="w"> </span><span class="p">{</span>

<span class="k">class</span><span class="w"> </span><span class="nc">OptionalTensorRef</span><span class="p">;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TensorRef</span><span class="p">;</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Tensor</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">TensorList</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="n">ITensorList</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">IListRef</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">;</span>

<span class="k">using</span><span class="w"> </span><span class="n">Stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">Stream</span><span class="p">;</span>

<span class="c1">// Tensor is a &quot;generic&quot; object holding a pointer to the underlying TensorImpl object, which</span>
<span class="c1">// has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr.</span>
<span class="c1">//</span>
<span class="c1">// For example:</span>
<span class="c1">//</span>
<span class="c1">// void func(Tensor a) {</span>
<span class="c1">//   Tensor b = a;</span>
<span class="c1">//   ...</span>
<span class="c1">// }</span>
<span class="c1">//</span>
<span class="c1">// In this example, when we say Tensor b = a, we are creating a new object that points to the</span>
<span class="c1">// same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the</span>
<span class="c1">// destructor decrements the reference count by calling release() on the TensorImpl it points to.</span>
<span class="c1">// The existing constructors, operator overloads, etc. take care to implement the correct semantics.</span>
<span class="c1">//</span>
<span class="c1">// Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and</span>
<span class="c1">// special care must be taken to handle this.</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">Tensor</span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">TensorBase</span><span class="w"> </span><span class="p">{</span>
<span class="w"> </span><span class="k">protected</span><span class="o">:</span>
<span class="w">  </span><span class="c1">// Create a Tensor with a +0 reference count. Special care must be</span>
<span class="w">  </span><span class="c1">// taken to avoid decrementing this reference count at destruction</span>
<span class="w">  </span><span class="c1">// time. Intended to support MaybeOwnedTraits&lt;Tensor&gt;.</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">Tensor</span><span class="p">(</span><span class="n">unsafe_borrow_t</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">rhs</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">TensorBase</span><span class="p">(</span><span class="n">unsafe_borrow_t</span><span class="p">{},</span><span class="w"> </span><span class="n">rhs</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="k">friend</span><span class="w"> </span><span class="n">MaybeOwnedTraits</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">friend</span><span class="w"> </span><span class="n">OptionalTensorRef</span><span class="p">;</span>
<span class="w">  </span><span class="k">friend</span><span class="w"> </span><span class="n">TensorRef</span><span class="p">;</span>

<span class="w"> </span><span class="k">public</span><span class="o">:</span>
<span class="w">  </span><span class="n">Tensor</span><span class="p">()</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="c1">// This constructor should not be used by end users and is an implementation</span>
<span class="w">  </span><span class="c1">// detail invoked by autogenerated code.</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">Tensor</span><span class="p">(</span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">TensorImpl</span><span class="p">,</span><span class="w"> </span><span class="n">UndefinedTensorImpl</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_impl</span><span class="p">)</span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">TensorBase</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">tensor_impl</span><span class="p">))</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>
<span class="w">  </span><span class="n">Tensor</span><span class="p">(</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">default</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Implicitly move-constructible from TensorBase, but must be explicit to increase refcount</span>
<span class="w">  </span><span class="k">explicit</span><span class="w"> </span><span class="n">Tensor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">TensorBase</span><span class="w"> </span><span class="o">&amp;</span><span class="n">base</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">TensorBase</span><span class="p">(</span><span class="n">base</span><span class="p">)</span><span class="w"> </span><span class="p">{}</span>
<span class="w">  </span><span class="cm">/*implicit*/</span><span class="w"> </span><span class="n">Tensor</span><span class="p">(</span><span class="n">TensorBase</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="n">base</span><span class="p">)</span><span class="o">:</span><span class="w"> </span><span class="n">TensorBase</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">base</span><span class="p">))</span><span class="w"> </span><span class="p">{}</span>

<span class="w">  </span><span class="c1">// Creates a new wrapper from TensorImpl. Intentionally a free method because</span>
<span class="w">  </span><span class="c1">// it should be used with care. Checks necessary invariants</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="n">wrap_tensor_impl</span><span class="p">(</span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">TensorImpl</span><span class="p">,</span><span class="w"> </span><span class="n">UndefinedTensorImpl</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_impl</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">wrap_tensor_impl</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">tensor_impl</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">contiguous</span><span class="p">(</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">is_complex</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>

<span class="w">    </span><span class="n">C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED</span><span class="p">(</span><span class="s">&quot;-Wswitch-enum&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="k">switch</span><span class="w"> </span><span class="p">(</span><span class="k">this</span><span class="o">-&gt;</span><span class="n">layout</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">kSparse</span><span class="p">:</span>
<span class="w">      </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">kSparseCsr</span><span class="p">:</span>
<span class="w">      </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">kSparseCsc</span><span class="p">:</span>
<span class="w">      </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">kSparseBsr</span><span class="p">:</span>
<span class="w">      </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">kSparseBsc</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">conj_physical</span><span class="p">();</span>
<span class="w">      </span><span class="k">default</span><span class="o">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">_conj</span><span class="p">();</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="n">C10_DIAGNOSTIC_POP</span><span class="p">()</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Aliased by Dimname overloads, so need explicit using</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">size</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">sym_size</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">stride</span><span class="p">;</span>

<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">expect_contiguous</span><span class="p">(</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">&amp;</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Use .contiguous() instead. Trying to borrow from a prvalue Tensor</span>
<span class="w">  </span><span class="c1">// will only lead to trouble and dangling references.</span>
<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">expect_contiguous</span><span class="p">(</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// The following overloads are very intriguing.  Consider the following</span>
<span class="w">  </span><span class="c1">// program:</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">//    x[1] = 3;</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// We would expect that the first entry of x is written to 3.  But how can we</span>
<span class="w">  </span><span class="c1">// actually achieve this?  x[1] evaluates to a tensor...</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// The answer is, using a ref-qualifier.  x[1] is an rvalue, which cannot be</span>
<span class="w">  </span><span class="c1">// (profitably) assigned to in the traditional sense, so we overload</span>
<span class="w">  </span><span class="c1">// assignment to mean, &quot;Actually, copy 3 into the tensor data.&quot;  This is done</span>
<span class="w">  </span><span class="c1">// with an rvalue-reference ref-qualified overload (the methods with &amp;&amp; at the</span>
<span class="w">  </span><span class="c1">// end of their type.)</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// There&#39;s one more fly in the ointment: We also want</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">//    Tensor x = y;</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// to work, and we want it NOT to copy.  So we need a traditional operator=</span>
<span class="w">  </span><span class="c1">// overload.  But we MUST specify a mutable lvalue ref-qualifier, to</span>
<span class="w">  </span><span class="c1">// disambiguate the traditional overload from the rvalue-reference</span>
<span class="w">  </span><span class="c1">// ref-qualified overload.  Otherwise, it will be ambiguous, because</span>
<span class="w">  </span><span class="c1">// a non ref-qualified method is eligible for all situations.</span>

<span class="w">  </span><span class="c1">// Unfortunately, we have to write these constructors out manually</span>
<span class="w">  </span><span class="c1">// to work around an MSVC bug:</span>
<span class="w">  </span><span class="c1">//    error C2580: &#39;at::Tensor &amp;at::Tensor::operator =(const at::Tensor &amp;) &amp;&#39;:</span>
<span class="w">  </span><span class="c1">//    multiple versions of a defaulted special member functions are not allowed</span>
<span class="w">  </span><span class="c1">// Tensor&amp; operator=(const Tensor&amp;) &amp; = default;</span>
<span class="w">  </span><span class="c1">// Tensor&amp; operator=(Tensor&amp;&amp;) &amp; = default;</span>

<span class="w">  </span><span class="c1">// Also MSVC will wrongly issue the following warning with the aforementioned fix</span>
<span class="w">  </span><span class="c1">//    warning C4522: &#39;at::Tensor&#39;: multiple assignment operators specified</span>
<span class="w">  </span><span class="c1">// Let&#39;s just skip the warning.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// TODO: temporarily disabled</span>

<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">impl_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="n">TensorBase</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">impl_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">unsafeReleaseIntrusivePtr</span><span class="p">();</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">TensorBase</span><span class="o">&amp;&amp;&gt;</span><span class="p">(</span><span class="n">x</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="n">v</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">fill_</span><span class="p">(</span><span class="n">v</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="n">rhs</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">copy_</span><span class="p">(</span><span class="n">rhs</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// NOLINTNEXTLINE(performance-noexcept-move-constructor)</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="n">Tensor</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">rhs</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">copy_</span><span class="p">(</span><span class="n">rhs</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">C10_DEPRECATED_MESSAGE</span><span class="p">(</span><span class="s">&quot;Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device().&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="n">DeprecatedTypeProperties</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">type</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">globalDeprecatedTypePropertiesRegistry</span><span class="p">().</span><span class="n">getDeprecatedTypeProperties</span><span class="p">(</span>
<span class="w">        </span><span class="n">dispatchKeyToBackend</span><span class="p">(</span><span class="n">legacyExtractDispatchKey</span><span class="p">(</span><span class="n">key_set</span><span class="p">())),</span>
<span class="w">        </span><span class="n">scalar_type</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">toType</span><span class="p">(</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">dtype</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// TODO: Deprecate me</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">toBackend</span><span class="p">(</span><span class="n">Backend</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">backendToDeviceType</span><span class="p">(</span><span class="n">b</span><span class="p">)).</span><span class="n">layout</span><span class="p">(</span><span class="n">layout_from_backend</span><span class="p">(</span><span class="n">b</span><span class="p">)),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">C10_DEPRECATED_MESSAGE</span><span class="p">(</span><span class="s">&quot;Tensor.is_variable() is deprecated; everything is a variable now. (If you want to assert that variable has been appropriately handled already, use at::impl::variable_excluded_from_dispatch())&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_variable</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">noexcept</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">!</span><span class="n">at</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">variable_excluded_from_dispatch</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">C10_DEPRECATED_MESSAGE</span><span class="p">(</span><span class="s">&quot;Tensor.data&lt;T&gt;() is deprecated. Please use Tensor.data_ptr&lt;T&gt;() instead.&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="n">T</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">data_ptr</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">T</span><span class="w"> </span><span class="n">item</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">U</span><span class="o">&gt;</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">PtrTraits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DefaultPtrTraits</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">index_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">int64_t</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">C10_DEPRECATED_MESSAGE</span><span class="p">(</span><span class="s">&quot;packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="n">GenericPackedTensorAccessor</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">PtrTraits</span><span class="p">,</span><span class="n">index_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">packed_accessor</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">generic_packed_accessor</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">PtrTraits</span><span class="p">,</span><span class="n">index_t</span><span class="o">&gt;</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">template</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">U</span><span class="o">&gt;</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">PtrTraits</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">DefaultPtrTraits</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="w"> </span><span class="nc">index_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">int64_t</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">C10_DEPRECATED_MESSAGE</span><span class="p">(</span><span class="s">&quot;packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="n">GenericPackedTensorAccessor</span><span class="o">&lt;</span><span class="n">T</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">PtrTraits</span><span class="p">,</span><span class="n">index_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">packed_accessor</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">delete</span><span class="p">;</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="o">~</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">bitwise_not</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="o">-</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">neg</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">+=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">add_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">+=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">add_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">-=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">sub_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">-=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">sub_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">*=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mul_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">*=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mul_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">/=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">div_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">/=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">div_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">&amp;=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">bitwise_and_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">|=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">bitwise_or_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="k">operator</span><span class="o">^=</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">bitwise_xor_</span><span class="p">(</span><span class="n">other</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">index</span><span class="p">.</span><span class="n">isIntegral</span><span class="p">(</span><span class="nb">false</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_CHECK_INDEX</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Can only index tensors with integral scalars&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="k">operator</span><span class="p">[](</span><span class="n">index</span><span class="p">.</span><span class="n">toLong</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="p">[](</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// These properties are checked in the Scalar constructor, but we already</span>
<span class="w">    </span><span class="c1">// check them here to provide more useful diagnostics for the user.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">index</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_CHECK_INDEX</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Can only index with tensors that are defined&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">index</span><span class="p">.</span><span class="n">dim</span><span class="p">()</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_CHECK_INDEX</span><span class="p">(</span><span class="nb">false</span><span class="p">,</span>
<span class="w">                        </span><span class="s">&quot;Can only index with tensors that are scalars (zero-dim)&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="c1">// The Scalar(Tensor) constructor is explicit, so we need to call it.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="k">operator</span><span class="p">[](</span><span class="n">index</span><span class="p">.</span><span class="n">item</span><span class="p">());</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="k">operator</span><span class="p">[](</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="n">index</span><span class="p">(</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_put_</span><span class="p">(</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">rhs</span><span class="p">);</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_put_</span><span class="p">(</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="o">&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">);</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_put_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">rhs</span><span class="p">);</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_put_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">initializer_list</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">indexing</span><span class="o">::</span><span class="n">TensorIndex</span><span class="o">&gt;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Scalar</span><span class="o">&amp;</span><span class="w"> </span><span class="n">v</span><span class="p">);</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cpu</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CPU</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// TODO: The Python version also accepts arguments</span>
<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cuda</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">CUDA</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">hip</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">HIP</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ve</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">VE</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">vulkan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">Vulkan</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">metal</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">Metal</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">meta</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">to</span><span class="p">(</span><span class="n">options</span><span class="p">().</span><span class="n">device</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">DeviceType</span><span class="o">::</span><span class="n">Meta</span><span class="p">),</span><span class="w"> </span><span class="cm">/*non_blocking*/</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="cm">/*copy*/</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// ~~~~~ Autograd API ~~~~~</span>


<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">gradient</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">retain_graph</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">create_graph</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">TensorList</span><span class="o">&gt;</span><span class="w"> </span><span class="n">inputs</span><span class="o">=</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// NB: Adding this wrapper to _backward here because we&#39;d like our</span>
<span class="w">    </span><span class="c1">// &#39;backwards&#39; api to accept the &#39;inputs&#39; argument optionally. Since code gen</span>
<span class="w">    </span><span class="c1">// currently does not support optional of TensorList our approach is to replace</span>
<span class="w">    </span><span class="c1">// backward in native_functions.yaml with _backward and call it here instead.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">has_value</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">value</span><span class="p">().</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;&#39;inputs&#39; argument to backward cannot be empty&quot;</span><span class="p">)</span>
<span class="w">      </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">_backward</span><span class="p">(</span><span class="n">inputs</span><span class="p">.</span><span class="n">value</span><span class="p">(),</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">_backward</span><span class="p">({},</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>





<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_requires_grad</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">set_requires_grad</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mutable_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">impl_</span><span class="o">-&gt;</span><span class="n">mutable_grad</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">maybe_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">impl_</span><span class="o">-&gt;</span><span class="n">grad</span><span class="p">();</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="o">!</span><span class="n">is_leaf</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">retains_grad</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="o">!</span><span class="n">maybe_grad</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="n">TORCH_WARN</span><span class="p">(</span>
<span class="w">        </span><span class="s">&quot;The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad &quot;</span>
<span class="w">        </span><span class="s">&quot;attribute won&#39;t be populated during autograd.backward(). If you indeed want the .grad &quot;</span>
<span class="w">        </span><span class="s">&quot;field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. &quot;</span>
<span class="w">        </span><span class="s">&quot;If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor &quot;</span>
<span class="w">        </span><span class="s">&quot;instead. See github.com/pytorch/pytorch/pull/30531 for more information.&quot;</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">maybe_grad</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// The Forward AD API functions below are low level and are not to be used by end</span>
<span class="w">  </span><span class="c1">// users who should use the API provided in torch/csrc/autograd.h</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">_fw_grad</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">impl_</span><span class="o">-&gt;</span><span class="n">_fw_grad</span><span class="p">(</span><span class="n">level</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">_set_fw_grad</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_grad</span><span class="p">,</span><span class="w"> </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_inplace_op</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">impl_</span><span class="o">-&gt;</span><span class="n">_set_fw_grad</span><span class="p">(</span><span class="n">new_grad</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="p">,</span><span class="w"> </span><span class="n">is_inplace_op</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>


<span class="w">  </span><span class="c1">// STOP.  Thinking of adding a method here, which only makes use</span>
<span class="w">  </span><span class="c1">// of other ATen methods?  Define it in native_functions.yaml.</span>

<span class="w">  </span><span class="c1">//example</span>
<span class="w">  </span><span class="c1">//Tensor * add(Tensor &amp; b);</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">__dispatch__backward</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorList</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">gradient</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">retain_graph</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">create_graph</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">__dispatch_set_data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_data</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__dispatch_data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_leaf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">__dispatch_output_nr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">__dispatch__version</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__dispatch_requires_grad_</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">__dispatch_retain_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_retains_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_fw_primal</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">level</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">rename_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="o">&gt;</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">rename</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="o">&gt;</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">align_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">align_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">order</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">ellipsis_idx</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">align_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">refine_names</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">abs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">abs_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">absolute</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">absolute_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">angle</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sgn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sgn_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">chalf</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__dispatch_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_conj_physical</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">conj_physical</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">conj_physical_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">resolve_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">resolve_neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_neg_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">acos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">acos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arccos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arccos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">add_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">add</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">add_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addmv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addmv_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_is_all_true</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_is_any_true</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">all</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">all</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">all</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">allclose</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">equal_nan</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">any</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">any</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">any</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argmax</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argmin</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">acosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">acosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arccosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arccosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">asinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">asinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arcsinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arcsinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">atanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">atanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arctanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arctanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">as_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">as_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">as_strided_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">as_strided__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">asin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">asin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arcsin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arcsin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">atan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">atan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arctan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arctan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">baddbmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">baddbmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bernoulli</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bernoulli_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bernoulli_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bernoulli</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bincount</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weights</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">minlength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bincount_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weights</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">minlength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_not</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_not_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">copysign</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">copysign_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">copysign</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">copysign_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_lazy_clone</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logical_not</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">logical_not_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logical_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">logical_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logical_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">logical_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logical_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">logical_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">broadcast_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">broadcast_to_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ceil</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ceil_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unsafe_chunk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">chunk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_split</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tensor_split</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor_indices_or_sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp_max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp_max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_max_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_max_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp_min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clamp_min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_min_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clamp_min_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clip</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clip</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clip_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">clip_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__dispatch_contiguous</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=</span><span class="n">c10</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">::</span><span class="n">Contiguous</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">copy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">count_nonzero</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">count_nonzero</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cov</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">correction</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fweights</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">aweights</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">corrcoef</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cummax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cummax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cummin</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cummin</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cumprod</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cumprod_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cumprod</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cumprod_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cumsum_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cumsum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cumsum_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diag_embed</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="o">=</span><span class="mi">-2</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="o">=</span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diagflat</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diagonal</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diagonal</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">outdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim2</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fill_diagonal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">wrap</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diff</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">prepend</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">append</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">true_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">true_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">true_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">true_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">dot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">vdot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_empty_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_full</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_full</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_full_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_full_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_zeros</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_zeros</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_zeros_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_zeros_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_ones</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_ones</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_ones_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">new_ones_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">resize_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">resize__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">erf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">erf_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">erfc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">erfc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">exp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">exp_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">exp2</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">exp2_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">expm1</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">expm1_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">expand</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">implicit</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">expand_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">implicit</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">expand_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">end_dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dims</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unflatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unflatten_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unflatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unflatten_symint</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">floor</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">floor_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">floor_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">floor_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">floor_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">floor_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">frac</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">frac_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">gcd</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">gcd_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lcm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lcm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_copy_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_copy</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_copy_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_copy</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_put_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_put</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isclose</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">rtol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">atol</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">equal_nan</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isnan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_distributed</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_floating_point</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_complex</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch__is_zerotensor</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isreal</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_nonzero</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_same_size</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_signed</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">__dispatch_is_inference</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">kron</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kthvalue</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kthvalue_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kthvalue</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kthvalue_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nan_to_num</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nan</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">posinf</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">neginf</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">nan_to_num_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nan</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">posinf</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">neginf</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ldexp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ldexp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">log_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log10</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">log10_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log1p</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">log1p_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log2</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">log2_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logaddexp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logaddexp2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">xlogy</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">xlogy</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">xlogy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">xlogy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logcumsumexp</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logcumsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">matmul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">matrix_power</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">matrix_exp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">aminmax</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">max</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">max</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">amax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nanmean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">median</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">median</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">median</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nanmedian</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nanmedian</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nanmedian</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">min</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">min</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">amin</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">mode</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">mode</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">multiply</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">multiply_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">multiply</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">multiply_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mvlgamma</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mvlgamma_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow_copy</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow_copy_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">narrow_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">permute</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">movedim</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">movedim</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">moveaxis</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">moveaxis</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">numpy_T</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">matrix_H</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mT</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">mH</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">adjoint</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_pinned</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">pin_memory</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">pinverse</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">rcond</span><span class="o">=</span><span class="mf">1e-15</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">rad2deg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">rad2deg_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">deg2rad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">deg2rad_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ravel</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">reciprocal</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">reciprocal_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">neg_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">negative</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">negative_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">repeats</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">repeats</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat_interleave</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat_interleave_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat_interleave</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">repeat_interleave_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">reshape</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">shape</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">reshape_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">shape</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_reshape_alias</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_reshape_alias_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">reshape_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">round</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">round_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">decimals</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">round_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">decimals</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">relu</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">relu_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">prelu</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">hardshrink</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">hardshrink_backward</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">grad_out</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">lambd</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">rsqrt</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">rsqrt_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">select</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">select</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">select_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sigmoid_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logit</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">eps</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">logit_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">eps</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sinc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sinc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">detach</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">detach_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">size</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice_inverse</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice_inverse_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">slice_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">select_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">select_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diagonal_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">as_strided_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">as_strided_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">smm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">softmax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unsafe_split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unsafe_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unsafe_split_with_sizes</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unsafe_split_with_sizes_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split_with_sizes</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">split_with_sizes_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">vsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">vsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">squeeze</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">squeeze_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">squeeze_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">squeeze_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">squeeze_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sspaddmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">stft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">return_complex</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">align_to_window</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">stft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">center</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">pad_mode</span><span class="o">=</span><span class="s">&quot;reflect&quot;</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">return_complex</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">align_to_window</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">istft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">center</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">length</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">return_complex</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">stride</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nansum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">hash_tensor</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">mode</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sum_to_size</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sum_to_size_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sqrt_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">square</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">square_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">prod</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">prod</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">prod</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">t</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">t_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">tan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">tanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tile</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tile_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">transpose</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">transpose_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flip</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fliplr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">flipud</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">roll</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">shifts</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">roll_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">shifts</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">rot90</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_nested_tensor_size</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_nested_tensor_strides</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_nested_tensor_storage_offsets</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">trunc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">trunc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fix</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fix_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">type_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unsqueeze</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">unsqueeze_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">view_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">where</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">where</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">frexp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">clone</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">positive</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">resize_as_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">the_template</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">resize_as_sparse_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">the_template</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">zero_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sub</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sub_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sub</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sub_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">subtract</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">subtract_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">subtract</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">subtract_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">heaviside</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">heaviside_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_addmm_activation</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">use_gelu</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sparse_resize_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sparse_resize_and_clear_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sparse_mask</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_sparse_mask_projection</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate_matches</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_dense</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">masked_grad</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_dense</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">masked_grad</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">sparse_dim</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">_dimI</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">dense_dim</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">_dimV</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">_nnz</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">coalesce</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_coalesced</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_values</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">_coalesced_</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">coalesced</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">values</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">crow_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">col_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ccol_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">row_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unbind</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">unbind</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse_csr</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse_csr</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse_csc</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse_csc</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse_bsr</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse_bsr</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_sparse_bsc</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_to_sparse_bsc</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_mkldnn</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">dequantize</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">double</span><span class="w"> </span><span class="nf">q_scale</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">q_zero_point</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">q_per_channel_scales</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">q_per_channel_zero_points</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">q_per_channel_axis</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">int_repr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">QScheme</span><span class="w"> </span><span class="nf">qscheme</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_autocast_to_reduced_precision</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">cuda_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">cpu_dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">_autocast_to_full_precision</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="nf">item</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set__symint</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set__symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="o">=</span><span class="p">{})</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">set_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">is_set_to</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">masked_fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">masked_fill</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">masked_fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">masked_fill</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">masked_scatter_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">masked_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">view</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">view_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">view</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">put_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">put</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_add_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_add</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_add</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_reduce_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_reduce</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_fill_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_fill</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_fill_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_fill</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_fill_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">index_fill_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_fill</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_fill</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter_add</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_add_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter_add</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">scatter_reduce</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">scatter_reduce_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">eq_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">eq_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__and__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__and__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__iand__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__iand__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__or__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__or__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ior__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ior__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__xor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__xor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ixor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ixor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__lshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__lshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ilshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__ilshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_left_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_left_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_left_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_left_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__rshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">__rshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__irshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">__irshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_right_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_right_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">bitwise_right_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">bitwise_right_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">tril_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">tril__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">triu_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">triu__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">digamma_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lerp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lerp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addbmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addbmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">random_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">from</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">random_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">random_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">uniform_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">from</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">to</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">cauchy_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">median</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">log_normal_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">exponential_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">lambd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">geometric_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">diag</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cross</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">triu</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">triu_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tril</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">tril_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">trace</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ne</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ne</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ne_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ne_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">not_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">not_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">not_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">not_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">eq</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">eq</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ge_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">ge_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">greater_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">greater_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">greater_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">greater_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">le</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">le</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">le_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">le_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">less_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">less_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">less_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">less_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">gt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">gt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">gt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">gt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">greater</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">greater</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">greater_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">greater_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">less</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">less</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">less_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">less_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">take</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">take_along_dim</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_select</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">index_select</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">masked_select</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nonzero</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nonzero_static</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">fill_value</span><span class="o">=</span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nonzero_static_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">fill_value</span><span class="o">=</span><span class="mi">-1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nonzero_numpy</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argwhere</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">gather</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sparse_grad</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">gather</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sparse_grad</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addcmul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addcmul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">addcdiv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">addcdiv_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">triangular_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">transpose</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unitriangular</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">svd</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">some</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">compute_uv</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">swapaxes</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">swapaxes_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">swapdims</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">swapdims_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cholesky</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cholesky_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">cholesky_inverse</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">qr</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">some</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">geqrf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">orgqr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ormqr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input3</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">left</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">transpose</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lu_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">LU_data</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">LU_pivots</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">multinomial</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">replacement</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">multinomial_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">replacement</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">lgamma_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lgamma</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">digamma</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">polygamma</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">polygamma_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">erfinv</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">erfinv_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">i0</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">i0_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">sign</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">sign_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">signbit</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">dist</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">atan2_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">atan2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">arctan2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">arctan2_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lerp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">lerp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">histc</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">histogram</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">density</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">histogram</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">range</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="o">=</span><span class="p">{},</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">density</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fmod</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fmod_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fmod</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fmod_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">hypot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">hypot_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">igamma</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">igamma_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">igammac</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">igammac_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">nextafter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">nextafter_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">remainder</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">remainder_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">remainder</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">remainder_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">min</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fmin</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">max</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">fmax</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">maximum</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">minimum</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">quantile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">quantile</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">nanquantile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">nanquantile</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="o">=</span><span class="s">&quot;linear&quot;</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">sort</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">msort</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argsort</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argsort</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">argsort</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">topk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">largest</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sorted</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">topk_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="o">=</span><span class="mi">-1</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">largest</span><span class="o">=</span><span class="nb">true</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sorted</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">all</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">any</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">renorm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">renorm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">unfold</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dimension</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">pow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">pow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">pow_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">pow_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">float_power</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">float_power</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">float_power_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">float_power_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">normal_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">std</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">alias</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isfinite</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isinf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">record_stream</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Stream</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isposinf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">isneginf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">det</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">slogdet</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">logdet</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">inverse</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">inner</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">outer</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">ger</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_padded_tensor</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to_padded_tensor_symint</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalSymIntArrayRef</span><span class="w"> </span><span class="n">output_size</span><span class="o">=::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Special C++ only overloads for std()-like functions (See gh-40287)</span>
<span class="w">  </span><span class="c1">// These are needed because int -&gt; bool conversion takes precedence over int -&gt; IntArrayRef</span>
<span class="w">  </span><span class="c1">// So, for example std(0) would select the std(unbiased=False) overload</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">var</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">var</span><span class="p">(</span><span class="n">IntArrayRef</span><span class="p">{</span><span class="n">dim</span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">std</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="p">(</span><span class="n">IntArrayRef</span><span class="p">{</span><span class="n">dim</span><span class="p">});</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the</span>
<span class="w">  </span><span class="c1">// at::kDouble and its friends to be TypeMeta&#39;s, but that hasn&#39;t happened yet.</span>
<span class="w">  </span><span class="c1">// Before that change, we make this method to maintain BC for C++ usage like</span>
<span class="w">  </span><span class="c1">// `x.to(y.dtype)`.</span>
<span class="w">  </span><span class="c1">// TODO: remove following two after at::kDouble and its friends are TypeMeta&#39;s.</span>
<span class="w">  </span><span class="kr">inline</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="n">caffe2</span><span class="o">::</span><span class="n">TypeMeta</span><span class="w"> </span><span class="n">type_meta</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">to</span><span class="p">(</span><span class="cm">/*scalar_type=*/</span><span class="n">typeMetaToScalarType</span><span class="p">(</span><span class="n">type_meta</span><span class="p">),</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="kr">inline</span><span class="w"> </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">caffe2</span><span class="o">::</span><span class="n">TypeMeta</span><span class="w"> </span><span class="n">type_meta</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="o">=</span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">this</span><span class="o">-&gt;</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="cm">/*scalar_type=*/</span><span class="n">typeMetaToScalarType</span><span class="p">(</span><span class="n">type_meta</span><span class="p">),</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">F</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">decltype</span><span class="p">(</span><span class="k">auto</span><span class="p">)</span><span class="w"> </span><span class="n">m</span><span class="p">(</span><span class="n">F</span><span class="w"> </span><span class="n">func</span><span class="p">,</span><span class="w"> </span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">params</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">(</span><span class="n">params</span><span class="p">)...);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">tensor_data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">tensor_data</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">variable_data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">variable_data</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">// Hooks</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">hook_return_void_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_void</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">std</span><span class="o">::</span><span class="n">invoke_result_t</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&gt;&gt;::</span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="kt">unsigned</span><span class="o">&gt;</span><span class="p">;</span>
<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">hook_return_var_t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">enable_if_t</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">is_same_v</span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">std</span><span class="o">::</span><span class="n">invoke_result_t</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">,</span><span class="w"> </span><span class="kt">unsigned</span><span class="o">&gt;</span><span class="p">;</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">hook_return_void_t</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">register_hook</span><span class="p">(</span><span class="n">T</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">hook</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>
<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="o">&gt;</span>
<span class="w">  </span><span class="n">hook_return_var_t</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="w"> </span><span class="n">register_hook</span><span class="p">(</span><span class="n">T</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">hook</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Variable methods</span>
<span class="w">  </span><span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="w">  </span><span class="n">Tensor</span><span class="w"> </span><span class="nf">data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">data</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">_backward</span><span class="p">(</span><span class="n">TensorList</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">keep_graph</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">requires_grad_</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">_requires_grad</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">TensorBase</span><span class="o">::</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">_requires_grad</span><span class="p">);</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">*</span><span class="k">this</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">detail</span><span class="w"> </span><span class="p">{</span>
<span class="c1">// Helper creator for Tensor class which doesn&#39;t requires the users to pass</span>
<span class="c1">// in an intrusive_ptr instead it just converts the argument passed to</span>
<span class="c1">// requested intrusive_ptr type.</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">T</span><span class="p">,</span><span class="w"> </span><span class="k">typename</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="n">Tensor</span><span class="w"> </span><span class="n">make_tensor</span><span class="p">(</span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Tensor</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">make_intrusive</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">)...));</span>
<span class="p">}</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace detail</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace at</span>


<span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="w"> </span><span class="p">{</span>

<span class="c1">// aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -&gt; ()</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">Tensor::__dispatch__backward</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorList</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">create_graph</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_backward</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">gradient</span><span class="p">,</span><span class="w"> </span><span class="n">retain_graph</span><span class="p">,</span><span class="w"> </span><span class="n">create_graph</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::set_data(Tensor(a!) self, Tensor new_data) -&gt; ()</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">Tensor::__dispatch_set_data</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_data</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set_data</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">new_data</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::data(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::__dispatch_data</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">data</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_leaf(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">Tensor::__dispatch_is_leaf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_leaf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::output_nr(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">Tensor::__dispatch_output_nr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">output_nr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_version(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="nf">Tensor::__dispatch__version</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_version</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::__dispatch_requires_grad_</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">requires_grad_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::retain_grad(Tensor(a!) self) -&gt; ()</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">Tensor::__dispatch_retain_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">retain_grad</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::retains_grad(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">Tensor::__dispatch_retains_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">retains_grad</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_fw_primal(Tensor(a) self, int level) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_fw_primal</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">level</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_fw_primal</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">level</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::rename_(Tensor(a!) self, Dimname[]? names) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::rename_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="o">&gt;</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rename_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::rename(Tensor(a) self, Dimname[]? names) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::rename</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="o">&gt;</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rename</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::align_to(Tensor(a) self, Dimname[] names) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::align_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">align_to</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::align_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">order</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">ellipsis_idx</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">align_to_ellipsis_idx</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">order</span><span class="p">,</span><span class="w"> </span><span class="n">ellipsis_idx</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::align_as(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::align_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">align_as</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::refine_names(Tensor(a) self, Dimname[] names) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::refine_names</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">refine_names</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::abs(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::abs</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">abs</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::abs_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::abs_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">abs_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::absolute(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::absolute</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">absolute</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::absolute_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::absolute_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">absolute_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::angle(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::angle</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">angle</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sgn(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::sgn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sgn</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sgn_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::sgn_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sgn_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::chalf</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">chalf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_conj(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_conj</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::conj(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::__dispatch_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">conj</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_conj_physical(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_conj_physical</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_conj_physical</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::conj_physical(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::conj_physical</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">conj_physical</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::conj_physical_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::conj_physical_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">conj_physical_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::resolve_conj(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::resolve_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resolve_conj</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::resolve_neg(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::resolve_neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resolve_neg</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_neg_view(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_neg_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_neg_view</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::acos(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::acos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">acos</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::acos_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::acos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">acos_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arccos(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arccos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arccos</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arccos_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arccos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arccos_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::add</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">add_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::add_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">add__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::add</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">add_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::add_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">add__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::addmv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addmv</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::addmv_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addmv_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat</span><span class="p">,</span><span class="w"> </span><span class="n">vec</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::addr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::addr_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addr_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">vec1</span><span class="p">,</span><span class="w"> </span><span class="n">vec2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_is_all_true(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_is_all_true</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_is_all_true</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_is_any_true(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_is_any_true</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_is_any_true</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::all.dim(Tensor self, int dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::all</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">all_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::all</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">all_dims</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::all</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">all_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">Tensor::allclose</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">rtol</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">atol</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">equal_nan</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">allclose</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="p">,</span><span class="w"> </span><span class="n">equal_nan</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::any.dim(Tensor self, int dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::any</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">any_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::any</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">any_dims</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::any</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">any_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::argmax</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argmax</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::argmin</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argmin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::acosh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::acosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">acosh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::acosh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::acosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">acosh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arccosh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arccosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arccosh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arccosh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arccosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arccosh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::asinh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::asinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">asinh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::asinh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::asinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">asinh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arcsinh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arcsinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arcsinh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arcsinh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arcsinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arcsinh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::atanh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::atanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atanh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::atanh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::atanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atanh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arctanh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arctanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctanh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arctanh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arctanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctanh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::as_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">storage_offset</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::as_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::as_strided_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">storage_offset</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::as_strided__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::asin(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::asin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">asin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::asin_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::asin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">asin_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arcsin(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arcsin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arcsin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arcsin_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arcsin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arcsin_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::atan(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::atan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atan</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::atan_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::atan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atan_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arctan(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::arctan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctan</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::arctan_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::arctan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctan_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::baddbmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">baddbmm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::baddbmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">baddbmm_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bernoulli(Tensor self, *, Generator? generator=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bernoulli</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bernoulli</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::bernoulli_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bernoulli__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::bernoulli_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bernoulli__float</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bernoulli</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bernoulli_p</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bincount</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">minlength</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bincount</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">minlength</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bincount_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">minlength</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bincount</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">minlength</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_not(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bitwise_not</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_not</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_not_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::bitwise_not_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_not_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::copysign.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::copysign</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">copysign_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::copysign_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">copysign__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::copysign.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::copysign</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">copysign_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::copysign_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">copysign__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_lazy_clone(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::_lazy_clone</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_lazy_clone</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logical_not(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::logical_not</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_not</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logical_not_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::logical_not_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_not_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logical_xor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::logical_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_xor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logical_xor_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::logical_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_xor_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logical_and(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::logical_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_and</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logical_and_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::logical_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_and_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logical_or(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::logical_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_or</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logical_or_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::logical_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logical_or_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bmm(Tensor self, Tensor mat2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::bmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bmm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::broadcast_to(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::broadcast_to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">broadcast_to</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::broadcast_to(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::broadcast_to_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">broadcast_to</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ceil(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="nf">Tensor::ceil</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ceil</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::ceil_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="nf">Tensor::ceil_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ceil_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsafe_chunk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsafe_chunk</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::chunk(Tensor(a -&gt; *) self, int chunks, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">chunk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">chunk</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">chunks</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tensor_split.sections(Tensor(a -&gt; *) self, SymInt sections, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tensor_split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tensor_split_sections</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tensor_split.sections(Tensor(a -&gt; *) self, SymInt sections, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tensor_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tensor_split_sections</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sections</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tensor_split.indices(Tensor(a -&gt; *) self, SymInt[] indices, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tensor_split</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tensor_split_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">indices</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tensor_split.indices(Tensor(a -&gt; *) self, SymInt[] indices, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tensor_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tensor_split_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tensor_split.tensor_indices_or_sections(Tensor(a -&gt; *) self, Tensor tensor_indices_or_sections, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tensor_split</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor_indices_or_sections</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tensor_split_tensor_indices_or_sections</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor_indices_or_sections</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_max(Tensor self, Scalar max) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_max</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_max.Tensor(Tensor self, Tensor max) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_max_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_max_(Tensor(a!) self, Scalar max) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_max_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_max_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_max_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_max__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_min(Tensor self, Scalar min) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_min</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_min.Tensor(Tensor self, Tensor min) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_min_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_min_(Tensor(a!) self, Scalar min) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_min_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_min_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clamp_min_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clamp_min__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clip</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clip</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clip</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clip_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clip_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clip_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clip_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clip__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_contiguous</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">contiguous</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">copy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">copy_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cos(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cos</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cos</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::cos_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cos_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cos_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::cosh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cosh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cosh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::cosh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cosh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cosh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">count_nonzero_dim_IntList</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::count_nonzero(Tensor self, int? dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">count_nonzero</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">count_nonzero</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cov</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fweights</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">aweights</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cov</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="n">fweights</span><span class="p">,</span><span class="w"> </span><span class="n">aweights</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::corrcoef(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">corrcoef</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">corrcoef</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::cummax(Tensor self, int dim) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cummax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cummax</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cummax.dimname(Tensor self, Dimname dim) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cummax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cummax_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cummin(Tensor self, int dim) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cummin</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cummin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cummin.dimname(Tensor self, Dimname dim) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cummin</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cummin_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumprod</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumprod</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumprod_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumprod_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumprod</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumprod_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumprod_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumprod__dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumsum</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumsum</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumsum_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumsum_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumsum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumsum_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cumsum_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cumsum__dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diag_embed</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diag_embed</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">dim2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diagflat(Tensor self, int offset=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diagflat</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diagflat</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diagonal</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diagonal</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">dim2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diagonal</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">outdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim2</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diagonal_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">outdim</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">dim2</span><span class="p">,</span><span class="w"> </span><span class="n">offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fill_diagonal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">wrap</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fill_diagonal_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">wrap</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diff</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">prepend</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">append</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diff</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">prepend</span><span class="p">,</span><span class="w"> </span><span class="n">append</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div_Tensor_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div__Tensor_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div_Scalar_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">div_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">div__Scalar_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide_Tensor_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide__Tensor_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide_Scalar_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="o">&gt;</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">divide__Scalar_mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rounding_mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::true_divide.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">true_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">true_divide_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">true_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">true_divide__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::true_divide.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">true_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">true_divide_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">true_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">true_divide__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::dot(Tensor self, Tensor tensor) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dot</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::vdot(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">vdot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">vdot</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_empty_strided_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_empty_strided</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_full</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_full</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_full</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_full</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_full_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_full</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_full_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_full</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">fill_value</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_zeros</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_zeros</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_zeros_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_zeros</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_zeros_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_zeros</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_ones</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_ones</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_ones</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_ones</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_ones_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_ones</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">());</span>
<span class="p">}</span>

<span class="c1">// aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">new_ones_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">new_ones</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">resize_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resize_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">resize__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resize_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::erf(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::erf_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erf_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erf_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::erfc(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erfc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erfc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::erfc_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erfc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erfc_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::exp(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">exp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">exp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::exp_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">exp_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">exp_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::exp2(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">exp2</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">exp2</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::exp2_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">exp2_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">exp2_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::expm1(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expm1</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">expm1</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::expm1_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expm1_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">expm1_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expand</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">implicit</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">expand</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">implicit</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expand_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">implicit</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">expand</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">implicit</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::expand_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expand_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">expand_as</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">end_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flatten_using_ints</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="n">end_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flatten_named_out_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">out_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flatten_using_names</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">start_dim</span><span class="p">,</span><span class="w"> </span><span class="n">end_dim</span><span class="p">,</span><span class="w"> </span><span class="n">out_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dims</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">out_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flatten_DimnameList</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dims</span><span class="p">,</span><span class="w"> </span><span class="n">out_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unflatten</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unflatten_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">sizes</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unflatten_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unflatten_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">sizes</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unflatten</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unflatten_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">sizes</span><span class="p">),</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unflatten_symint</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">names</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unflatten_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">sizes</span><span class="p">,</span><span class="w"> </span><span class="n">names</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fill_.Scalar(Tensor(a!) self, Scalar value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fill__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fill_.Tensor(Tensor(a!) self, Tensor value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fill__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::floor(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::floor_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::floor_divide(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor_divide</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor_divide__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::floor_divide.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor_divide</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor_divide_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">floor_divide_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">floor_divide__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::frac(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">frac</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">frac</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::frac_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">frac_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">frac_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::gcd(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gcd</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gcd</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gcd_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gcd_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gcd_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lcm(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lcm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lcm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lcm_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lcm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lcm_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index.Tensor(Tensor self, Tensor?[] indices) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_copy_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_copy_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_copy</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_copy</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_copy_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_copy__dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_copy</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_copy_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_put_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_put_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">accumulate</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_put</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">List</span><span class="o">&lt;::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_put</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">values</span><span class="p">,</span><span class="w"> </span><span class="n">accumulate</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isclose</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">rtol</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">atol</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">equal_nan</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isclose</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">rtol</span><span class="p">,</span><span class="w"> </span><span class="n">atol</span><span class="p">,</span><span class="w"> </span><span class="n">equal_nan</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::isnan(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isnan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isnan</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_distributed(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_distributed</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_distributed</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_floating_point(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_floating_point</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_floating_point</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_complex(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_complex</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_complex</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_conj(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_conj</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_conj</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_is_zerotensor(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch__is_zerotensor</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_is_zerotensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_neg(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_neg</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::isreal(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isreal</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isreal</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_nonzero(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_nonzero</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_nonzero</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_same_size(Tensor self, Tensor other) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_same_size</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_same_size</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::is_signed(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_signed</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_signed</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_inference(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__dispatch_is_inference</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_inference</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::kron(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">kron</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">kron</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">kthvalue</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">kthvalue</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">kthvalue_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">kthvalue</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">kthvalue</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">kthvalue_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">kthvalue_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">kthvalue_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nan_to_num</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nan</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">posinf</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">neginf</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nan_to_num</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">nan</span><span class="p">,</span><span class="w"> </span><span class="n">posinf</span><span class="p">,</span><span class="w"> </span><span class="n">neginf</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nan_to_num_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">nan</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">posinf</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">neginf</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nan_to_num_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">nan</span><span class="p">,</span><span class="w"> </span><span class="n">posinf</span><span class="p">,</span><span class="w"> </span><span class="n">neginf</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ldexp.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ldexp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ldexp_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ldexp_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ldexp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ldexp_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::log(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log10(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log10</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log10</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log10_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log10_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log10_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log1p(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log1p</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log1p</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log1p_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log1p_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log1p_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log2(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log2</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log2</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::log2_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log2_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log2_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logaddexp(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logaddexp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logaddexp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logaddexp2(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logaddexp2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logaddexp2</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::xlogy.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">xlogy</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">xlogy_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::xlogy.Scalar_Other(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">xlogy</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">xlogy_Scalar_Other</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">xlogy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">xlogy__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">xlogy_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">xlogy__Scalar_Other</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log_softmax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log_softmax_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log_softmax_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logcumsumexp(Tensor self, int dim) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logcumsumexp</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logcumsumexp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logcumsumexp.dimname(Tensor self, Dimname dim) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logcumsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logcumsumexp_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logsumexp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logsumexp_names</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::matmul(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">matmul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">matmul</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::matrix_power(Tensor self, int n) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">matrix_power</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">matrix_power</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::matrix_exp(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">matrix_exp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">matrix_exp</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -&gt; (Tensor min, Tensor max)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">aminmax</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">aminmax</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::max.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">max_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">max_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">amax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">amax</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mean(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mean</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mean</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mean_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mean_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanmean</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanmean</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::median(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">median</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">median</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::median.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">median</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">median_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">median</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">median_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nanmedian(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanmedian</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanmedian</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanmedian</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanmedian_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanmedian_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::min.dim(Tensor self, int dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">min_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">min_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">amin</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">amin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mm(Tensor self, Tensor mat2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mode(Tensor self, int dim=-1, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mode</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mode</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mode</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mode_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mul.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mul_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mul_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mul__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mul.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mul_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mul_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mul__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multiply.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multiply</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multiply_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multiply_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multiply__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multiply.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multiply</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multiply_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multiply_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multiply__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mv(Tensor self, Tensor vec) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mv</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">vec</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mvlgamma(Tensor self, int p) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mvlgamma</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mvlgamma</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::mvlgamma_(Tensor(a!) self, int p) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mvlgamma_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mvlgamma_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow_copy</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow_copy</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow_copy_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow_copy</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">narrow_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">length</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">narrow_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::permute(Tensor(a) self, int[] dims) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">permute</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">permute</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">movedim</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">movedim_intlist</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">destination</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::movedim.int(Tensor(a) self, int source, int destination) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">movedim</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">movedim_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">destination</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">moveaxis</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">moveaxis_intlist</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">destination</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::moveaxis.int(Tensor(a) self, int source, int destination) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">moveaxis</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">destination</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">moveaxis_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">destination</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::numpy_T(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">numpy_T</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">numpy_T</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::matrix_H(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">matrix_H</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">matrix_H</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::mT(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mT</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mT</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::mH(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">mH</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">mH</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::adjoint(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">adjoint</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">adjoint</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_pinned(Tensor self, Device? device=None) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_pinned</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_pinned</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pin_memory(Tensor(a) self, Device? device=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pin_memory</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pin_memory</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">device</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pinverse(Tensor self, float rcond=1e-15) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pinverse</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">rcond</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pinverse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">rcond</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::rad2deg(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">rad2deg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rad2deg</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::rad2deg_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">rad2deg_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rad2deg_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::deg2rad(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">deg2rad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">deg2rad</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::deg2rad_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">deg2rad_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">deg2rad_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::ravel(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ravel</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ravel</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::reciprocal(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">reciprocal</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">reciprocal</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::reciprocal_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">reciprocal_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">reciprocal_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::neg(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">neg</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">neg</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::neg_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">neg_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">neg_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::negative(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">negative</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">negative</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::negative_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">negative_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">negative_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::repeat(Tensor self, SymInt[] repeats) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">repeats</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">repeats</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::repeat(Tensor self, SymInt[] repeats) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">repeats</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">repeats</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat_interleave_self_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">output_size</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat_interleave_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat_interleave_self_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat_interleave_self_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">output_size</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">repeat_interleave_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">repeat_interleave_self_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">repeats</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::reshape(Tensor(a) self, SymInt[] shape) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">reshape</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">shape</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">reshape</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">shape</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::reshape(Tensor(a) self, SymInt[] shape) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">reshape_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">shape</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">reshape</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">shape</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_reshape_alias</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_reshape_alias</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_reshape_alias_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_reshape_alias</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::reshape_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">reshape_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">reshape_as</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::round(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">round</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">round</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::round_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">round_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">round_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::round.decimals(Tensor self, *, int decimals) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">round</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">decimals</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">round_decimals</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">decimals</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::round_.decimals(Tensor(a!) self, *, int decimals) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">round_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">decimals</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">round__decimals</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">decimals</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::relu(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">relu</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">relu</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::relu_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">relu_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">relu_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::prelu(Tensor self, Tensor weight) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">prelu</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">prelu</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hardshrink(Tensor self, Scalar lambd=0.5) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hardshrink</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">lambd</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hardshrink</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">lambd</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hardshrink_backward</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">grad_out</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">lambd</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hardshrink_backward</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="n">grad_out</span><span class="p">,</span><span class="w"> </span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">lambd</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::rsqrt(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">rsqrt</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rsqrt</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::rsqrt_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">rsqrt_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rsqrt_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">select</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">select_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::select.int(Tensor(a) self, int dim, SymInt index) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">select</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">select_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::select.int(Tensor(a) self, int dim, SymInt index) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">select_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">select_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sigmoid(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sigmoid</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sigmoid</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sigmoid_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sigmoid_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sigmoid_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logit(Tensor self, float? eps=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logit</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">eps</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logit</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">eps</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::logit_(Tensor(a!) self, float? eps=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logit_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">eps</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logit_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">eps</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sin(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sin</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sin_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sin_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sin_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sinc(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sinc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sinc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sinc_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sinc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sinc_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sinh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sinh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sinh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sinh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sinh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sinh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::detach(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">detach</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">detach</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::detach_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">detach_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">detach_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::size.Dimname(Tensor self, Dimname dim) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">size</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">size_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">start</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">end</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice_symint</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice_inverse</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_inverse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">start</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">end</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice_inverse_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_inverse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">start</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">end</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slice_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slice_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">select_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">select_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">select_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">select_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diagonal_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diagonal_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">offset</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">,</span><span class="w"> </span><span class="n">dim2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">as_strided_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">),</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="p">(</span><span class="o">*</span><span class="n">storage_offset</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">as_strided_scatter_symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="o">&gt;</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">as_strided_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::smm(Tensor self, Tensor mat2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">smm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">smm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">softmax</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">softmax_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">softmax</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">softmax_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsafe_split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsafe_split_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsafe_split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsafe_split_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split.Tensor(Tensor(a -&gt; *) self, SymInt split_size, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split.Tensor(Tensor(a -&gt; *) self, SymInt split_size, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split.sizes(Tensor(a -&gt; *) self, SymInt[] split_size, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">split_size</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split.sizes(Tensor(a -&gt; *) self, SymInt[] split_size, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_size</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsafe_split_with_sizes</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsafe_split_with_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsafe_split_with_sizes_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsafe_split_with_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split_with_sizes(Tensor(a -&gt; *) self, SymInt[] split_sizes, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_with_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">split_sizes</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::split_with_sizes(Tensor(a -&gt; *) self, SymInt[] split_sizes, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">split_with_sizes_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">split_with_sizes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">split_sizes</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hsplit_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sections</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hsplit_array</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::vsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">vsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">vsplit_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sections</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::vsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">vsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">vsplit_array</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::dsplit.int(Tensor(a -&gt; *) self, int sections) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dsplit</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sections</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dsplit_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sections</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::dsplit.array(Tensor(a -&gt; *) self, int[] indices) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dsplit</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">indices</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dsplit_array</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze.dim(Tensor(a) self, int dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze.dimname(Tensor(a) self, Dimname dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze.dims(Tensor(a) self, int[] dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze_dims</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze_.dim(Tensor(a!) self, int dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze__dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze_.dims(Tensor(a!) self, int[] dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze__dims</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">squeeze_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">squeeze__dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sspaddmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sspaddmm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">stft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">return_complex</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">align_to_window</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">stft</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="n">return_complex</span><span class="p">,</span><span class="w"> </span><span class="n">align_to_window</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=&quot;reflect&quot;, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">stft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">pad_mode</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">return_complex</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">align_to_window</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">stft_center</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="n">pad_mode</span><span class="p">,</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="n">return_complex</span><span class="p">,</span><span class="w"> </span><span class="n">align_to_window</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">istft</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">return_complex</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">istft</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n_fft</span><span class="p">,</span><span class="w"> </span><span class="n">hop_length</span><span class="p">,</span><span class="w"> </span><span class="n">win_length</span><span class="p">,</span><span class="w"> </span><span class="n">window</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="p">,</span><span class="w"> </span><span class="n">normalized</span><span class="p">,</span><span class="w"> </span><span class="n">onesided</span><span class="p">,</span><span class="w"> </span><span class="n">length</span><span class="p">,</span><span class="w"> </span><span class="n">return_complex</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::stride.Dimname(Tensor self, Dimname dim) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">stride</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">stride_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sum(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sum</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sum</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sum_dim_IntList</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sum_dim_DimnameList</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nansum</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nansum</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hash_tensor(Tensor self, int[1] dim=[], *, bool keepdim=False, int mode=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hash_tensor</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">mode</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hash_tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">mode</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sum_to_size(Tensor self, SymInt[] size) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sum_to_size</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sum_to_size</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sum_to_size(Tensor self, SymInt[] size) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sum_to_size_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sum_to_size</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sqrt(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sqrt</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sqrt</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sqrt_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sqrt_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sqrt_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::square(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">square</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">square</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::square_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">square_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">square_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::std(Tensor self, bool unbiased=True) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">std</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">unbiased</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">std_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">std_correction</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">std_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">std</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">std_correction_names</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::prod(Tensor self, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">prod</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">prod</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">prod</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">prod_dim_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">prod</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">prod_dim_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::t(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">t</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">t</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::t_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">t_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">t_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tan(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tan</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tan</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tan_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tan_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tan_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tanh(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tanh</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tanh</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tanh_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tanh_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tanh_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tile(Tensor self, SymInt[] dims) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tile</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tile</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">dims</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::tile(Tensor self, SymInt[] dims) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tile_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tile</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::transpose.int(Tensor(a) self, int dim0, int dim1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">transpose</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">transpose_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">transpose</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">transpose_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::transpose_(Tensor(a!) self, int dim0, int dim1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">transpose_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">transpose_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::flip(Tensor self, int[] dims) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flip</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flip</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fliplr(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fliplr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fliplr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::flipud(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">flipud</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">flipud</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">roll</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">shifts</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">roll</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">shifts</span><span class="p">),</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">roll_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">shifts</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">roll</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">shifts</span><span class="p">,</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">rot90</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dims</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">rot90</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dims</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_nested_tensor_size(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_nested_tensor_size</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_nested_tensor_size</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_nested_tensor_strides(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_nested_tensor_strides</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_nested_tensor_strides</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_nested_tensor_storage_offsets(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_nested_tensor_storage_offsets</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_nested_tensor_storage_offsets</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::trunc(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">trunc</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">trunc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::trunc_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">trunc_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">trunc_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::fix(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fix</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fix</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::fix_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fix_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fix_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::type_as(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">type_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">type_as</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsqueeze(Tensor(a) self, int dim) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsqueeze</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsqueeze</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unsqueeze_(Tensor(a!) self, int dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unsqueeze_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::var(Tensor self, bool unbiased=True) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">var</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">var</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">unbiased</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">var_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">var_correction</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">var_names_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">unbiased</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">var</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">var_correction_names</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">correction</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::view_as(Tensor(a) self, Tensor other) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">view_as</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">view_as</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::where.self(Tensor condition, Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">where</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">where_self</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">where</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">where_ScalarOther</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span><span class="w"> </span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_ScalarOpt_dtype</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.Scalar(Tensor self, Scalar p=2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_ScalarOpt_dim_dtype</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_ScalarOpt_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_names_ScalarOpt_dim_dtype</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">norm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">DimnameList</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">norm_names_ScalarOpt_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::frexp.Tensor(Tensor self) -&gt; (Tensor mantissa, Tensor exponent)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">frexp</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">frexp_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">clone</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">clone</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::positive(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">positive</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">positive</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">resize_as_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">the_template</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resize_as_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">the_template</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">resize_as_sparse_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">the_template</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">resize_as_sparse_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">the_template</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::zero_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">zero_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">zero_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sub</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sub_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sub_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sub__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sub</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sub_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sub_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sub__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">subtract</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">subtract_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">subtract_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">subtract__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">subtract</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">subtract_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">subtract_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">subtract__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::heaviside(Tensor self, Tensor values) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">heaviside</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">heaviside</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">values</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::heaviside_(Tensor(a!) self, Tensor values) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">heaviside_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">values</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">heaviside_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">values</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addmm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addmm_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_addmm_activation</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">use_gelu</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_addmm_activation</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mat1</span><span class="p">,</span><span class="w"> </span><span class="n">mat2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">,</span><span class="w"> </span><span class="n">use_gelu</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sparse_resize_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sparse_resize_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sparse_resize_and_clear_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sparse_resize_and_clear_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sparse_mask(Tensor self, Tensor mask) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sparse_mask</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sparse_mask</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_sparse_mask_projection(Tensor self, Tensor mask, bool accumulate_matches=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_sparse_mask_projection</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate_matches</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_sparse_mask_projection</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">accumulate_matches</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_dense</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">masked_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_dense</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">masked_grad</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_dense(Tensor self, ScalarType? dtype=None, bool? masked_grad=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_dense</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">masked_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_dense</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">masked_grad</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sparse_dim(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sparse_dim</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sparse_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_dimI(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_dimI</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_dimI</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::dense_dim(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dense_dim</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dense_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_dimV(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_dimV</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_dimV</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_nnz(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_nnz</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_nnz</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::coalesce(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">coalesce</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">coalesce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_coalesced(Tensor self) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_coalesced</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_coalesced</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_values(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_values</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_values</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_coalesced_(Tensor(a!) self, bool coalesced) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_coalesced_</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">coalesced</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_coalesced_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">coalesced</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::values(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">values</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">values</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::crow_indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">crow_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">crow_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::col_indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">col_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">col_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::ccol_indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ccol_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ccol_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::row_indices(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">row_indices</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">row_indices</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::unbind.int(Tensor(a -&gt; *) self, int dim=0) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unbind</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unbind_int</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unbind.Dimname(Tensor(a -&gt; *) self, Dimname dim) -&gt; Tensor(a)[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unbind</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unbind_Dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse_sparse_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse_sparse_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">sparse_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse_csr(Tensor self, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse_csr</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse_csr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse_csr(Tensor self, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse_csr</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse_csr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse_csc(Tensor self, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse_csc</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse_csc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse_csc</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse_csc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse_bsr</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse_bsr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse_bsr</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse_bsr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_sparse_bsc</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_sparse_bsc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_to_sparse_bsc</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_to_sparse_bsc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">blocksize</span><span class="p">,</span><span class="w"> </span><span class="n">dense_dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_mkldnn</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_mkldnn</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::dequantize.self(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dequantize</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dequantize_self</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::q_scale(Tensor self) -&gt; float</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">q_scale</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">q_scale</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::q_zero_point(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">q_zero_point</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">q_zero_point</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::q_per_channel_scales(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">q_per_channel_scales</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">q_per_channel_scales</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::q_per_channel_zero_points(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">q_per_channel_zero_points</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">q_per_channel_zero_points</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::q_per_channel_axis(Tensor self) -&gt; int</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">q_per_channel_axis</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">q_per_channel_axis</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::int_repr(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">int_repr</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">int_repr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::qscheme(Tensor self) -&gt; QScheme</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">QScheme</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">qscheme</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">qscheme</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_autocast_to_reduced_precision</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">cuda_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">cpu_dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_autocast_to_reduced_precision</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">,</span><span class="w"> </span><span class="n">cuda_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">_autocast_to_full_precision</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">_autocast_to_full_precision</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">cuda_enabled</span><span class="p">,</span><span class="w"> </span><span class="n">cpu_enabled</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorOptions</span><span class="w"> </span><span class="n">options</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_dtype_layout</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">optTypeMetaToScalarType</span><span class="p">(</span><span class="n">options</span><span class="p">.</span><span class="n">dtype_opt</span><span class="p">()),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">layout_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">device_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">options</span><span class="p">.</span><span class="n">pinned_memory_opt</span><span class="p">(),</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">check_tensor_options_and_extract_memory_format</span><span class="p">(</span><span class="n">options</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Layout</span><span class="o">&gt;</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="o">&gt;</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_dtype_layout</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">layout</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">pin_memory</span><span class="p">,</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Device</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_device</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">device</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_dtype</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">MemoryFormat</span><span class="o">&gt;</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_other</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">non_blocking</span><span class="p">,</span><span class="w"> </span><span class="n">copy</span><span class="p">,</span><span class="w"> </span><span class="n">memory_format</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::item(Tensor self) -&gt; Scalar</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">item</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">item</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Storage(Tensor(a!) self, Storage source) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Storage</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Storage_storage_offset</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set__symint</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Storage</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Storage_storage_offset</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Tensor_storage_offset</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">stride</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set__symint</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">stride</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Tensor_storage_offset</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">storage_offset</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">stride</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set__source_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::set_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">set_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">set_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::is_set_to(Tensor self, Tensor tensor) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">is_set_to</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">is_set_to</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_fill__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_fill</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_fill_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_fill_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_fill__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_fill</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_fill_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_scatter_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_scatter_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_scatter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_scatter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">view</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">IntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">view</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="n">size</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::view(Tensor(a) self, SymInt[] size) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">view_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymIntArrayRef</span><span class="w"> </span><span class="n">size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">view</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::view.dtype(Tensor(a) self, ScalarType dtype) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">view</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">view_dtype</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">put_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">put_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">accumulate</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">put</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">accumulate</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">put</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">accumulate</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_add_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_add_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_add</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_add</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_add</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_add_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_reduce_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_reduce_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="n">include_self</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_reduce</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_reduce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">source</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="n">include_self</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill__int_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill_int_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill__int_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill_int_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill__Dimname_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill_</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill__Dimname_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill_Dimname_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_fill</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_fill_Dimname_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_src</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter__src</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_value</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter__value</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_reduce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter__reduce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_value_reduce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter__value_reduce</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_dimname_src</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_dimname_value</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_add</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_add</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_add_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_add_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_add</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_add_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_reduce</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_reduce_two</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="n">include_self</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">scatter_reduce_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">include_self</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">scatter_reduce__two</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">reduce</span><span class="p">,</span><span class="w"> </span><span class="n">include_self</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::eq_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">eq_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">eq__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::eq_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">eq_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">eq__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_and.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_and_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_and.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_and</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_and_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_and__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_and_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_and__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__and__.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__and__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__and___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__and__.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__and__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__and___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__iand__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__iand___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__iand__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__iand___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_or.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_or_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_or.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_or</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_or_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_or__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_or_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_or__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__or__.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__or__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__or___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__or__.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__or__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__or___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ior__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ior___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ior__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ior___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_xor.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_xor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_xor.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_xor</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_xor_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_xor__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_xor_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_xor__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__xor__.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__xor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__xor___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__xor__.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__xor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__xor___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ixor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ixor___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ixor__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ixor___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__lshift__.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__lshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__lshift___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__lshift__.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__lshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__lshift___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ilshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ilshift___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__ilshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__ilshift___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_left_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_left_shift_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_left_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_left_shift__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_left_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_left_shift_Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_left_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_left_shift__Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__rshift__.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__rshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__rshift___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__rshift__.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__rshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__rshift___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__irshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__irshift___Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">__irshift__</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">__irshift___Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_right_shift_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_right_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_right_shift__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_right_shift</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_right_shift_Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">bitwise_right_shift_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">bitwise_right_shift__Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tril_(Tensor(a!) self, SymInt diagonal=0) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tril_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tril_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tril_(Tensor(a!) self, SymInt diagonal=0) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tril__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tril_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::triu_(Tensor(a!) self, SymInt diagonal=0) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">triu_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">triu_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::triu_(Tensor(a!) self, SymInt diagonal=0) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">triu__symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">triu_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::digamma_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">digamma_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">digamma_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lerp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lerp__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lerp_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lerp__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addbmm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addbmm_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addbmm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">alpha</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addbmm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">batch1</span><span class="p">,</span><span class="w"> </span><span class="n">batch2</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">random_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">from</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">random__from</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">from</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">random_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">random__to</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::random_(Tensor(a!) self, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">random_</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">random_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">uniform_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">from</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">uniform_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">from</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cauchy_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">median</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cauchy_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">median</span><span class="p">,</span><span class="w"> </span><span class="n">sigma</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">log_normal_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">std</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">log_normal_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">exponential_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">lambd</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">exponential_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">lambd</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">geometric_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">geometric_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::diag(Tensor self, int diagonal=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">diag</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">diag</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cross(Tensor self, Tensor other, int? dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cross</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cross</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::triu(Tensor self, SymInt diagonal=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">triu</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">triu</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::triu(Tensor self, SymInt diagonal=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">triu_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">triu</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tril(Tensor self, SymInt diagonal=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tril</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tril</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::tril(Tensor self, SymInt diagonal=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">tril_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">diagonal</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">tril</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">diagonal</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::trace(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">trace</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">trace</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::ne.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ne</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ne_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ne.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ne</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ne_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ne_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ne_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ne__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ne_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ne_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ne__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::not_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">not_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">not_equal_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::not_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">not_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">not_equal_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">not_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">not_equal__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">not_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">not_equal__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::eq.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">eq</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">eq_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::eq.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">eq</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">eq_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ge.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ge_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ge.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ge_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ge_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ge_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ge__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ge_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ge_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ge__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_equal_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_equal_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_equal__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_equal__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::le.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">le</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">le_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::le.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">le</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">le_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::le_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">le_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">le__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::le_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">le_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">le__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_equal.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_equal_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_equal.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_equal_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_equal__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_equal_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_equal__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gt.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gt_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gt.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gt_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gt_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gt__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gt_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gt__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::greater_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">greater_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">greater__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lt.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lt_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lt.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lt</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lt_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lt_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lt__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lt_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lt_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lt__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::less_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">less_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">less__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::take(Tensor self, Tensor index) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">take</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">take</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">take_along_dim</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">take_along_dim</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">indices</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_select(Tensor self, int dim, Tensor index) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_select</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_select</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">index_select</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">index_select_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::masked_select(Tensor self, Tensor mask) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">masked_select</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">mask</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">masked_select</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mask</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nonzero(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nonzero</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nonzero</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nonzero_static</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">fill_value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nonzero_static</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">fill_value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nonzero_static_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">fill_value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nonzero_static</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">fill_value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nonzero_numpy(Tensor self) -&gt; Tensor[]</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nonzero_numpy</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nonzero_numpy</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::argwhere(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">argwhere</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argwhere</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gather</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sparse_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gather</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">sparse_grad</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">gather</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sparse_grad</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">gather_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">index</span><span class="p">,</span><span class="w"> </span><span class="n">sparse_grad</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addcmul</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addcmul</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addcmul_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addcmul_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addcdiv</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addcdiv</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">addcdiv_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">value</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">addcdiv_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">tensor1</span><span class="p">,</span><span class="w"> </span><span class="n">tensor2</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -&gt; (Tensor solution, Tensor cloned_coefficient)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">triangular_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">transpose</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">unitriangular</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">triangular_solve</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="p">,</span><span class="w"> </span><span class="n">transpose</span><span class="p">,</span><span class="w"> </span><span class="n">unitriangular</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::svd(Tensor self, bool some=True, bool compute_uv=True) -&gt; (Tensor U, Tensor S, Tensor V)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">svd</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">some</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">compute_uv</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">svd</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">some</span><span class="p">,</span><span class="w"> </span><span class="n">compute_uv</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::swapaxes(Tensor(a) self, int axis0, int axis1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">swapaxes</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">swapaxes</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="n">axis1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">swapaxes_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">axis1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">swapaxes_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">axis0</span><span class="p">,</span><span class="w"> </span><span class="n">axis1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::swapdims(Tensor(a) self, int dim0, int dim1) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">swapdims</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">swapdims</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">swapdims_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim1</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">swapdims_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim0</span><span class="p">,</span><span class="w"> </span><span class="n">dim1</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cholesky(Tensor self, bool upper=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cholesky</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cholesky</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">upper</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cholesky_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cholesky_solve</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="n">upper</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::cholesky_inverse(Tensor self, bool upper=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">cholesky_inverse</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">upper</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">cholesky_inverse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">upper</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::qr(Tensor self, bool some=True) -&gt; (Tensor Q, Tensor R)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">qr</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">some</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">qr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">some</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::geqrf(Tensor self) -&gt; (Tensor a, Tensor tau)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">geqrf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">geqrf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::orgqr(Tensor self, Tensor input2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">orgqr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">orgqr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">input2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ormqr</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">input3</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">left</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">transpose</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ormqr</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">input2</span><span class="p">,</span><span class="w"> </span><span class="n">input3</span><span class="p">,</span><span class="w"> </span><span class="n">left</span><span class="p">,</span><span class="w"> </span><span class="n">transpose</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lu_solve</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">LU_data</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">LU_pivots</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lu_solve</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">LU_data</span><span class="p">,</span><span class="w"> </span><span class="n">LU_pivots</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multinomial</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">replacement</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multinomial</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="n">replacement</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">multinomial_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">replacement</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">multinomial</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">num_samples</span><span class="p">,</span><span class="w"> </span><span class="n">replacement</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lgamma_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lgamma_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lgamma_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::lgamma(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lgamma</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lgamma</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::digamma(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">digamma</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">digamma</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::polygamma(int n, Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">polygamma</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">polygamma</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::polygamma_(Tensor(a!) self, int n) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">polygamma_</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">polygamma_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::erfinv(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erfinv</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erfinv</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::erfinv_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">erfinv_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">erfinv_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::i0(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">i0</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">i0</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::i0_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">i0_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">i0_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sign(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sign</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sign</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::sign_(Tensor(a!) self) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sign_</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sign_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::signbit(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">signbit</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">signbit</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::dist(Tensor self, Tensor other, Scalar p=2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">dist</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">dist</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">,</span><span class="w"> </span><span class="n">p</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::atan2_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">atan2_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atan2_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::atan2(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">atan2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">atan2</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::arctan2(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">arctan2</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctan2</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::arctan2_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">arctan2_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">arctan2_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lerp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lerp_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">lerp</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">lerp_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">end</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">histc</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">max</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">histc</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="n">min</span><span class="p">,</span><span class="w"> </span><span class="n">max</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor bin_edges)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">histogram</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">density</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">histogram_bins_tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">density</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -&gt; (Tensor hist, Tensor bin_edges)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">histogram</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ArrayRef</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">range</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">density</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">histogram_bin_ct</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">bins</span><span class="p">,</span><span class="w"> </span><span class="n">range</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">,</span><span class="w"> </span><span class="n">density</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fmod.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmod</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmod_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmod_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmod__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fmod.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmod</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmod_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmod_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmod__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hypot(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hypot</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hypot</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::hypot_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">hypot_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">hypot_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::igamma(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">igamma</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">igamma</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::igamma_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">igamma_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">igamma_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::igammac(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">igammac</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">igammac</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::igammac_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">igammac_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">igammac_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nextafter(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nextafter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nextafter</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nextafter_(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nextafter_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nextafter_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::remainder.Scalar(Tensor self, Scalar other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">remainder</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">remainder_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">remainder_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">remainder__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::remainder.Tensor(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">remainder</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">remainder_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">remainder_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">remainder__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::min(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">min</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">min</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::fmin(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmin</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmin</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::max(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">max</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">max</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::fmax(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">fmax</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">fmax</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::maximum(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">maximum</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">maximum</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::max.other(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">max</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">max_other</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::minimum(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">minimum</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">minimum</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::min.other(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">min</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">min_other</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=&#39;linear&#39;) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">quantile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">quantile</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">interpolation</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=&#39;linear&#39;) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">quantile</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">quantile_scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">interpolation</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=&#39;linear&#39;) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanquantile</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanquantile</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">interpolation</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=&#39;linear&#39;) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">nanquantile</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">int64_t</span><span class="o">&gt;</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">string_view</span><span class="w"> </span><span class="n">interpolation</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">nanquantile_scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">keepdim</span><span class="p">,</span><span class="w"> </span><span class="n">interpolation</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sort(Tensor self, int dim=-1, bool descending=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sort</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sort_stable</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sort_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">sort</span><span class="p">(</span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">sort_dimname_stable</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::msort(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">msort</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">msort</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::argsort(Tensor self, int dim=-1, bool descending=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">argsort</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argsort</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">argsort</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argsort_stable</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">stable</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">argsort</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Dimname</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">descending</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">argsort_dimname</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">descending</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">topk</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">largest</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sorted</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">topk</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">largest</span><span class="p">,</span><span class="w"> </span><span class="n">sorted</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -&gt; (Tensor values, Tensor indices)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">topk_symint</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">SymInt</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">largest</span><span class="p">,</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">sorted</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">topk</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">largest</span><span class="p">,</span><span class="w"> </span><span class="n">sorted</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::all(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">all</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">all</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::any(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">any</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">any</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">renorm</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">renorm</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">renorm_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">renorm_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">dim</span><span class="p">,</span><span class="w"> </span><span class="n">maxnorm</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::unfold(Tensor(a) self, int dimension, int size, int step) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">unfold</span><span class="p">(</span><span class="kt">int64_t</span><span class="w"> </span><span class="n">dimension</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">step</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">unfold</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">dimension</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="p">,</span><span class="w"> </span><span class="n">step</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::equal(Tensor self, Tensor other) -&gt; bool</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">equal</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">equal</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pow_Tensor_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pow_Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pow_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pow__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">pow_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">pow__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">float_power</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">float_power_Tensor_Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">float_power</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">float_power_Tensor_Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">float_power_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Scalar</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">float_power__Scalar</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">float_power_</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">exponent</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">float_power__Tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">exponent</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -&gt; Tensor(a!)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">normal_</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">std</span><span class="p">,</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Generator</span><span class="o">&gt;</span><span class="w"> </span><span class="n">generator</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">normal_</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">mean</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="p">,</span><span class="w"> </span><span class="n">generator</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::alias(Tensor(a) self) -&gt; Tensor(a)</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">alias</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">alias</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::isfinite(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isfinite</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isfinite</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::isinf(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isinf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isinf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::record_stream(Tensor(a!) self, Stream s) -&gt; ()</span>
<span class="kr">inline</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">record_stream</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Stream</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">record_stream</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">s</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::isposinf(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isposinf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isposinf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::isneginf(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">isneginf</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">isneginf</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::det(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">det</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">det</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::slogdet(Tensor self) -&gt; (Tensor sign, Tensor logabsdet)</span>
<span class="kr">inline</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">,</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">slogdet</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">slogdet</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::logdet(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">logdet</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">logdet</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::inverse(Tensor self) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">inverse</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">inverse</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">));</span>
<span class="p">}</span>

<span class="c1">// aten::inner(Tensor self, Tensor other) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">inner</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">inner</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">other</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::outer(Tensor self, Tensor vec2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">outer</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">outer</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">vec2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::ger(Tensor self, Tensor vec2) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">ger</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">vec2</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">ger</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">vec2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_padded_tensor</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalIntArrayRef</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_padded_tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">make_optional</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">fromIntArrayRefSlow</span><span class="p">(</span><span class="o">*</span><span class="n">output_size</span><span class="p">))</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="o">::</span><span class="n">std</span><span class="o">::</span><span class="n">nullopt</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -&gt; Tensor</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">to_padded_tensor_symint</span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">OptionalSymIntArrayRef</span><span class="w"> </span><span class="n">output_size</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">_ops</span><span class="o">::</span><span class="n">to_padded_tensor</span><span class="o">::</span><span class="n">call</span><span class="p">(</span><span class="k">const_cast</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&amp;&gt;</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">),</span><span class="w"> </span><span class="n">padding</span><span class="p">,</span><span class="w"> </span><span class="n">output_size</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace at</span>


<span class="k">namespace</span><span class="w"> </span><span class="nn">c10</span><span class="w"> </span><span class="p">{</span>
<span class="k">template</span><span class="w"> </span><span class="o">&lt;&gt;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">MaybeOwnedTraits</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">owned_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">borrow_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">borrow_type</span><span class="w"> </span><span class="nf">createBorrow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">owned_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">from</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// NOTE: this can be implemented without the special</span>
<span class="w">    </span><span class="c1">// unsafe_borrow_t Tensor constructor as</span>
<span class="w">    </span><span class="c1">//</span>
<span class="w">    </span><span class="c1">// return borrow_type(c10::intrusive_ptr&lt;at::TensorImpl, at::UndefinedTensorImpl&gt;::reclaim(from.unsafeGetTensorImpl()));</span>
<span class="w">    </span><span class="c1">//</span>
<span class="w">    </span><span class="c1">// but that hurts inlining due to the nullptr check in the</span>
<span class="w">    </span><span class="c1">// Tensor(c10::intrusive_ptr&lt;...&gt;) constructor. We already know</span>
<span class="w">    </span><span class="c1">// that from.impl_ isn&#39;t null because from is a valid Tensor, so</span>
<span class="w">    </span><span class="c1">// we needn&#39;t do the check again. (using __builtin_assume can</span>
<span class="w">    </span><span class="c1">// avoid this, but wouldn&#39;t be portable to MSVC.)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">borrow_type</span><span class="p">(</span><span class="n">borrow_type</span><span class="o">::</span><span class="n">unsafe_borrow_t</span><span class="p">{},</span><span class="w"> </span><span class="n">from</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">assignBorrow</span><span class="p">(</span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">lhs</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">rhs</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lhs</span><span class="p">.</span><span class="n">unsafeReleaseTensorImpl</span><span class="p">();</span>
<span class="w">    </span><span class="c1">// See above note: this can be implemented with public API</span>
<span class="w">    </span><span class="c1">// similarly to createBorrow(), but that would hurt inlining.</span>
<span class="w">    </span><span class="n">lhs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">borrow_type</span><span class="p">(</span><span class="n">borrow_type</span><span class="o">::</span><span class="n">unsafe_borrow_t</span><span class="p">{},</span><span class="w"> </span><span class="n">rhs</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">destroyBorrow</span><span class="p">(</span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">toDestroy</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">toDestroy</span><span class="p">.</span><span class="n">unsafeReleaseTensorImpl</span><span class="p">();</span><span class="w"> </span><span class="c1">// &quot;leak&quot; it, but it was already +0.</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">owned_type</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">referenceFromBorrow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">borrow</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">borrow</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">owned_type</span><span class="o">*</span><span class="w"> </span><span class="nf">pointerFromBorrow</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">borrow</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="n">borrow</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">debugBorrowIsValid</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">borrow_type</span><span class="o">&amp;</span><span class="w"> </span><span class="cm">/*borrow*/</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nb">true</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;&gt;</span>
<span class="k">struct</span><span class="w"> </span><span class="nc">ExclusivelyOwnedTraits</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">repr_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">pointer_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span><span class="p">;</span>
<span class="w">  </span><span class="k">using</span><span class="w"> </span><span class="n">const_pointer_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">*</span><span class="p">;</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">repr_type</span><span class="w"> </span><span class="nf">nullRepr</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">();</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">class</span><span class="p">...</span><span class="w"> </span><span class="n">Args</span><span class="o">&gt;</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">repr_type</span><span class="w"> </span><span class="n">createInPlace</span><span class="p">(</span><span class="n">Args</span><span class="o">&amp;&amp;</span><span class="p">...</span><span class="w"> </span><span class="n">args</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">forward</span><span class="o">&lt;</span><span class="n">Args</span><span class="o">&gt;</span><span class="p">(</span><span class="n">args</span><span class="p">)...);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">repr_type</span><span class="w"> </span><span class="n">moveToRepr</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">destroyOwned</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">ExclusivelyOwnedTraits</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&gt;::</span><span class="n">destroyOwned</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">take</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">pointer_type</span><span class="w"> </span><span class="n">getImpl</span><span class="p">(</span><span class="n">repr_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="n">const_pointer_type</span><span class="w"> </span><span class="n">getImpl</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">repr_type</span><span class="o">&amp;</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">&amp;</span><span class="n">x</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">};</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace c10</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="w"> </span><span class="p">{</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">borrow_from_optional_tensor</span><span class="p">(</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span><span class="w"> </span><span class="n">opt</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">opt</span><span class="p">.</span><span class="n">has_value</span><span class="p">()</span>
<span class="w">    </span><span class="o">?</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;::</span><span class="n">borrowed</span><span class="p">(</span><span class="o">*</span><span class="n">opt</span><span class="p">)</span>
<span class="w">    </span><span class="o">:</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;::</span><span class="n">owned</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">in_place</span><span class="p">);</span>
<span class="p">}</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Tensor</span><span class="o">::</span><span class="n">expect_contiguous</span><span class="p">(</span><span class="n">MemoryFormat</span><span class="w"> </span><span class="n">memory_format</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="p">))</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;::</span><span class="n">borrowed</span><span class="p">(</span><span class="o">*</span><span class="k">this</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">MaybeOwned</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&gt;::</span><span class="n">owned</span><span class="p">(</span><span class="n">__dispatch_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace at</span>
</pre></div>
</div>
</section>


                </article>
              
  </article>
  
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="feedback">
  
<div class="rating">
    Rate this Page
    <div class="stars">
        
        <span class="star" data-behavior="tutorial-rating" data-count="1" data-value="1"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="2" data-value="2"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="3" data-value="3"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="4" data-value="4"></span>
        
        <span class="star" data-behavior="tutorial-rating" data-count="5" data-value="5"></span>
        
    </div>
</div>

  <div class="feedback-send">
    <button class="feedback-btn"
            onclick="openGitHubIssue()"
            data-bs-title="Create a GitHub Issue"
            data-bs-placement="bottom"
            data-bs-toggle="tooltip"
            data-gtm="feedback-btn-click">Send Feedback
    </button>
  </div>
</div>

<div class="prev-next-area">
    <a class="left-prev"
       href="file_build_aten_src_ATen_core_TensorBody.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File TensorBody.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_aten_src_ATen_TensorOptions.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File TensorOptions.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>

<div class="footer-info">
  <p class="copyright">
    
  </p>

  <p class="theme-version">
    Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
  </p>
</div>
</div>
  
</div>
                </footer>
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="file_build_aten_src_ATen_core_TensorBody.h.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">File TensorBody.h</p>
      </div>
    </a>
    <a class="right-next"
       href="file_aten_src_ATen_TensorOptions.h.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">File TensorOptions.h</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc">
<div class="sidebar-secondary-items sidebar-secondary__inner">
    
       <div class="sidebar-secondary-item">
    <div class="tocsection sourcelink">
      <a href="../_sources/api/program_listing_file_build_aten_src_ATen_core_TensorBody.h.rst.txt">
        <i class="fa-solid fa-file-lines"></i> Show Source
      </a>
    </div>
</div>
    




<div class="sidebar-secondary-item">
  <div class="sidebar-heading">PyTorch Libraries</div>
  <ul style="list-style-type: none; padding: 0; padding-bottom: 80px;">
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/ao" style="color: var(--pst-color-text-muted)">torchao</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchrec" style="color: var(--pst-color-text-muted)">torchrec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchft" style="color: var(--pst-color-text-muted)">torchft</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/torchcodec" style="color: var(--pst-color-text-muted)">TorchCodec</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/vision" style="color: var(--pst-color-text-muted)">torchvision</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/executorch" style="color: var(--pst-color-text-muted)">ExecuTorch</a></li>
  
   <li><a class="nav-link nav-external" href="https://docs.pytorch.org/xla" style="color: var(--pst-color-text-muted)">PyTorch on XLA Devices</a></li>
  
  </ul>
</div>

</div>
</div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  

<div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/docs/stable/index.html">View Docs</a>
      </div>

      <div class="col-md-4">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://docs.pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>




<footer class="site-footer">

  <div class="container footer-container">

    <div class="newsletter" id="newsletter">

      <p class="newsletter__title is-style-max-width-800"><strong>Stay in touch</strong> for updates, event info, and
        the latest news</p>


      <script charset="utf-8" type="text/javascript" src="//js.hsforms.net/forms/embed/v2.js"></script>
      <script>
        hbspt.forms.create({
          region: "na1",
          portalId: "8112310",
          formId: "2fb2231c-000b-4ec5-88a0-1ab242549c9e"
        });
      </script>


      <p class="newsletter__privacy">By submitting this form, I consent to receive marketing emails from the LF and its
        projects regarding their events, training, research, developments, and related announcements. I understand that
        I can unsubscribe at any time using the links in the footers of the emails I receive. <a
          href="https://www.linuxfoundation.org/privacy/">Privacy Policy</a>.</p>

    </div>

    <div class="lf-grid">
      <ul class="social-links">
        <li><a href="https://www.facebook.com/pytorch" target="_blank" title="PyTorch on Facebook">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-0.51 -0.26 26.45 26.45" aria-label="Facebook">
              <path fill="currentColor"
                d="M25.497 13.075c0-2.45-.698-4.848-2.011-6.911a12.765 12.765 0 0 0-5.398-4.73A12.671 12.671 0 0 0 11.008.38a12.705 12.705 0 0 0-6.529 2.95A12.827 12.827 0 0 0 .563 9.358a12.896 12.896 0 0 0-.07 7.201 12.831 12.831 0 0 0 3.801 6.103 12.709 12.709 0 0 0 6.471 3.078v-8.957H7.53v-3.708h3.235v-2.824c0-3.213 1.903-4.988 4.813-4.988.956.014 1.909.097 2.852.25V8.67h-1.607a1.83 1.83 0 0 0-1.518.497 1.854 1.854 0 0 0-.561 1.505v2.404h3.535l-.563 3.708h-2.97v8.957a12.725 12.725 0 0 0 7.697-4.337 12.87 12.87 0 0 0 3.054-8.328z" />
            </svg>
          </a></li>
        <li><a href="https://twitter.com/pytorch" target="_blank" title="PyTorch on X">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0 0 300 300" aria-label="X">
              <path fill="currentColor"
                d="M178.57 127.15 290.27 0h-26.46l-97.03 110.38L89.34 0H0l117.13 166.93L0 300.25h26.46l102.4-116.59 81.8 116.59h89.34M36.01 19.54H76.66l187.13 262.13h-40.66" />
            </svg>
          </a></li>
        <li><a href="https://www.youtube.com/pytorch" target="_blank" title="PyTorch on YouTube">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="0.21 0.27 34.45 25.07" aria-label="YouTube">
              <path fill="currentColor"
                d="M33.729 6.084s-.327-2.33-1.317-3.356a4.691 4.691 0 0 0-3.32-1.432c-4.634-.34-11.589-.34-11.589-.34h-.014s-6.954 0-11.59.342a4.692 4.692 0 0 0-3.32 1.432c-.993 1.025-1.315 3.354-1.315 3.354a52.189 52.189 0 0 0-.331 5.473v2.566c.014 1.829.125 3.656.331 5.472 0 0 .322 2.33 1.316 3.36 1.26 1.345 2.916 1.3 3.653 1.445 2.65.26 11.263.34 11.263.34s6.96-.01 11.597-.353a4.691 4.691 0 0 0 3.32-1.432c.993-1.026 1.316-3.356 1.316-3.356.206-1.817.316-3.644.33-5.473v-2.57a52.26 52.26 0 0 0-.33-5.472zM14.076 17.232V7.729l8.951 4.768-8.95 4.735z" />
            </svg>
          </a></li>
        <li><a href="https://www.linkedin.com/company/pytorch" target="_blank" title="PyTorch on LinkedIn">
            <svg xmlns="http://www.w3.org/2000/svg" viewbox="-10.23 -10.23 531.96 531.96" aria-label="LinkedIn">
              <rect width="512" height="512" rx="0" fill="currentColor" />
              <circle fill="#000" cx="142" cy="138" r="37" />
              <path stroke="#000" stroke-width="66" d="M244 194v198M142 194v198" />
              <path fill="#000"
                d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32" />
            </svg>
          </a></li>
        <li><a href="https://pytorch.slack.com" target="_blank" title="PyTorch Slack">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.16 -0.03 21.19 21.19" aria-label="Slack">
              <path fill="currentColor"
                d="M4.896 13.27a2.147 2.147 0 0 1-2.141 2.142A2.147 2.147 0 0 1 .613 13.27c0-1.178.963-2.141 2.142-2.141h2.141v2.141zm1.08 0c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.363a2.147 2.147 0 0 1-2.142 2.141 2.147 2.147 0 0 1-2.141-2.142V13.27zm2.141-8.6a2.147 2.147 0 0 1-2.141-2.14c0-1.18.962-2.142 2.141-2.142s2.142.963 2.142 2.141v2.142H8.117zm0 1.08c1.179 0 2.141.962 2.141 2.141a2.147 2.147 0 0 1-2.141 2.142H2.755A2.147 2.147 0 0 1 .613 7.89c0-1.179.963-2.141 2.142-2.141h5.362zm8.599 2.141c0-1.179.963-2.141 2.141-2.141 1.179 0 2.143.962 2.143 2.14a2.147 2.147 0 0 1-2.142 2.142h-2.141V7.89zm-1.08 0a2.147 2.147 0 0 1-2.141 2.142 2.147 2.147 0 0 1-2.141-2.142V2.53c0-1.178.962-2.141 2.141-2.141s2.142.963 2.142 2.141v5.362zm-2.141 8.6c1.179 0 2.142.962 2.142 2.14a2.147 2.147 0 0 1-2.142 2.142 2.147 2.147 0 0 1-2.141-2.141V16.49h2.141zm0-1.08a2.147 2.147 0 0 1-2.141-2.141c0-1.179.962-2.142 2.141-2.142h5.362c1.179 0 2.142.963 2.142 2.142a2.147 2.147 0 0 1-2.142 2.142h-5.362z">
              </path>
            </svg>
          </a></li>
        <li><a href="https://pytorch.org/wechat" title="PyTorch on WeChat">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0.14 -0.17 38.02 33.02" aria-label="WeChat">
              <path fill="currentColor"
                d="M26.289 10.976a12.972 12.972 0 0 0-8.742 3.53 10.386 10.386 0 0 0-3.224 8.795c-1.326-.164-2.535-.345-3.75-.448a2.332 2.332 0 0 0-1.273.216c-1.18.666-2.311 1.418-3.652 2.255.246-1.112.405-2.087.687-3.024a1.15 1.15 0 0 0-.523-1.52C1.737 17.902.02 13.601 1.307 9.165c1.189-4.1 4.11-6.587 8.077-7.884A13.54 13.54 0 0 1 24.18 5.617a10.135 10.135 0 0 1 2.109 5.359zM10.668 9.594a1.564 1.564 0 0 0-2.095-1.472 1.52 1.52 0 0 0-.895 1.964 1.502 1.502 0 0 0 1.391.966 1.545 1.545 0 0 0 1.598-1.46v.002zm8.15-1.566a1.567 1.567 0 0 0-1.528 1.543 1.528 1.528 0 0 0 1.571 1.492 1.52 1.52 0 0 0 1.375-2.117 1.518 1.518 0 0 0-1.415-.919l-.003.001z">
              </path>
              <path fill="currentColor"
                d="M33.914 32.137c-1.075-.478-2.062-1.196-3.11-1.306-1.049-.11-2.145.494-3.24.605a10.821 10.821 0 0 1-8.781-2.864c-4.682-4.33-4.013-10.97 1.403-14.518 4.811-3.154 11.874-2.102 15.268 2.273a8.671 8.671 0 0 1-1.002 12.095c-1.046.929-1.422 1.693-.751 2.917.102.257.174.525.213.798zM21.68 20.292a1.264 1.264 0 1 0 .01-2.528 1.264 1.264 0 0 0-.01 2.528zm7.887-2.526a1.266 1.266 0 0 0-1.256 1.21 1.247 1.247 0 1 0 1.256-1.21z">
              </path>
            </svg>
          </a></li>
      </ul>
    </div>
    
    <div class="privacy-policy">
      <div class="copyright">
      
        <p>
          &copy; PyTorch. Copyright  The Linux Foundation. All rights reserved. The Linux Foundation has registered
          trademarks and uses trademarks. For more information, including terms of use, privacy policy, and trademark
          usage, please see our <a href="https://www.linuxfoundation.org/legal/policies">Policies</a> page. <a
            href="https://www.linuxfoundation.org/trademark-usage">Trademark Usage</a>. <a
            href="http://www.linuxfoundation.org/privacy">Privacy Policy</a>.
        </p>
        
      </div>
    </div>


  </div>
</footer>

<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/img/pytorch-x.svg">
  </div>
</div>
  
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
       Copyright PyTorch Contributors.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.15.4.
</p></div>
      
    </div>
  
</div>

  </footer>
  <script type="application/ld+json">
    {
       "@context": "https://schema.org",
       "@type": "Article",
       "name": "Program Listing for File TensorBody.h",
       "headline": "Program Listing for File TensorBody.h",
       "description": "PyTorch Documentation. Explore PyTorch, an open-source machine learning library that accelerates the path from research prototyping to production deployment. Discover tutorials, API references, and guides to help you build and deploy deep learning models efficiently.",
       "url": "/api/program_listing_file_build_aten_src_ATen_core_TensorBody.h.html",
       "articleBody": "Program Listing for File TensorBody.h# \u21b0 Return to documentation for file (build/aten/src/ATen/core/TensorBody.h) #pragma once #ifdef TORCH_ASSERT_NO_OPERATORS #error This change adds a dependency on native_functions.yaml, \\ meaning the file will need to be re-compiled every time an operator \\ is changed or added. Consider if your change would be better placed in \\ another file, or if a more specific header might achieve the same goal. \\ See NOTE: [Tensor vs. TensorBase] #endif #include \u003cc10/core/Device.h\u003e #include \u003cc10/core/Layout.h\u003e #include \u003cc10/core/MemoryFormat.h\u003e #include \u003cc10/core/QScheme.h\u003e #include \u003cc10/core/Stream.h\u003e #include \u003cc10/core/Scalar.h\u003e #include \u003cc10/core/ScalarType.h\u003e #include \u003cc10/core/ScalarTypeToTypeMeta.h\u003e #include \u003cc10/core/Storage.h\u003e #include \u003cc10/core/TensorImpl.h\u003e #include \u003cc10/core/UndefinedTensorImpl.h\u003e #include \u003cc10/core/WrapDimMinimal.h\u003e #include \u003cc10/util/Exception.h\u003e #include \u003cc10/util/ExclusivelyOwned.h\u003e #include \u003cc10/util/Deprecated.h\u003e #include \u003cc10/util/MaybeOwned.h\u003e #include \u003coptional\u003e #include \u003cc10/util/OptionalArrayRef.h\u003e #include \u003cc10/util/intrusive_ptr.h\u003e #include \u003cc10/macros/Export.h\u003e #include \u003cc10/macros/Macros.h\u003e #include \u003cATen/core/CheckMemoryFormat.h\u003e #include \u003cATen/core/DeprecatedTypePropertiesRegistry.h\u003e #include \u003cATen/core/DeprecatedTypeProperties.h\u003e #include \u003cATen/core/NamedTensor.h\u003e #include \u003cATen/core/QuantizerBase.h\u003e #include \u003cc10/core/SymInt.h\u003e #include \u003cATen/core/TensorAccessor.h\u003e #include \u003cATen/core/TensorBase.h\u003e #include \u003cATen/MethodOperators.h\u003e namespace c10{ template\u003cclass T\u003e class List; template\u003cclass T\u003e class IListRef; } namespace at { struct Generator; struct Type; class DeprecatedTypeProperties; class Tensor; } // namespace at namespace at { namespace indexing { struct TensorIndex; } // namespace indexing } // namespace at namespace torch { namespace autograd { struct Node; }} // namespace torch::autograd namespace at { class OptionalTensorRef; class TensorRef; class Tensor; using TensorList = ArrayRef\u003cTensor\u003e; using ITensorList = c10::IListRef\u003cTensor\u003e; using Stream = c10::Stream; // Tensor is a \"generic\" object holding a pointer to the underlying TensorImpl object, which // has an embedded reference count. In this way, Tensor is similar to boost::intrusive_ptr. // // For example: // // void func(Tensor a) { // Tensor b = a; // ... // } // // In this example, when we say Tensor b = a, we are creating a new object that points to the // same underlying TensorImpl, and bumps its reference count. When b goes out of scope, the // destructor decrements the reference count by calling release() on the TensorImpl it points to. // The existing constructors, operator overloads, etc. take care to implement the correct semantics. // // Note that Tensor can also be NULL, i.e. it is not associated with any underlying TensorImpl, and // special care must be taken to handle this. class TORCH_API Tensor: public TensorBase { protected: // Create a Tensor with a +0 reference count. Special care must be // taken to avoid decrementing this reference count at destruction // time. Intended to support MaybeOwnedTraits\u003cTensor\u003e. explicit Tensor(unsafe_borrow_t, const TensorBase\u0026 rhs): TensorBase(unsafe_borrow_t{}, rhs) {} friend MaybeOwnedTraits\u003cTensor\u003e; friend OptionalTensorRef; friend TensorRef; public: Tensor() = default; // This constructor should not be used by end users and is an implementation // detail invoked by autogenerated code. explicit Tensor( c10::intrusive_ptr\u003cTensorImpl, UndefinedTensorImpl\u003e tensor_impl) : TensorBase(std::move(tensor_impl)) {} Tensor(const Tensor \u0026tensor) = default; Tensor(Tensor \u0026\u0026tensor) = default; // Implicitly move-constructible from TensorBase, but must be explicit to increase refcount explicit Tensor(const TensorBase \u0026base): TensorBase(base) {} /*implicit*/ Tensor(TensorBase \u0026\u0026base): TensorBase(std::move(base)) {} // Creates a new wrapper from TensorImpl. Intentionally a free method because // it should be used with care. Checks necessary invariants static Tensor wrap_tensor_impl( c10::intrusive_ptr\u003cTensorImpl, UndefinedTensorImpl\u003e tensor_impl) { return TensorBase::wrap_tensor_impl(std::move(tensor_impl)); } Tensor contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) const { return TensorBase::contiguous(memory_format); } Tensor conj() const { if (!this-\u003eis_complex()) { return *this; } C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED(\"-Wswitch-enum\") switch (this-\u003elayout()) { case at::kSparse: case at::kSparseCsr: case at::kSparseCsc: case at::kSparseBsr: case at::kSparseBsc: return this-\u003econj_physical(); default: return this-\u003e_conj(); } C10_DIAGNOSTIC_POP() } // Aliased by Dimname overloads, so need explicit using using TensorBase::size; using TensorBase::sym_size; using TensorBase::stride; c10::MaybeOwned\u003cTensor\u003e expect_contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) const \u0026; // Use .contiguous() instead. Trying to borrow from a prvalue Tensor // will only lead to trouble and dangling references. c10::MaybeOwned\u003cTensor\u003e expect_contiguous(MemoryFormat memory_format=MemoryFormat::Contiguous) \u0026\u0026 = delete; // The following overloads are very intriguing. Consider the following // program: // // x[1] = 3; // // We would expect that the first entry of x is written to 3. But how can we // actually achieve this? x[1] evaluates to a tensor... // // The answer is, using a ref-qualifier. x[1] is an rvalue, which cannot be // (profitably) assigned to in the traditional sense, so we overload // assignment to mean, \"Actually, copy 3 into the tensor data.\" This is done // with an rvalue-reference ref-qualified overload (the methods with \u0026\u0026 at the // end of their type.) // // There\u0027s one more fly in the ointment: We also want // // Tensor x = y; // // to work, and we want it NOT to copy. So we need a traditional operator= // overload. But we MUST specify a mutable lvalue ref-qualifier, to // disambiguate the traditional overload from the rvalue-reference // ref-qualified overload. Otherwise, it will be ambiguous, because // a non ref-qualified method is eligible for all situations. // Unfortunately, we have to write these constructors out manually // to work around an MSVC bug: // error C2580: \u0027at::Tensor \u0026at::Tensor::operator =(const at::Tensor \u0026) \u0026\u0027: // multiple versions of a defaulted special member functions are not allowed // Tensor\u0026 operator=(const Tensor\u0026) \u0026 = default; // Tensor\u0026 operator=(Tensor\u0026\u0026) \u0026 = default; // Also MSVC will wrongly issue the following warning with the aforementioned fix // warning C4522: \u0027at::Tensor\u0027: multiple assignment operators specified // Let\u0027s just skip the warning. // // TODO: temporarily disabled Tensor\u0026 operator=(const TensorBase\u0026 x) \u0026 noexcept { impl_ = x.getIntrusivePtr(); return *this; } Tensor\u0026 operator=(TensorBase\u0026\u0026 x) \u0026 noexcept { impl_ = x.unsafeReleaseIntrusivePtr(); return *this; } Tensor\u0026 operator=(const Tensor \u0026x) \u0026 noexcept { return operator=(static_cast\u003cconst TensorBase\u0026\u003e(x)); } Tensor\u0026 operator=(Tensor \u0026\u0026x) \u0026 noexcept { return operator=(static_cast\u003cTensorBase\u0026\u0026\u003e(x)); } Tensor\u0026 operator=(const Scalar \u0026v) \u0026\u0026 { return fill_(v); } Tensor\u0026 operator=(const Tensor \u0026rhs) \u0026\u0026 { return copy_(rhs); } // NOLINTNEXTLINE(performance-noexcept-move-constructor) Tensor\u0026 operator=(Tensor\u0026\u0026 rhs) \u0026\u0026 { return copy_(rhs); } C10_DEPRECATED_MESSAGE(\"Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device().\") DeprecatedTypeProperties \u0026 type() const { return globalDeprecatedTypePropertiesRegistry().getDeprecatedTypeProperties( dispatchKeyToBackend(legacyExtractDispatchKey(key_set())), scalar_type()); } Tensor toType(ScalarType t) const { return to(options().dtype(t), /*non_blocking*/ false, /*copy*/ false); } // TODO: Deprecate me Tensor toBackend(Backend b) const { return to(options().device(backendToDeviceType(b)).layout(layout_from_backend(b)), /*non_blocking*/ false, /*copy*/ false); } C10_DEPRECATED_MESSAGE(\"Tensor.is_variable() is deprecated; everything is a variable now. (If you want to assert that variable has been appropriately handled already, use at::impl::variable_excluded_from_dispatch())\") bool is_variable() const noexcept { return !at::impl::variable_excluded_from_dispatch(); } template\u003ctypename T\u003e C10_DEPRECATED_MESSAGE(\"Tensor.data\u003cT\u003e() is deprecated. Please use Tensor.data_ptr\u003cT\u003e() instead.\") T * data() const { return data_ptr\u003cT\u003e(); } template \u003ctypename T\u003e T item() const; template\u003ctypename T, size_t N, template \u003ctypename U\u003e class PtrTraits = DefaultPtrTraits, typename index_t = int64_t\u003e C10_DEPRECATED_MESSAGE(\"packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead\") GenericPackedTensorAccessor\u003cT,N,PtrTraits,index_t\u003e packed_accessor() const \u0026 { return generic_packed_accessor\u003cT,N,PtrTraits,index_t\u003e(); } template\u003ctypename T, size_t N, template \u003ctypename U\u003e class PtrTraits = DefaultPtrTraits, typename index_t = int64_t\u003e C10_DEPRECATED_MESSAGE(\"packed_accessor is deprecated, use packed_accessor32 or packed_accessor64 instead\") GenericPackedTensorAccessor\u003cT,N,PtrTraits,index_t\u003e packed_accessor() \u0026\u0026 = delete; Tensor operator~() const { return bitwise_not(); } Tensor operator-() const { return neg(); } Tensor\u0026 operator+=(const Tensor \u0026 other) { return add_(other); } Tensor\u0026 operator+=(const Scalar \u0026 other) { return add_(other); } Tensor\u0026 operator-=(const Tensor \u0026 other) { return sub_(other); } Tensor\u0026 operator-=(const Scalar \u0026 other) { return sub_(other); } Tensor\u0026 operator*=(const Tensor \u0026 other) { return mul_(other); } Tensor\u0026 operator*=(const Scalar \u0026 other) { return mul_(other); } Tensor\u0026 operator/=(const Tensor \u0026 other) { return div_(other); } Tensor\u0026 operator/=(const Scalar \u0026 other) { return div_(other); } Tensor\u0026 operator\u0026=(const Tensor \u0026 other) { return bitwise_and_(other); } Tensor\u0026 operator|=(const Tensor \u0026 other) { return bitwise_or_(other); } Tensor\u0026 operator^=(const Tensor \u0026 other) { return bitwise_xor_(other); } Tensor operator[](const Scalar \u0026 index) const { if (!index.isIntegral(false)) { TORCH_CHECK_INDEX(false, \"Can only index tensors with integral scalars\"); } return this-\u003eoperator[](index.toLong()); } Tensor operator[](const Tensor \u0026 index) const { // These properties are checked in the Scalar constructor, but we already // check them here to provide more useful diagnostics for the user. if (!index.defined()) { TORCH_CHECK_INDEX(false, \"Can only index with tensors that are defined\"); } if (index.dim() != 0) { TORCH_CHECK_INDEX(false, \"Can only index with tensors that are scalars (zero-dim)\"); } // The Scalar(Tensor) constructor is explicit, so we need to call it. return this-\u003eoperator[](index.item()); } Tensor operator[](int64_t index) const { return select(0, index); } Tensor index(ArrayRef\u003cat::indexing::TensorIndex\u003e indices) const; Tensor index(std::initializer_list\u003cat::indexing::TensorIndex\u003e indices) const; Tensor \u0026 index_put_(ArrayRef\u003cat::indexing::TensorIndex\u003e indices, Tensor const \u0026 rhs); Tensor \u0026 index_put_(ArrayRef\u003cat::indexing::TensorIndex\u003e indices, const Scalar\u0026 v); Tensor \u0026 index_put_(std::initializer_list\u003cat::indexing::TensorIndex\u003e indices, Tensor const \u0026 rhs); Tensor \u0026 index_put_(std::initializer_list\u003cat::indexing::TensorIndex\u003e indices, const Scalar\u0026 v); Tensor cpu() const { return to(options().device(c10::DeviceType::CPU), /*non_blocking*/ false, /*copy*/ false); } // TODO: The Python version also accepts arguments Tensor cuda() const { return to(options().device(c10::DeviceType::CUDA), /*non_blocking*/ false, /*copy*/ false); } Tensor hip() const { return to(options().device(c10::DeviceType::HIP), /*non_blocking*/ false, /*copy*/ false); } Tensor ve() const { return to(options().device(c10::DeviceType::VE), /*non_blocking*/ false, /*copy*/ false); } Tensor vulkan() const { return to(options().device(c10::DeviceType::Vulkan), /*non_blocking*/ false, /*copy*/ false); } Tensor metal() const { return to(options().device(c10::DeviceType::Metal), /*non_blocking*/ false, /*copy*/ false); } Tensor meta() const { return to(options().device(c10::DeviceType::Meta), /*non_blocking*/ false, /*copy*/ false); } // ~~~~~ Autograd API ~~~~~ void backward(const Tensor \u0026 gradient={}, std::optional\u003cbool\u003e retain_graph=std::nullopt, bool create_graph=false, std::optional\u003cTensorList\u003e inputs=std::nullopt) const { // NB: Adding this wrapper to _backward here because we\u0027d like our // \u0027backwards\u0027 api to accept the \u0027inputs\u0027 argument optionally. Since code gen // currently does not support optional of TensorList our approach is to replace // backward in native_functions.yaml with _backward and call it here instead. if (inputs.has_value()) { TORCH_CHECK(inputs.value().size() \u003e 0, \"\u0027inputs\u0027 argument to backward cannot be empty\") this-\u003e_backward(inputs.value(), gradient, retain_graph, create_graph); } else { this-\u003e_backward({}, gradient, retain_graph, create_graph); } } const Tensor\u0026 set_requires_grad(bool requires_grad) const { TensorBase::set_requires_grad(requires_grad); return *this; } Tensor\u0026 mutable_grad() const { return impl_-\u003emutable_grad(); } const Tensor\u0026 grad() const { const Tensor\u0026 maybe_grad = impl_-\u003egrad(); if (!is_leaf() \u0026\u0026 !retains_grad() \u0026\u0026 !maybe_grad.defined()) { TORCH_WARN( \"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \" \"attribute won\u0027t be populated during autograd.backward(). If you indeed want the .grad \" \"field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. \" \"If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor \" \"instead. See github.com/pytorch/pytorch/pull/30531 for more information.\"); } return maybe_grad; } // The Forward AD API functions below are low level and are not to be used by end // users who should use the API provided in torch/csrc/autograd.h const Tensor\u0026 _fw_grad(uint64_t level) const { return impl_-\u003e_fw_grad(level, *this); } void _set_fw_grad(const TensorBase\u0026 new_grad, uint64_t level, bool is_inplace_op) const { impl_-\u003e_set_fw_grad(new_grad, *this, level, is_inplace_op); } // STOP. Thinking of adding a method here, which only makes use // of other ATen methods? Define it in native_functions.yaml. //example //Tensor * add(Tensor \u0026 b); void __dispatch__backward(at::TensorList inputs, const ::std::optional\u003cat::Tensor\u003e \u0026 gradient={}, ::std::optional\u003cbool\u003e retain_graph=::std::nullopt, bool create_graph=false) const; void __dispatch_set_data(const at::Tensor \u0026 new_data) const; at::Tensor __dispatch_data() const; bool __dispatch_is_leaf() const; int64_t __dispatch_output_nr() const; int64_t __dispatch__version() const; at::Tensor \u0026 __dispatch_requires_grad_(bool requires_grad=true) const; void __dispatch_retain_grad() const; bool __dispatch_retains_grad() const; at::Tensor _fw_primal(int64_t level) const; at::Tensor \u0026 rename_(::std::optional\u003cat::DimnameList\u003e names) const; at::Tensor rename(::std::optional\u003cat::DimnameList\u003e names) const; at::Tensor align_to(at::DimnameList names) const; at::Tensor align_to(at::DimnameList order, int64_t ellipsis_idx) const; at::Tensor align_as(const at::Tensor \u0026 other) const; at::Tensor refine_names(at::DimnameList names) const; at::Tensor abs() const; at::Tensor \u0026 abs_() const; at::Tensor absolute() const; at::Tensor \u0026 absolute_() const; at::Tensor angle() const; at::Tensor sgn() const; at::Tensor \u0026 sgn_() const; at::Tensor chalf(::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor _conj() const; at::Tensor __dispatch_conj() const; at::Tensor _conj_physical() const; at::Tensor conj_physical() const; at::Tensor \u0026 conj_physical_() const; at::Tensor resolve_conj() const; at::Tensor resolve_neg() const; at::Tensor _neg_view() const; at::Tensor acos() const; at::Tensor \u0026 acos_() const; at::Tensor arccos() const; at::Tensor \u0026 arccos_() const; at::Tensor add(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 add_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor add(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 add_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor addmv(const at::Tensor \u0026 mat, const at::Tensor \u0026 vec, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 addmv_(const at::Tensor \u0026 mat, const at::Tensor \u0026 vec, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor addr(const at::Tensor \u0026 vec1, const at::Tensor \u0026 vec2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 addr_(const at::Tensor \u0026 vec1, const at::Tensor \u0026 vec2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor _is_all_true() const; at::Tensor _is_any_true() const; at::Tensor all(int64_t dim, bool keepdim=false) const; at::Tensor all(at::OptionalIntArrayRef dim, bool keepdim=false) const; at::Tensor all(at::Dimname dim, bool keepdim=false) const; bool allclose(const at::Tensor \u0026 other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const; at::Tensor any(int64_t dim, bool keepdim=false) const; at::Tensor any(at::OptionalIntArrayRef dim, bool keepdim=false) const; at::Tensor any(at::Dimname dim, bool keepdim=false) const; at::Tensor argmax(::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false) const; at::Tensor argmin(::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false) const; at::Tensor acosh() const; at::Tensor \u0026 acosh_() const; at::Tensor arccosh() const; at::Tensor \u0026 arccosh_() const; at::Tensor asinh() const; at::Tensor \u0026 asinh_() const; at::Tensor arcsinh() const; at::Tensor \u0026 arcsinh_() const; at::Tensor atanh() const; at::Tensor \u0026 atanh_() const; at::Tensor arctanh() const; at::Tensor \u0026 arctanh_() const; at::Tensor as_strided(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset=::std::nullopt) const; at::Tensor as_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset=::std::nullopt) const; const at::Tensor \u0026 as_strided_(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset=::std::nullopt) const; const at::Tensor \u0026 as_strided__symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset=::std::nullopt) const; at::Tensor asin() const; at::Tensor \u0026 asin_() const; at::Tensor arcsin() const; at::Tensor \u0026 arcsin_() const; at::Tensor atan() const; at::Tensor \u0026 atan_() const; at::Tensor arctan() const; at::Tensor \u0026 arctan_() const; at::Tensor baddbmm(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 baddbmm_(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor bernoulli(::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 bernoulli_(const at::Tensor \u0026 p, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 bernoulli_(double p=0.5, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor bernoulli(double p, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor bincount(const ::std::optional\u003cat::Tensor\u003e \u0026 weights={}, int64_t minlength=0) const; at::Tensor bincount_symint(const ::std::optional\u003cat::Tensor\u003e \u0026 weights={}, c10::SymInt minlength=0) const; at::Tensor bitwise_not() const; at::Tensor \u0026 bitwise_not_() const; at::Tensor copysign(const at::Tensor \u0026 other) const; at::Tensor \u0026 copysign_(const at::Tensor \u0026 other) const; at::Tensor copysign(const at::Scalar \u0026 other) const; at::Tensor \u0026 copysign_(const at::Scalar \u0026 other) const; at::Tensor _lazy_clone() const; at::Tensor logical_not() const; at::Tensor \u0026 logical_not_() const; at::Tensor logical_xor(const at::Tensor \u0026 other) const; at::Tensor \u0026 logical_xor_(const at::Tensor \u0026 other) const; at::Tensor logical_and(const at::Tensor \u0026 other) const; at::Tensor \u0026 logical_and_(const at::Tensor \u0026 other) const; at::Tensor logical_or(const at::Tensor \u0026 other) const; at::Tensor \u0026 logical_or_(const at::Tensor \u0026 other) const; at::Tensor bmm(const at::Tensor \u0026 mat2) const; at::Tensor broadcast_to(at::IntArrayRef size) const; at::Tensor broadcast_to_symint(c10::SymIntArrayRef size) const; at::Tensor ceil() const; at::Tensor \u0026 ceil_() const; ::std::vector\u003cat::Tensor\u003e unsafe_chunk(int64_t chunks, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e chunk(int64_t chunks, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e tensor_split(int64_t sections, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e tensor_split_symint(c10::SymInt sections, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e tensor_split(at::IntArrayRef indices, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e tensor_split_symint(c10::SymIntArrayRef indices, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e tensor_split(const at::Tensor \u0026 tensor_indices_or_sections, int64_t dim=0) const; at::Tensor clamp(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max=::std::nullopt) const; at::Tensor clamp(const ::std::optional\u003cat::Tensor\u003e \u0026 min={}, const ::std::optional\u003cat::Tensor\u003e \u0026 max={}) const; at::Tensor \u0026 clamp_(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max=::std::nullopt) const; at::Tensor \u0026 clamp_(const ::std::optional\u003cat::Tensor\u003e \u0026 min={}, const ::std::optional\u003cat::Tensor\u003e \u0026 max={}) const; at::Tensor clamp_max(const at::Scalar \u0026 max) const; at::Tensor clamp_max(const at::Tensor \u0026 max) const; at::Tensor \u0026 clamp_max_(const at::Scalar \u0026 max) const; at::Tensor \u0026 clamp_max_(const at::Tensor \u0026 max) const; at::Tensor clamp_min(const at::Scalar \u0026 min) const; at::Tensor clamp_min(const at::Tensor \u0026 min) const; at::Tensor \u0026 clamp_min_(const at::Scalar \u0026 min) const; at::Tensor \u0026 clamp_min_(const at::Tensor \u0026 min) const; at::Tensor clip(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max=::std::nullopt) const; at::Tensor clip(const ::std::optional\u003cat::Tensor\u003e \u0026 min={}, const ::std::optional\u003cat::Tensor\u003e \u0026 max={}) const; at::Tensor \u0026 clip_(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max=::std::nullopt) const; at::Tensor \u0026 clip_(const ::std::optional\u003cat::Tensor\u003e \u0026 min={}, const ::std::optional\u003cat::Tensor\u003e \u0026 max={}) const; at::Tensor __dispatch_contiguous(at::MemoryFormat memory_format=c10::MemoryFormat::Contiguous) const; at::Tensor \u0026 copy_(const at::Tensor \u0026 src, bool non_blocking=false) const; at::Tensor cos() const; at::Tensor \u0026 cos_() const; at::Tensor cosh() const; at::Tensor \u0026 cosh_() const; at::Tensor count_nonzero(at::IntArrayRef dim) const; at::Tensor count_nonzero(::std::optional\u003cint64_t\u003e dim=::std::nullopt) const; at::Tensor cov(int64_t correction=1, const ::std::optional\u003cat::Tensor\u003e \u0026 fweights={}, const ::std::optional\u003cat::Tensor\u003e \u0026 aweights={}) const; at::Tensor corrcoef() const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e cummax(int64_t dim) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e cummax(at::Dimname dim) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e cummin(int64_t dim) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e cummin(at::Dimname dim) const; at::Tensor cumprod(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor \u0026 cumprod_(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor cumprod(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor \u0026 cumprod_(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor cumsum(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor \u0026 cumsum_(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor cumsum(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor \u0026 cumsum_(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor diag_embed(int64_t offset=0, int64_t dim1=-2, int64_t dim2=-1) const; at::Tensor diagflat(int64_t offset=0) const; at::Tensor diagonal(int64_t offset=0, int64_t dim1=0, int64_t dim2=1) const; at::Tensor diagonal(at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset=0) const; at::Tensor \u0026 fill_diagonal_(const at::Scalar \u0026 fill_value, bool wrap=false) const; at::Tensor diff(int64_t n=1, int64_t dim=-1, const ::std::optional\u003cat::Tensor\u003e \u0026 prepend={}, const ::std::optional\u003cat::Tensor\u003e \u0026 append={}) const; at::Tensor div(const at::Tensor \u0026 other) const; at::Tensor \u0026 div_(const at::Tensor \u0026 other) const; at::Tensor div(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor \u0026 div_(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor div(const at::Scalar \u0026 other) const; at::Tensor \u0026 div_(const at::Scalar \u0026 other) const; at::Tensor div(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor \u0026 div_(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor divide(const at::Tensor \u0026 other) const; at::Tensor \u0026 divide_(const at::Tensor \u0026 other) const; at::Tensor divide(const at::Scalar \u0026 other) const; at::Tensor \u0026 divide_(const at::Scalar \u0026 other) const; at::Tensor divide(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor \u0026 divide_(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor divide(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor \u0026 divide_(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const; at::Tensor true_divide(const at::Tensor \u0026 other) const; at::Tensor \u0026 true_divide_(const at::Tensor \u0026 other) const; at::Tensor true_divide(const at::Scalar \u0026 other) const; at::Tensor \u0026 true_divide_(const at::Scalar \u0026 other) const; at::Tensor dot(const at::Tensor \u0026 tensor) const; at::Tensor vdot(const at::Tensor \u0026 other) const; at::Tensor new_empty(at::IntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_empty(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_empty_symint(c10::SymIntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_empty_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_empty_strided(at::IntArrayRef size, at::IntArrayRef stride, at::TensorOptions options={}) const; at::Tensor new_empty_strided(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::TensorOptions options={}) const; at::Tensor new_empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_full(at::IntArrayRef size, const at::Scalar \u0026 fill_value, at::TensorOptions options={}) const; at::Tensor new_full(at::IntArrayRef size, const at::Scalar \u0026 fill_value, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_full_symint(c10::SymIntArrayRef size, const at::Scalar \u0026 fill_value, at::TensorOptions options={}) const; at::Tensor new_full_symint(c10::SymIntArrayRef size, const at::Scalar \u0026 fill_value, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_zeros(at::IntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_zeros(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_zeros_symint(c10::SymIntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_zeros_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_ones(at::IntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_ones(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; at::Tensor new_ones_symint(c10::SymIntArrayRef size, at::TensorOptions options={}) const; at::Tensor new_ones_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const; const at::Tensor \u0026 resize_(at::IntArrayRef size, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; const at::Tensor \u0026 resize__symint(c10::SymIntArrayRef size, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor erf() const; at::Tensor \u0026 erf_() const; at::Tensor erfc() const; at::Tensor \u0026 erfc_() const; at::Tensor exp() const; at::Tensor \u0026 exp_() const; at::Tensor exp2() const; at::Tensor \u0026 exp2_() const; at::Tensor expm1() const; at::Tensor \u0026 expm1_() const; at::Tensor expand(at::IntArrayRef size, bool implicit=false) const; at::Tensor expand_symint(c10::SymIntArrayRef size, bool implicit=false) const; at::Tensor expand_as(const at::Tensor \u0026 other) const; at::Tensor flatten(int64_t start_dim=0, int64_t end_dim=-1) const; at::Tensor flatten(int64_t start_dim, int64_t end_dim, at::Dimname out_dim) const; at::Tensor flatten(at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) const; at::Tensor flatten(at::DimnameList dims, at::Dimname out_dim) const; at::Tensor unflatten(int64_t dim, at::IntArrayRef sizes) const; at::Tensor unflatten_symint(int64_t dim, c10::SymIntArrayRef sizes) const; at::Tensor unflatten(at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) const; at::Tensor unflatten_symint(at::Dimname dim, c10::SymIntArrayRef sizes, at::DimnameList names) const; at::Tensor \u0026 fill_(const at::Scalar \u0026 value) const; at::Tensor \u0026 fill_(const at::Tensor \u0026 value) const; at::Tensor floor() const; at::Tensor \u0026 floor_() const; at::Tensor floor_divide(const at::Tensor \u0026 other) const; at::Tensor \u0026 floor_divide_(const at::Tensor \u0026 other) const; at::Tensor floor_divide(const at::Scalar \u0026 other) const; at::Tensor \u0026 floor_divide_(const at::Scalar \u0026 other) const; at::Tensor frac() const; at::Tensor \u0026 frac_() const; at::Tensor gcd(const at::Tensor \u0026 other) const; at::Tensor \u0026 gcd_(const at::Tensor \u0026 other) const; at::Tensor lcm(const at::Tensor \u0026 other) const; at::Tensor \u0026 lcm_(const at::Tensor \u0026 other) const; at::Tensor index(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices) const; at::Tensor \u0026 index_copy_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const; at::Tensor index_copy(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const; at::Tensor \u0026 index_copy_(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const; at::Tensor index_copy(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const; at::Tensor \u0026 index_put_(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices, const at::Tensor \u0026 values, bool accumulate=false) const; at::Tensor index_put(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices, const at::Tensor \u0026 values, bool accumulate=false) const; at::Tensor isclose(const at::Tensor \u0026 other, double rtol=1e-05, double atol=1e-08, bool equal_nan=false) const; at::Tensor isnan() const; bool is_distributed() const; bool __dispatch_is_floating_point() const; bool __dispatch_is_complex() const; bool __dispatch_is_conj() const; bool __dispatch__is_zerotensor() const; bool __dispatch_is_neg() const; at::Tensor isreal() const; bool is_nonzero() const; bool is_same_size(const at::Tensor \u0026 other) const; bool __dispatch_is_signed() const; bool __dispatch_is_inference() const; at::Tensor kron(const at::Tensor \u0026 other) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e kthvalue(int64_t k, int64_t dim=-1, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e kthvalue_symint(c10::SymInt k, int64_t dim=-1, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e kthvalue(int64_t k, at::Dimname dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e kthvalue_symint(c10::SymInt k, at::Dimname dim, bool keepdim=false) const; at::Tensor nan_to_num(::std::optional\u003cdouble\u003e nan=::std::nullopt, ::std::optional\u003cdouble\u003e posinf=::std::nullopt, ::std::optional\u003cdouble\u003e neginf=::std::nullopt) const; at::Tensor \u0026 nan_to_num_(::std::optional\u003cdouble\u003e nan=::std::nullopt, ::std::optional\u003cdouble\u003e posinf=::std::nullopt, ::std::optional\u003cdouble\u003e neginf=::std::nullopt) const; at::Tensor ldexp(const at::Tensor \u0026 other) const; at::Tensor \u0026 ldexp_(const at::Tensor \u0026 other) const; at::Tensor log() const; at::Tensor \u0026 log_() const; at::Tensor log10() const; at::Tensor \u0026 log10_() const; at::Tensor log1p() const; at::Tensor \u0026 log1p_() const; at::Tensor log2() const; at::Tensor \u0026 log2_() const; at::Tensor logaddexp(const at::Tensor \u0026 other) const; at::Tensor logaddexp2(const at::Tensor \u0026 other) const; at::Tensor xlogy(const at::Tensor \u0026 other) const; at::Tensor xlogy(const at::Scalar \u0026 other) const; at::Tensor \u0026 xlogy_(const at::Tensor \u0026 other) const; at::Tensor \u0026 xlogy_(const at::Scalar \u0026 other) const; at::Tensor log_softmax(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor log_softmax(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor logcumsumexp(int64_t dim) const; at::Tensor logcumsumexp(at::Dimname dim) const; at::Tensor logsumexp(at::IntArrayRef dim, bool keepdim=false) const; at::Tensor logsumexp(at::DimnameList dim, bool keepdim=false) const; at::Tensor matmul(const at::Tensor \u0026 other) const; at::Tensor matrix_power(int64_t n) const; at::Tensor matrix_exp() const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e aminmax(::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e max(int64_t dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e max(at::Dimname dim, bool keepdim=false) const; at::Tensor amax(at::IntArrayRef dim={}, bool keepdim=false) const; at::Tensor mean(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor mean(at::OptionalIntArrayRef dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor mean(at::DimnameList dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor nanmean(at::OptionalIntArrayRef dim=::std::nullopt, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor median() const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e median(int64_t dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e median(at::Dimname dim, bool keepdim=false) const; at::Tensor nanmedian() const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e nanmedian(int64_t dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e nanmedian(at::Dimname dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e min(int64_t dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e min(at::Dimname dim, bool keepdim=false) const; at::Tensor amin(at::IntArrayRef dim={}, bool keepdim=false) const; at::Tensor mm(const at::Tensor \u0026 mat2) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e mode(int64_t dim=-1, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e mode(at::Dimname dim, bool keepdim=false) const; at::Tensor mul(const at::Tensor \u0026 other) const; at::Tensor \u0026 mul_(const at::Tensor \u0026 other) const; at::Tensor mul(const at::Scalar \u0026 other) const; at::Tensor \u0026 mul_(const at::Scalar \u0026 other) const; at::Tensor multiply(const at::Tensor \u0026 other) const; at::Tensor \u0026 multiply_(const at::Tensor \u0026 other) const; at::Tensor multiply(const at::Scalar \u0026 other) const; at::Tensor \u0026 multiply_(const at::Scalar \u0026 other) const; at::Tensor mv(const at::Tensor \u0026 vec) const; at::Tensor mvlgamma(int64_t p) const; at::Tensor \u0026 mvlgamma_(int64_t p) const; at::Tensor narrow_copy(int64_t dim, int64_t start, int64_t length) const; at::Tensor narrow_copy_symint(int64_t dim, c10::SymInt start, c10::SymInt length) const; at::Tensor narrow(int64_t dim, int64_t start, int64_t length) const; at::Tensor narrow_symint(int64_t dim, c10::SymInt start, c10::SymInt length) const; at::Tensor narrow(int64_t dim, const at::Tensor \u0026 start, int64_t length) const; at::Tensor narrow_symint(int64_t dim, const at::Tensor \u0026 start, c10::SymInt length) const; at::Tensor permute(at::IntArrayRef dims) const; at::Tensor movedim(at::IntArrayRef source, at::IntArrayRef destination) const; at::Tensor movedim(int64_t source, int64_t destination) const; at::Tensor moveaxis(at::IntArrayRef source, at::IntArrayRef destination) const; at::Tensor moveaxis(int64_t source, int64_t destination) const; at::Tensor numpy_T() const; at::Tensor matrix_H() const; at::Tensor mT() const; at::Tensor mH() const; at::Tensor adjoint() const; bool is_pinned(::std::optional\u003cat::Device\u003e device=::std::nullopt) const; at::Tensor pin_memory(::std::optional\u003cat::Device\u003e device=::std::nullopt) const; at::Tensor pinverse(double rcond=1e-15) const; at::Tensor rad2deg() const; at::Tensor \u0026 rad2deg_() const; at::Tensor deg2rad() const; at::Tensor \u0026 deg2rad_() const; at::Tensor ravel() const; at::Tensor reciprocal() const; at::Tensor \u0026 reciprocal_() const; at::Tensor neg() const; at::Tensor \u0026 neg_() const; at::Tensor negative() const; at::Tensor \u0026 negative_() const; at::Tensor repeat(at::IntArrayRef repeats) const; at::Tensor repeat_symint(c10::SymIntArrayRef repeats) const; at::Tensor repeat_interleave(const at::Tensor \u0026 repeats, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, ::std::optional\u003cint64_t\u003e output_size=::std::nullopt) const; at::Tensor repeat_interleave_symint(const at::Tensor \u0026 repeats, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, ::std::optional\u003cc10::SymInt\u003e output_size=::std::nullopt) const; at::Tensor repeat_interleave(int64_t repeats, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, ::std::optional\u003cint64_t\u003e output_size=::std::nullopt) const; at::Tensor repeat_interleave_symint(c10::SymInt repeats, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, ::std::optional\u003cc10::SymInt\u003e output_size=::std::nullopt) const; at::Tensor reshape(at::IntArrayRef shape) const; at::Tensor reshape_symint(c10::SymIntArrayRef shape) const; at::Tensor _reshape_alias(at::IntArrayRef size, at::IntArrayRef stride) const; at::Tensor _reshape_alias_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride) const; at::Tensor reshape_as(const at::Tensor \u0026 other) const; at::Tensor round() const; at::Tensor \u0026 round_() const; at::Tensor round(int64_t decimals) const; at::Tensor \u0026 round_(int64_t decimals) const; at::Tensor relu() const; at::Tensor \u0026 relu_() const; at::Tensor prelu(const at::Tensor \u0026 weight) const; at::Tensor hardshrink(const at::Scalar \u0026 lambd=0.5) const; at::Tensor hardshrink_backward(const at::Tensor \u0026 grad_out, const at::Scalar \u0026 lambd) const; at::Tensor rsqrt() const; at::Tensor \u0026 rsqrt_() const; at::Tensor select(at::Dimname dim, int64_t index) const; at::Tensor select(int64_t dim, int64_t index) const; at::Tensor select_symint(int64_t dim, c10::SymInt index) const; at::Tensor sigmoid() const; at::Tensor \u0026 sigmoid_() const; at::Tensor logit(::std::optional\u003cdouble\u003e eps=::std::nullopt) const; at::Tensor \u0026 logit_(::std::optional\u003cdouble\u003e eps=::std::nullopt) const; at::Tensor sin() const; at::Tensor \u0026 sin_() const; at::Tensor sinc() const; at::Tensor \u0026 sinc_() const; at::Tensor sinh() const; at::Tensor \u0026 sinh_() const; at::Tensor detach() const; at::Tensor \u0026 detach_() const; int64_t size(at::Dimname dim) const; at::Tensor slice(int64_t dim=0, ::std::optional\u003cint64_t\u003e start=::std::nullopt, ::std::optional\u003cint64_t\u003e end=::std::nullopt, int64_t step=1) const; at::Tensor slice_symint(int64_t dim=0, ::std::optional\u003cc10::SymInt\u003e start=::std::nullopt, ::std::optional\u003cc10::SymInt\u003e end=::std::nullopt, c10::SymInt step=1) const; at::Tensor slice_inverse(const at::Tensor \u0026 src, int64_t dim=0, ::std::optional\u003cint64_t\u003e start=::std::nullopt, ::std::optional\u003cint64_t\u003e end=::std::nullopt, int64_t step=1) const; at::Tensor slice_inverse_symint(const at::Tensor \u0026 src, int64_t dim=0, ::std::optional\u003cc10::SymInt\u003e start=::std::nullopt, ::std::optional\u003cc10::SymInt\u003e end=::std::nullopt, c10::SymInt step=1) const; at::Tensor slice_scatter(const at::Tensor \u0026 src, int64_t dim=0, ::std::optional\u003cint64_t\u003e start=::std::nullopt, ::std::optional\u003cint64_t\u003e end=::std::nullopt, int64_t step=1) const; at::Tensor slice_scatter_symint(const at::Tensor \u0026 src, int64_t dim=0, ::std::optional\u003cc10::SymInt\u003e start=::std::nullopt, ::std::optional\u003cc10::SymInt\u003e end=::std::nullopt, c10::SymInt step=1) const; at::Tensor select_scatter(const at::Tensor \u0026 src, int64_t dim, int64_t index) const; at::Tensor select_scatter_symint(const at::Tensor \u0026 src, int64_t dim, c10::SymInt index) const; at::Tensor diagonal_scatter(const at::Tensor \u0026 src, int64_t offset=0, int64_t dim1=0, int64_t dim2=1) const; at::Tensor as_strided_scatter(const at::Tensor \u0026 src, at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset=::std::nullopt) const; at::Tensor as_strided_scatter_symint(const at::Tensor \u0026 src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset=::std::nullopt) const; at::Tensor smm(const at::Tensor \u0026 mat2) const; at::Tensor softmax(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor softmax(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; ::std::vector\u003cat::Tensor\u003e unsafe_split(int64_t split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e unsafe_split_symint(c10::SymInt split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split(int64_t split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split_symint(c10::SymInt split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split(at::IntArrayRef split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split_symint(c10::SymIntArrayRef split_size, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e unsafe_split_with_sizes(at::IntArrayRef split_sizes, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e unsafe_split_with_sizes_symint(c10::SymIntArrayRef split_sizes, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split_with_sizes(at::IntArrayRef split_sizes, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e split_with_sizes_symint(c10::SymIntArrayRef split_sizes, int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e hsplit(int64_t sections) const; ::std::vector\u003cat::Tensor\u003e hsplit(at::IntArrayRef indices) const; ::std::vector\u003cat::Tensor\u003e vsplit(int64_t sections) const; ::std::vector\u003cat::Tensor\u003e vsplit(at::IntArrayRef indices) const; ::std::vector\u003cat::Tensor\u003e dsplit(int64_t sections) const; ::std::vector\u003cat::Tensor\u003e dsplit(at::IntArrayRef indices) const; at::Tensor squeeze() const; at::Tensor squeeze(int64_t dim) const; at::Tensor squeeze(at::Dimname dim) const; at::Tensor squeeze(at::IntArrayRef dim) const; at::Tensor \u0026 squeeze_() const; at::Tensor \u0026 squeeze_(int64_t dim) const; at::Tensor \u0026 squeeze_(at::IntArrayRef dim) const; at::Tensor \u0026 squeeze_(at::Dimname dim) const; at::Tensor sspaddmm(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor stft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length, ::std::optional\u003cint64_t\u003e win_length, const ::std::optional\u003cat::Tensor\u003e \u0026 window, bool normalized, ::std::optional\u003cbool\u003e onesided=::std::nullopt, ::std::optional\u003cbool\u003e return_complex=::std::nullopt, ::std::optional\u003cbool\u003e align_to_window=::std::nullopt) const; at::Tensor stft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length=::std::nullopt, ::std::optional\u003cint64_t\u003e win_length=::std::nullopt, const ::std::optional\u003cat::Tensor\u003e \u0026 window={}, bool center=true, c10::string_view pad_mode=\"reflect\", bool normalized=false, ::std::optional\u003cbool\u003e onesided=::std::nullopt, ::std::optional\u003cbool\u003e return_complex=::std::nullopt, ::std::optional\u003cbool\u003e align_to_window=::std::nullopt) const; at::Tensor istft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length=::std::nullopt, ::std::optional\u003cint64_t\u003e win_length=::std::nullopt, const ::std::optional\u003cat::Tensor\u003e \u0026 window={}, bool center=true, bool normalized=false, ::std::optional\u003cbool\u003e onesided=::std::nullopt, ::std::optional\u003cint64_t\u003e length=::std::nullopt, bool return_complex=false) const; int64_t stride(at::Dimname dim) const; at::Tensor sum(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor sum(at::OptionalIntArrayRef dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor sum(at::DimnameList dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor nansum(at::OptionalIntArrayRef dim=::std::nullopt, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor hash_tensor(at::IntArrayRef dim={}, bool keepdim=false, int64_t mode=0) const; at::Tensor sum_to_size(at::IntArrayRef size) const; at::Tensor sum_to_size_symint(c10::SymIntArrayRef size) const; at::Tensor sqrt() const; at::Tensor \u0026 sqrt_() const; at::Tensor square() const; at::Tensor \u0026 square_() const; at::Tensor std(bool unbiased) const; at::Tensor std(at::OptionalIntArrayRef dim, bool unbiased, bool keepdim=false) const; at::Tensor std(at::OptionalIntArrayRef dim=::std::nullopt, const ::std::optional\u003cat::Scalar\u003e \u0026 correction=::std::nullopt, bool keepdim=false) const; at::Tensor std(at::DimnameList dim, bool unbiased, bool keepdim=false) const; at::Tensor std(at::DimnameList dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction=::std::nullopt, bool keepdim=false) const; at::Tensor prod(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor prod(int64_t dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor prod(at::Dimname dim, bool keepdim=false, ::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor t() const; at::Tensor \u0026 t_() const; at::Tensor tan() const; at::Tensor \u0026 tan_() const; at::Tensor tanh() const; at::Tensor \u0026 tanh_() const; at::Tensor tile(at::IntArrayRef dims) const; at::Tensor tile_symint(c10::SymIntArrayRef dims) const; at::Tensor transpose(int64_t dim0, int64_t dim1) const; at::Tensor transpose(at::Dimname dim0, at::Dimname dim1) const; at::Tensor \u0026 transpose_(int64_t dim0, int64_t dim1) const; at::Tensor flip(at::IntArrayRef dims) const; at::Tensor fliplr() const; at::Tensor flipud() const; at::Tensor roll(at::IntArrayRef shifts, at::IntArrayRef dims={}) const; at::Tensor roll_symint(c10::SymIntArrayRef shifts, at::IntArrayRef dims={}) const; at::Tensor rot90(int64_t k=1, at::IntArrayRef dims={0,1}) const; at::Tensor _nested_tensor_size() const; at::Tensor _nested_tensor_strides() const; at::Tensor _nested_tensor_storage_offsets() const; at::Tensor trunc() const; at::Tensor \u0026 trunc_() const; at::Tensor fix() const; at::Tensor \u0026 fix_() const; at::Tensor type_as(const at::Tensor \u0026 other) const; at::Tensor unsqueeze(int64_t dim) const; at::Tensor \u0026 unsqueeze_(int64_t dim) const; at::Tensor var(bool unbiased) const; at::Tensor var(at::OptionalIntArrayRef dim, bool unbiased, bool keepdim=false) const; at::Tensor var(at::OptionalIntArrayRef dim=::std::nullopt, const ::std::optional\u003cat::Scalar\u003e \u0026 correction=::std::nullopt, bool keepdim=false) const; at::Tensor var(at::DimnameList dim, bool unbiased, bool keepdim=false) const; at::Tensor var(at::DimnameList dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction=::std::nullopt, bool keepdim=false) const; at::Tensor view_as(const at::Tensor \u0026 other) const; at::Tensor where(const at::Tensor \u0026 condition, const at::Tensor \u0026 other) const; at::Tensor where(const at::Tensor \u0026 condition, const at::Scalar \u0026 other) const; at::Tensor norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::ScalarType dtype) const; at::Tensor norm(const at::Scalar \u0026 p=2) const; at::Tensor norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) const; at::Tensor norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::IntArrayRef dim, bool keepdim=false) const; at::Tensor norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) const; at::Tensor norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::DimnameList dim, bool keepdim=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e frexp() const; at::Tensor clone(::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor positive() const; const at::Tensor \u0026 resize_as_(const at::Tensor \u0026 the_template, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; const at::Tensor \u0026 resize_as_sparse_(const at::Tensor \u0026 the_template) const; at::Tensor \u0026 zero_() const; at::Tensor sub(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 sub_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor sub(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 sub_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor subtract(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 subtract_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor subtract(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 subtract_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha=1) const; at::Tensor heaviside(const at::Tensor \u0026 values) const; at::Tensor \u0026 heaviside_(const at::Tensor \u0026 values) const; at::Tensor addmm(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 addmm_(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor _addmm_activation(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1, bool use_gelu=false) const; const at::Tensor \u0026 sparse_resize_(at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const; const at::Tensor \u0026 sparse_resize_and_clear_(at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const; at::Tensor sparse_mask(const at::Tensor \u0026 mask) const; at::Tensor _sparse_mask_projection(const at::Tensor \u0026 mask, bool accumulate_matches=false) const; at::Tensor to_dense(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt, ::std::optional\u003cbool\u003e masked_grad=::std::nullopt) const; at::Tensor _to_dense(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt, ::std::optional\u003cbool\u003e masked_grad=::std::nullopt) const; int64_t sparse_dim() const; int64_t _dimI() const; int64_t dense_dim() const; int64_t _dimV() const; int64_t _nnz() const; at::Tensor coalesce() const; bool is_coalesced() const; at::Tensor _indices() const; at::Tensor _values() const; at::Tensor \u0026 _coalesced_(bool coalesced) const; at::Tensor indices() const; at::Tensor values() const; at::Tensor crow_indices() const; at::Tensor col_indices() const; at::Tensor ccol_indices() const; at::Tensor row_indices() const; ::std::vector\u003cat::Tensor\u003e unbind(int64_t dim=0) const; ::std::vector\u003cat::Tensor\u003e unbind(at::Dimname dim) const; at::Tensor to_sparse(int64_t sparse_dim) const; at::Tensor _to_sparse(int64_t sparse_dim) const; at::Tensor to_sparse(::std::optional\u003cat::Layout\u003e layout=::std::nullopt, at::OptionalIntArrayRef blocksize=::std::nullopt, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor _to_sparse(::std::optional\u003cat::Layout\u003e layout=::std::nullopt, at::OptionalIntArrayRef blocksize=::std::nullopt, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor to_sparse_csr(::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor _to_sparse_csr(::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor to_sparse_csc(::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor _to_sparse_csc(::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor to_sparse_bsr(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor _to_sparse_bsr(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor to_sparse_bsc(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor _to_sparse_bsc(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim=::std::nullopt) const; at::Tensor to_mkldnn(::std::optional\u003cat::ScalarType\u003e dtype=::std::nullopt) const; at::Tensor dequantize() const; double q_scale() const; int64_t q_zero_point() const; at::Tensor q_per_channel_scales() const; at::Tensor q_per_channel_zero_points() const; int64_t q_per_channel_axis() const; at::Tensor int_repr() const; at::QScheme qscheme() const; at::Tensor _autocast_to_reduced_precision(bool cuda_enabled, bool cpu_enabled, at::ScalarType cuda_dtype, at::ScalarType cpu_dtype) const; at::Tensor _autocast_to_full_precision(bool cuda_enabled, bool cpu_enabled) const; at::Tensor to(at::TensorOptions options={}, bool non_blocking=false, bool copy=false, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor to(::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const; at::Tensor to(at::Device device, at::ScalarType dtype, bool non_blocking=false, bool copy=false, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor to(at::ScalarType dtype, bool non_blocking=false, bool copy=false, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Tensor to(const at::Tensor \u0026 other, bool non_blocking=false, bool copy=false, ::std::optional\u003cat::MemoryFormat\u003e memory_format=::std::nullopt) const; at::Scalar item() const; at::Tensor \u0026 set_(at::Storage source) const; at::Tensor \u0026 set_(at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride={}) const; at::Tensor \u0026 set__symint(at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride={}) const; at::Tensor \u0026 set_(const at::Tensor \u0026 source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride={}) const; at::Tensor \u0026 set__symint(const at::Tensor \u0026 source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride={}) const; at::Tensor \u0026 set_(const at::Tensor \u0026 source) const; at::Tensor \u0026 set_() const; bool is_set_to(const at::Tensor \u0026 tensor) const; at::Tensor \u0026 masked_fill_(const at::Tensor \u0026 mask, const at::Scalar \u0026 value) const; at::Tensor masked_fill(const at::Tensor \u0026 mask, const at::Scalar \u0026 value) const; at::Tensor \u0026 masked_fill_(const at::Tensor \u0026 mask, const at::Tensor \u0026 value) const; at::Tensor masked_fill(const at::Tensor \u0026 mask, const at::Tensor \u0026 value) const; at::Tensor \u0026 masked_scatter_(const at::Tensor \u0026 mask, const at::Tensor \u0026 source) const; at::Tensor masked_scatter(const at::Tensor \u0026 mask, const at::Tensor \u0026 source) const; at::Tensor view(at::IntArrayRef size) const; at::Tensor view_symint(c10::SymIntArrayRef size) const; at::Tensor view(at::ScalarType dtype) const; at::Tensor \u0026 put_(const at::Tensor \u0026 index, const at::Tensor \u0026 source, bool accumulate=false) const; at::Tensor put(const at::Tensor \u0026 index, const at::Tensor \u0026 source, bool accumulate=false) const; at::Tensor \u0026 index_add_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha=1) const; at::Tensor index_add(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha=1) const; at::Tensor index_add(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 index_reduce_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, c10::string_view reduce, bool include_self=true) const; at::Tensor index_reduce(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, c10::string_view reduce, bool include_self=true) const; at::Tensor \u0026 index_fill_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor index_fill(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor \u0026 index_fill_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const; at::Tensor index_fill(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const; at::Tensor \u0026 index_fill_(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor \u0026 index_fill_(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const; at::Tensor index_fill(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor index_fill(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const; at::Tensor scatter(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor \u0026 scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor scatter(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor \u0026 scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor scatter(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce) const; at::Tensor \u0026 scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce) const; at::Tensor scatter(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value, c10::string_view reduce) const; at::Tensor \u0026 scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value, c10::string_view reduce) const; at::Tensor scatter(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor scatter(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const; at::Tensor scatter_add(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor \u0026 scatter_add_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor scatter_add(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const; at::Tensor scatter_reduce(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce, bool include_self=true) const; at::Tensor \u0026 scatter_reduce_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce, bool include_self=true) const; at::Tensor \u0026 eq_(const at::Scalar \u0026 other) const; at::Tensor \u0026 eq_(const at::Tensor \u0026 other) const; at::Tensor bitwise_and(const at::Scalar \u0026 other) const; at::Tensor bitwise_and(const at::Tensor \u0026 other) const; at::Tensor \u0026 bitwise_and_(const at::Scalar \u0026 other) const; at::Tensor \u0026 bitwise_and_(const at::Tensor \u0026 other) const; at::Tensor __and__(const at::Scalar \u0026 other) const; at::Tensor __and__(const at::Tensor \u0026 other) const; at::Tensor \u0026 __iand__(const at::Scalar \u0026 other) const; at::Tensor \u0026 __iand__(const at::Tensor \u0026 other) const; at::Tensor bitwise_or(const at::Scalar \u0026 other) const; at::Tensor bitwise_or(const at::Tensor \u0026 other) const; at::Tensor \u0026 bitwise_or_(const at::Scalar \u0026 other) const; at::Tensor \u0026 bitwise_or_(const at::Tensor \u0026 other) const; at::Tensor __or__(const at::Scalar \u0026 other) const; at::Tensor __or__(const at::Tensor \u0026 other) const; at::Tensor \u0026 __ior__(const at::Scalar \u0026 other) const; at::Tensor \u0026 __ior__(const at::Tensor \u0026 other) const; at::Tensor bitwise_xor(const at::Scalar \u0026 other) const; at::Tensor bitwise_xor(const at::Tensor \u0026 other) const; at::Tensor \u0026 bitwise_xor_(const at::Scalar \u0026 other) const; at::Tensor \u0026 bitwise_xor_(const at::Tensor \u0026 other) const; at::Tensor __xor__(const at::Scalar \u0026 other) const; at::Tensor __xor__(const at::Tensor \u0026 other) const; at::Tensor \u0026 __ixor__(const at::Scalar \u0026 other) const; at::Tensor \u0026 __ixor__(const at::Tensor \u0026 other) const; at::Tensor __lshift__(const at::Scalar \u0026 other) const; at::Tensor __lshift__(const at::Tensor \u0026 other) const; at::Tensor \u0026 __ilshift__(const at::Scalar \u0026 other) const; at::Tensor \u0026 __ilshift__(const at::Tensor \u0026 other) const; at::Tensor bitwise_left_shift(const at::Tensor \u0026 other) const; at::Tensor \u0026 bitwise_left_shift_(const at::Tensor \u0026 other) const; at::Tensor bitwise_left_shift(const at::Scalar \u0026 other) const; at::Tensor \u0026 bitwise_left_shift_(const at::Scalar \u0026 other) const; at::Tensor __rshift__(const at::Scalar \u0026 other) const; at::Tensor __rshift__(const at::Tensor \u0026 other) const; at::Tensor \u0026 __irshift__(const at::Scalar \u0026 other) const; at::Tensor \u0026 __irshift__(const at::Tensor \u0026 other) const; at::Tensor bitwise_right_shift(const at::Tensor \u0026 other) const; at::Tensor \u0026 bitwise_right_shift_(const at::Tensor \u0026 other) const; at::Tensor bitwise_right_shift(const at::Scalar \u0026 other) const; at::Tensor \u0026 bitwise_right_shift_(const at::Scalar \u0026 other) const; at::Tensor \u0026 tril_(int64_t diagonal=0) const; at::Tensor \u0026 tril__symint(c10::SymInt diagonal=0) const; at::Tensor \u0026 triu_(int64_t diagonal=0) const; at::Tensor \u0026 triu__symint(c10::SymInt diagonal=0) const; at::Tensor \u0026 digamma_() const; at::Tensor \u0026 lerp_(const at::Tensor \u0026 end, const at::Scalar \u0026 weight) const; at::Tensor \u0026 lerp_(const at::Tensor \u0026 end, const at::Tensor \u0026 weight) const; at::Tensor \u0026 addbmm_(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor addbmm(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta=1, const at::Scalar \u0026 alpha=1) const; at::Tensor \u0026 random_(int64_t from, ::std::optional\u003cint64_t\u003e to, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 random_(int64_t to, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 random_(::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 uniform_(double from=0, double to=1, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 cauchy_(double median=0, double sigma=1, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 log_normal_(double mean=1, double std=2, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 exponential_(double lambd=1, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 geometric_(double p, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor diag(int64_t diagonal=0) const; at::Tensor cross(const at::Tensor \u0026 other, ::std::optional\u003cint64_t\u003e dim=::std::nullopt) const; at::Tensor triu(int64_t diagonal=0) const; at::Tensor triu_symint(c10::SymInt diagonal=0) const; at::Tensor tril(int64_t diagonal=0) const; at::Tensor tril_symint(c10::SymInt diagonal=0) const; at::Tensor trace() const; at::Tensor ne(const at::Scalar \u0026 other) const; at::Tensor ne(const at::Tensor \u0026 other) const; at::Tensor \u0026 ne_(const at::Scalar \u0026 other) const; at::Tensor \u0026 ne_(const at::Tensor \u0026 other) const; at::Tensor not_equal(const at::Scalar \u0026 other) const; at::Tensor not_equal(const at::Tensor \u0026 other) const; at::Tensor \u0026 not_equal_(const at::Scalar \u0026 other) const; at::Tensor \u0026 not_equal_(const at::Tensor \u0026 other) const; at::Tensor eq(const at::Scalar \u0026 other) const; at::Tensor eq(const at::Tensor \u0026 other) const; at::Tensor ge(const at::Scalar \u0026 other) const; at::Tensor ge(const at::Tensor \u0026 other) const; at::Tensor \u0026 ge_(const at::Scalar \u0026 other) const; at::Tensor \u0026 ge_(const at::Tensor \u0026 other) const; at::Tensor greater_equal(const at::Scalar \u0026 other) const; at::Tensor greater_equal(const at::Tensor \u0026 other) const; at::Tensor \u0026 greater_equal_(const at::Scalar \u0026 other) const; at::Tensor \u0026 greater_equal_(const at::Tensor \u0026 other) const; at::Tensor le(const at::Scalar \u0026 other) const; at::Tensor le(const at::Tensor \u0026 other) const; at::Tensor \u0026 le_(const at::Scalar \u0026 other) const; at::Tensor \u0026 le_(const at::Tensor \u0026 other) const; at::Tensor less_equal(const at::Scalar \u0026 other) const; at::Tensor less_equal(const at::Tensor \u0026 other) const; at::Tensor \u0026 less_equal_(const at::Scalar \u0026 other) const; at::Tensor \u0026 less_equal_(const at::Tensor \u0026 other) const; at::Tensor gt(const at::Scalar \u0026 other) const; at::Tensor gt(const at::Tensor \u0026 other) const; at::Tensor \u0026 gt_(const at::Scalar \u0026 other) const; at::Tensor \u0026 gt_(const at::Tensor \u0026 other) const; at::Tensor greater(const at::Scalar \u0026 other) const; at::Tensor greater(const at::Tensor \u0026 other) const; at::Tensor \u0026 greater_(const at::Scalar \u0026 other) const; at::Tensor \u0026 greater_(const at::Tensor \u0026 other) const; at::Tensor lt(const at::Scalar \u0026 other) const; at::Tensor lt(const at::Tensor \u0026 other) const; at::Tensor \u0026 lt_(const at::Scalar \u0026 other) const; at::Tensor \u0026 lt_(const at::Tensor \u0026 other) const; at::Tensor less(const at::Scalar \u0026 other) const; at::Tensor less(const at::Tensor \u0026 other) const; at::Tensor \u0026 less_(const at::Scalar \u0026 other) const; at::Tensor \u0026 less_(const at::Tensor \u0026 other) const; at::Tensor take(const at::Tensor \u0026 index) const; at::Tensor take_along_dim(const at::Tensor \u0026 indices, ::std::optional\u003cint64_t\u003e dim=::std::nullopt) const; at::Tensor index_select(int64_t dim, const at::Tensor \u0026 index) const; at::Tensor index_select(at::Dimname dim, const at::Tensor \u0026 index) const; at::Tensor masked_select(const at::Tensor \u0026 mask) const; at::Tensor nonzero() const; at::Tensor nonzero_static(int64_t size, int64_t fill_value=-1) const; at::Tensor nonzero_static_symint(c10::SymInt size, int64_t fill_value=-1) const; ::std::vector\u003cat::Tensor\u003e nonzero_numpy() const; at::Tensor argwhere() const; at::Tensor gather(int64_t dim, const at::Tensor \u0026 index, bool sparse_grad=false) const; at::Tensor gather(at::Dimname dim, const at::Tensor \u0026 index, bool sparse_grad=false) const; at::Tensor addcmul(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value=1) const; at::Tensor \u0026 addcmul_(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value=1) const; at::Tensor addcdiv(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value=1) const; at::Tensor \u0026 addcdiv_(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value=1) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e triangular_solve(const at::Tensor \u0026 A, bool upper=true, bool transpose=false, bool unitriangular=false) const; ::std::tuple\u003cat::Tensor,at::Tensor,at::Tensor\u003e svd(bool some=true, bool compute_uv=true) const; at::Tensor swapaxes(int64_t axis0, int64_t axis1) const; at::Tensor \u0026 swapaxes_(int64_t axis0, int64_t axis1) const; at::Tensor swapdims(int64_t dim0, int64_t dim1) const; at::Tensor \u0026 swapdims_(int64_t dim0, int64_t dim1) const; at::Tensor cholesky(bool upper=false) const; at::Tensor cholesky_solve(const at::Tensor \u0026 input2, bool upper=false) const; at::Tensor cholesky_inverse(bool upper=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e qr(bool some=true) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e geqrf() const; at::Tensor orgqr(const at::Tensor \u0026 input2) const; at::Tensor ormqr(const at::Tensor \u0026 input2, const at::Tensor \u0026 input3, bool left=true, bool transpose=false) const; at::Tensor lu_solve(const at::Tensor \u0026 LU_data, const at::Tensor \u0026 LU_pivots) const; at::Tensor multinomial(int64_t num_samples, bool replacement=false, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor multinomial_symint(c10::SymInt num_samples, bool replacement=false, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor \u0026 lgamma_() const; at::Tensor lgamma() const; at::Tensor digamma() const; at::Tensor polygamma(int64_t n) const; at::Tensor \u0026 polygamma_(int64_t n) const; at::Tensor erfinv() const; at::Tensor \u0026 erfinv_() const; at::Tensor i0() const; at::Tensor \u0026 i0_() const; at::Tensor sign() const; at::Tensor \u0026 sign_() const; at::Tensor signbit() const; at::Tensor dist(const at::Tensor \u0026 other, const at::Scalar \u0026 p=2) const; at::Tensor \u0026 atan2_(const at::Tensor \u0026 other) const; at::Tensor atan2(const at::Tensor \u0026 other) const; at::Tensor arctan2(const at::Tensor \u0026 other) const; at::Tensor \u0026 arctan2_(const at::Tensor \u0026 other) const; at::Tensor lerp(const at::Tensor \u0026 end, const at::Scalar \u0026 weight) const; at::Tensor lerp(const at::Tensor \u0026 end, const at::Tensor \u0026 weight) const; at::Tensor histc(int64_t bins=100, const at::Scalar \u0026 min=0, const at::Scalar \u0026 max=0) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e histogram(const at::Tensor \u0026 bins, const ::std::optional\u003cat::Tensor\u003e \u0026 weight={}, bool density=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e histogram(int64_t bins=100, ::std::optional\u003cat::ArrayRef\u003cdouble\u003e\u003e range=::std::nullopt, const ::std::optional\u003cat::Tensor\u003e \u0026 weight={}, bool density=false) const; at::Tensor fmod(const at::Scalar \u0026 other) const; at::Tensor \u0026 fmod_(const at::Scalar \u0026 other) const; at::Tensor fmod(const at::Tensor \u0026 other) const; at::Tensor \u0026 fmod_(const at::Tensor \u0026 other) const; at::Tensor hypot(const at::Tensor \u0026 other) const; at::Tensor \u0026 hypot_(const at::Tensor \u0026 other) const; at::Tensor igamma(const at::Tensor \u0026 other) const; at::Tensor \u0026 igamma_(const at::Tensor \u0026 other) const; at::Tensor igammac(const at::Tensor \u0026 other) const; at::Tensor \u0026 igammac_(const at::Tensor \u0026 other) const; at::Tensor nextafter(const at::Tensor \u0026 other) const; at::Tensor \u0026 nextafter_(const at::Tensor \u0026 other) const; at::Tensor remainder(const at::Scalar \u0026 other) const; at::Tensor \u0026 remainder_(const at::Scalar \u0026 other) const; at::Tensor remainder(const at::Tensor \u0026 other) const; at::Tensor \u0026 remainder_(const at::Tensor \u0026 other) const; at::Tensor min() const; at::Tensor fmin(const at::Tensor \u0026 other) const; at::Tensor max() const; at::Tensor fmax(const at::Tensor \u0026 other) const; at::Tensor maximum(const at::Tensor \u0026 other) const; at::Tensor max(const at::Tensor \u0026 other) const; at::Tensor minimum(const at::Tensor \u0026 other) const; at::Tensor min(const at::Tensor \u0026 other) const; at::Tensor quantile(const at::Tensor \u0026 q, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false, c10::string_view interpolation=\"linear\") const; at::Tensor quantile(double q, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false, c10::string_view interpolation=\"linear\") const; at::Tensor nanquantile(const at::Tensor \u0026 q, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false, c10::string_view interpolation=\"linear\") const; at::Tensor nanquantile(double q, ::std::optional\u003cint64_t\u003e dim=::std::nullopt, bool keepdim=false, c10::string_view interpolation=\"linear\") const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e sort(int64_t dim=-1, bool descending=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e sort(::std::optional\u003cbool\u003e stable, int64_t dim=-1, bool descending=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e sort(at::Dimname dim, bool descending=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e sort(::std::optional\u003cbool\u003e stable, at::Dimname dim, bool descending=false) const; at::Tensor msort() const; at::Tensor argsort(int64_t dim=-1, bool descending=false) const; at::Tensor argsort(bool stable, int64_t dim=-1, bool descending=false) const; at::Tensor argsort(at::Dimname dim, bool descending=false) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e topk(int64_t k, int64_t dim=-1, bool largest=true, bool sorted=true) const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e topk_symint(c10::SymInt k, int64_t dim=-1, bool largest=true, bool sorted=true) const; at::Tensor all() const; at::Tensor any() const; at::Tensor renorm(const at::Scalar \u0026 p, int64_t dim, const at::Scalar \u0026 maxnorm) const; at::Tensor \u0026 renorm_(const at::Scalar \u0026 p, int64_t dim, const at::Scalar \u0026 maxnorm) const; at::Tensor unfold(int64_t dimension, int64_t size, int64_t step) const; bool equal(const at::Tensor \u0026 other) const; at::Tensor pow(const at::Tensor \u0026 exponent) const; at::Tensor pow(const at::Scalar \u0026 exponent) const; at::Tensor \u0026 pow_(const at::Scalar \u0026 exponent) const; at::Tensor \u0026 pow_(const at::Tensor \u0026 exponent) const; at::Tensor float_power(const at::Tensor \u0026 exponent) const; at::Tensor float_power(const at::Scalar \u0026 exponent) const; at::Tensor \u0026 float_power_(const at::Scalar \u0026 exponent) const; at::Tensor \u0026 float_power_(const at::Tensor \u0026 exponent) const; at::Tensor \u0026 normal_(double mean=0, double std=1, ::std::optional\u003cat::Generator\u003e generator=::std::nullopt) const; at::Tensor alias() const; at::Tensor isfinite() const; at::Tensor isinf() const; void record_stream(at::Stream s) const; at::Tensor isposinf() const; at::Tensor isneginf() const; at::Tensor det() const; ::std::tuple\u003cat::Tensor,at::Tensor\u003e slogdet() const; at::Tensor logdet() const; at::Tensor inverse() const; at::Tensor inner(const at::Tensor \u0026 other) const; at::Tensor outer(const at::Tensor \u0026 vec2) const; at::Tensor ger(const at::Tensor \u0026 vec2) const; at::Tensor to_padded_tensor(double padding, at::OptionalIntArrayRef output_size=::std::nullopt) const; at::Tensor to_padded_tensor_symint(double padding, at::OptionalSymIntArrayRef output_size=::std::nullopt) const; // Special C++ only overloads for std()-like functions (See gh-40287) // These are needed because int -\u003e bool conversion takes precedence over int -\u003e IntArrayRef // So, for example std(0) would select the std(unbiased=False) overload Tensor var(int dim) const { return var(IntArrayRef{dim}); } Tensor std(int dim) const { return std(IntArrayRef{dim}); } // We changed .dtype() to return a TypeMeta in #12766. Ideally, we want the // at::kDouble and its friends to be TypeMeta\u0027s, but that hasn\u0027t happened yet. // Before that change, we make this method to maintain BC for C++ usage like // `x.to(y.dtype)`. // TODO: remove following two after at::kDouble and its friends are TypeMeta\u0027s. inline Tensor to(caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const { return this-\u003eto(/*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy); } inline Tensor to(Device device, caffe2::TypeMeta type_meta, bool non_blocking=false, bool copy=false) const { return this-\u003eto(device, /*scalar_type=*/typeMetaToScalarType(type_meta), non_blocking, copy); } template \u003ctypename F, typename... Args\u003e decltype(auto) m(F func, Args\u0026\u0026... params) const { return func(*this, std::forward\u003cArgs\u003e(params)...); } at::Tensor tensor_data() const { return TensorBase::tensor_data(); } at::Tensor variable_data() const { return TensorBase::variable_data(); } // Hooks //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ template \u003ctypename T\u003e using hook_return_void_t = std::enable_if_t\u003cstd::is_void\u003ctypename std::invoke_result_t\u003cT\u0026, Tensor\u003e\u003e::value, unsigned\u003e; template \u003ctypename T\u003e using hook_return_var_t = std::enable_if_t\u003cstd::is_same_v\u003ctypename std::invoke_result_t\u003cT\u0026, Tensor\u003e, Tensor\u003e, unsigned\u003e; template \u003ctypename T\u003e hook_return_void_t\u003cT\u003e register_hook(T\u0026\u0026 hook) const; template \u003ctypename T\u003e hook_return_var_t\u003cT\u003e register_hook(T\u0026\u0026 hook) const; // Variable methods //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Tensor data() const { return TensorBase::data(); } void _backward(TensorList inputs, const std::optional\u003cTensor\u003e\u0026 gradient, std::optional\u003cbool\u003e keep_graph, bool create_graph) const; const Tensor\u0026 requires_grad_(bool _requires_grad=true) const { TensorBase::requires_grad_(_requires_grad); return *this; } }; namespace detail { // Helper creator for Tensor class which doesn\u0027t requires the users to pass // in an intrusive_ptr instead it just converts the argument passed to // requested intrusive_ptr type. template \u003ctypename T, typename... Args\u003e Tensor make_tensor(Args\u0026\u0026... args) { return Tensor(c10::make_intrusive\u003cT\u003e(std::forward\u003cArgs\u003e(args)...)); } } // namespace detail } // namespace at namespace at { // aten::_backward(Tensor self, Tensor[] inputs, Tensor? gradient=None, bool? retain_graph=None, bool create_graph=False) -\u003e () inline void Tensor::__dispatch__backward(at::TensorList inputs, const ::std::optional\u003cat::Tensor\u003e \u0026 gradient, ::std::optional\u003cbool\u003e retain_graph, bool create_graph) const { return at::_ops::_backward::call(const_cast\u003cTensor\u0026\u003e(*this), inputs, gradient, retain_graph, create_graph); } // aten::set_data(Tensor(a!) self, Tensor new_data) -\u003e () inline void Tensor::__dispatch_set_data(const at::Tensor \u0026 new_data) const { return at::_ops::set_data::call(const_cast\u003cTensor\u0026\u003e(*this), new_data); } // aten::data(Tensor self) -\u003e Tensor inline at::Tensor Tensor::__dispatch_data() const { return at::_ops::data::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_leaf(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_leaf() const { return at::_ops::is_leaf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::output_nr(Tensor self) -\u003e int inline int64_t Tensor::__dispatch_output_nr() const { return at::_ops::output_nr::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_version(Tensor self) -\u003e int inline int64_t Tensor::__dispatch__version() const { return at::_ops::_version::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::requires_grad_(Tensor(a!) self, bool requires_grad=True) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__dispatch_requires_grad_(bool requires_grad) const { return at::_ops::requires_grad_::call(const_cast\u003cTensor\u0026\u003e(*this), requires_grad); } // aten::retain_grad(Tensor(a!) self) -\u003e () inline void Tensor::__dispatch_retain_grad() const { return at::_ops::retain_grad::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::retains_grad(Tensor self) -\u003e bool inline bool Tensor::__dispatch_retains_grad() const { return at::_ops::retains_grad::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_fw_primal(Tensor(a) self, int level) -\u003e Tensor(a) inline at::Tensor Tensor::_fw_primal(int64_t level) const { return at::_ops::_fw_primal::call(const_cast\u003cTensor\u0026\u003e(*this), level); } // aten::rename_(Tensor(a!) self, Dimname[]? names) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::rename_(::std::optional\u003cat::DimnameList\u003e names) const { return at::_ops::rename_::call(const_cast\u003cTensor\u0026\u003e(*this), names); } // aten::rename(Tensor(a) self, Dimname[]? names) -\u003e Tensor(a) inline at::Tensor Tensor::rename(::std::optional\u003cat::DimnameList\u003e names) const { return at::_ops::rename::call(const_cast\u003cTensor\u0026\u003e(*this), names); } // aten::align_to(Tensor(a) self, Dimname[] names) -\u003e Tensor(a) inline at::Tensor Tensor::align_to(at::DimnameList names) const { return at::_ops::align_to::call(const_cast\u003cTensor\u0026\u003e(*this), names); } // aten::align_to.ellipsis_idx(Tensor(a) self, Dimname[] order, int ellipsis_idx) -\u003e Tensor(a) inline at::Tensor Tensor::align_to(at::DimnameList order, int64_t ellipsis_idx) const { return at::_ops::align_to_ellipsis_idx::call(const_cast\u003cTensor\u0026\u003e(*this), order, ellipsis_idx); } // aten::align_as(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::align_as(const at::Tensor \u0026 other) const { return at::_ops::align_as::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::refine_names(Tensor(a) self, Dimname[] names) -\u003e Tensor(a) inline at::Tensor Tensor::refine_names(at::DimnameList names) const { return at::_ops::refine_names::call(const_cast\u003cTensor\u0026\u003e(*this), names); } // aten::abs(Tensor self) -\u003e Tensor inline at::Tensor Tensor::abs() const { return at::_ops::abs::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::abs_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::abs_() const { return at::_ops::abs_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::absolute(Tensor self) -\u003e Tensor inline at::Tensor Tensor::absolute() const { return at::_ops::absolute::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::absolute_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::absolute_() const { return at::_ops::absolute_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::angle(Tensor self) -\u003e Tensor inline at::Tensor Tensor::angle() const { return at::_ops::angle::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sgn(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sgn() const { return at::_ops::sgn::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sgn_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sgn_() const { return at::_ops::sgn_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::chalf(Tensor self, *, MemoryFormat? memory_format=None) -\u003e Tensor inline at::Tensor Tensor::chalf(::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::chalf::call(const_cast\u003cTensor\u0026\u003e(*this), memory_format); } // aten::_conj(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::_conj() const { return at::_ops::_conj::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::conj(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::__dispatch_conj() const { return at::_ops::conj::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_conj_physical(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_conj_physical() const { return at::_ops::_conj_physical::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::conj_physical(Tensor self) -\u003e Tensor inline at::Tensor Tensor::conj_physical() const { return at::_ops::conj_physical::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::conj_physical_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::conj_physical_() const { return at::_ops::conj_physical_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::resolve_conj(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::resolve_conj() const { return at::_ops::resolve_conj::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::resolve_neg(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::resolve_neg() const { return at::_ops::resolve_neg::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_neg_view(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::_neg_view() const { return at::_ops::_neg_view::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::acos(Tensor self) -\u003e Tensor inline at::Tensor Tensor::acos() const { return at::_ops::acos::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::acos_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::acos_() const { return at::_ops::acos_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arccos(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arccos() const { return at::_ops::arccos::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arccos_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arccos_() const { return at::_ops::arccos_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::add(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::add_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::add_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::add__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::add(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::add_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::add_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::add__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::addmv(const at::Tensor \u0026 mat, const at::Tensor \u0026 vec, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addmv::call(const_cast\u003cTensor\u0026\u003e(*this), mat, vec, beta, alpha); } // aten::addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addmv_(const at::Tensor \u0026 mat, const at::Tensor \u0026 vec, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addmv_::call(const_cast\u003cTensor\u0026\u003e(*this), mat, vec, beta, alpha); } // aten::addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::addr(const at::Tensor \u0026 vec1, const at::Tensor \u0026 vec2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addr::call(const_cast\u003cTensor\u0026\u003e(*this), vec1, vec2, beta, alpha); } // aten::addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addr_(const at::Tensor \u0026 vec1, const at::Tensor \u0026 vec2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addr_::call(const_cast\u003cTensor\u0026\u003e(*this), vec1, vec2, beta, alpha); } // aten::_is_all_true(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_is_all_true() const { return at::_ops::_is_all_true::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_is_any_true(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_is_any_true() const { return at::_ops::_is_any_true::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::all.dim(Tensor self, int dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::all(int64_t dim, bool keepdim) const { return at::_ops::all_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::all.dims(Tensor self, int[]? dim=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::all(at::OptionalIntArrayRef dim, bool keepdim) const { return at::_ops::all_dims::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::all.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::all(at::Dimname dim, bool keepdim) const { return at::_ops::all_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::allclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -\u003e bool inline bool Tensor::allclose(const at::Tensor \u0026 other, double rtol, double atol, bool equal_nan) const { return at::_ops::allclose::call(const_cast\u003cTensor\u0026\u003e(*this), other, rtol, atol, equal_nan); } // aten::any.dim(Tensor self, int dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::any(int64_t dim, bool keepdim) const { return at::_ops::any_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::any.dims(Tensor self, int[]? dim=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::any(at::OptionalIntArrayRef dim, bool keepdim) const { return at::_ops::any_dims::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::any.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::any(at::Dimname dim, bool keepdim) const { return at::_ops::any_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::argmax(Tensor self, int? dim=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::argmax(::std::optional\u003cint64_t\u003e dim, bool keepdim) const { return at::_ops::argmax::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::argmin(Tensor self, int? dim=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::argmin(::std::optional\u003cint64_t\u003e dim, bool keepdim) const { return at::_ops::argmin::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::acosh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::acosh() const { return at::_ops::acosh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::acosh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::acosh_() const { return at::_ops::acosh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arccosh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arccosh() const { return at::_ops::arccosh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arccosh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arccosh_() const { return at::_ops::arccosh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::asinh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::asinh() const { return at::_ops::asinh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::asinh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::asinh_() const { return at::_ops::asinh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arcsinh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arcsinh() const { return at::_ops::arcsinh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arcsinh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arcsinh_() const { return at::_ops::arcsinh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::atanh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::atanh() const { return at::_ops::atanh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::atanh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::atanh_() const { return at::_ops::atanh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arctanh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arctanh() const { return at::_ops::arctanh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arctanh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arctanh_() const { return at::_ops::arctanh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a) inline at::Tensor Tensor::as_strided(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset) const { return at::_ops::as_strided::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), storage_offset.has_value() ? ::std::make_optional(c10::SymInt(*storage_offset)) : ::std::nullopt); } // aten::as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a) inline at::Tensor Tensor::as_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset) const { return at::_ops::as_strided::call(const_cast\u003cTensor\u0026\u003e(*this), size, stride, storage_offset); } // aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::as_strided_(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset) const { return at::_ops::as_strided_::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), storage_offset.has_value() ? ::std::make_optional(c10::SymInt(*storage_offset)) : ::std::nullopt); } // aten::as_strided_(Tensor(a!) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::as_strided__symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset) const { return at::_ops::as_strided_::call(const_cast\u003cTensor\u0026\u003e(*this), size, stride, storage_offset); } // aten::asin(Tensor self) -\u003e Tensor inline at::Tensor Tensor::asin() const { return at::_ops::asin::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::asin_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::asin_() const { return at::_ops::asin_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arcsin(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arcsin() const { return at::_ops::arcsin::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arcsin_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arcsin_() const { return at::_ops::arcsin_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::atan(Tensor self) -\u003e Tensor inline at::Tensor Tensor::atan() const { return at::_ops::atan::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::atan_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::atan_() const { return at::_ops::atan_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arctan(Tensor self) -\u003e Tensor inline at::Tensor Tensor::arctan() const { return at::_ops::arctan::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::arctan_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arctan_() const { return at::_ops::arctan_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::baddbmm(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::baddbmm::call(const_cast\u003cTensor\u0026\u003e(*this), batch1, batch2, beta, alpha); } // aten::baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::baddbmm_(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::baddbmm_::call(const_cast\u003cTensor\u0026\u003e(*this), batch1, batch2, beta, alpha); } // aten::bernoulli(Tensor self, *, Generator? generator=None) -\u003e Tensor inline at::Tensor Tensor::bernoulli(::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::bernoulli::call(const_cast\u003cTensor\u0026\u003e(*this), generator); } // aten::bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bernoulli_(const at::Tensor \u0026 p, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::bernoulli__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), p, generator); } // aten::bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bernoulli_(double p, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::bernoulli__float::call(const_cast\u003cTensor\u0026\u003e(*this), p, generator); } // aten::bernoulli.p(Tensor self, float p, *, Generator? generator=None) -\u003e Tensor inline at::Tensor Tensor::bernoulli(double p, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::bernoulli_p::call(const_cast\u003cTensor\u0026\u003e(*this), p, generator); } // aten::bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -\u003e Tensor inline at::Tensor Tensor::bincount(const ::std::optional\u003cat::Tensor\u003e \u0026 weights, int64_t minlength) const { return at::_ops::bincount::call(const_cast\u003cTensor\u0026\u003e(*this), weights, minlength); } // aten::bincount(Tensor self, Tensor? weights=None, SymInt minlength=0) -\u003e Tensor inline at::Tensor Tensor::bincount_symint(const ::std::optional\u003cat::Tensor\u003e \u0026 weights, c10::SymInt minlength) const { return at::_ops::bincount::call(const_cast\u003cTensor\u0026\u003e(*this), weights, minlength); } // aten::bitwise_not(Tensor self) -\u003e Tensor inline at::Tensor Tensor::bitwise_not() const { return at::_ops::bitwise_not::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::bitwise_not_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_not_() const { return at::_ops::bitwise_not_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::copysign.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::copysign(const at::Tensor \u0026 other) const { return at::_ops::copysign_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::copysign_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::copysign_(const at::Tensor \u0026 other) const { return at::_ops::copysign__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::copysign.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::copysign(const at::Scalar \u0026 other) const { return at::_ops::copysign_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::copysign_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::copysign_(const at::Scalar \u0026 other) const { return at::_ops::copysign__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::_lazy_clone(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_lazy_clone() const { return at::_ops::_lazy_clone::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logical_not(Tensor self) -\u003e Tensor inline at::Tensor Tensor::logical_not() const { return at::_ops::logical_not::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logical_not_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::logical_not_() const { return at::_ops::logical_not_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logical_xor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::logical_xor(const at::Tensor \u0026 other) const { return at::_ops::logical_xor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logical_xor_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::logical_xor_(const at::Tensor \u0026 other) const { return at::_ops::logical_xor_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logical_and(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::logical_and(const at::Tensor \u0026 other) const { return at::_ops::logical_and::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logical_and_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::logical_and_(const at::Tensor \u0026 other) const { return at::_ops::logical_and_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logical_or(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::logical_or(const at::Tensor \u0026 other) const { return at::_ops::logical_or::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logical_or_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::logical_or_(const at::Tensor \u0026 other) const { return at::_ops::logical_or_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bmm(Tensor self, Tensor mat2) -\u003e Tensor inline at::Tensor Tensor::bmm(const at::Tensor \u0026 mat2) const { return at::_ops::bmm::call(const_cast\u003cTensor\u0026\u003e(*this), mat2); } // aten::broadcast_to(Tensor(a) self, SymInt[] size) -\u003e Tensor(a) inline at::Tensor Tensor::broadcast_to(at::IntArrayRef size) const { return at::_ops::broadcast_to::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size)); } // aten::broadcast_to(Tensor(a) self, SymInt[] size) -\u003e Tensor(a) inline at::Tensor Tensor::broadcast_to_symint(c10::SymIntArrayRef size) const { return at::_ops::broadcast_to::call(const_cast\u003cTensor\u0026\u003e(*this), size); } // aten::ceil(Tensor self) -\u003e Tensor inline at::Tensor Tensor::ceil() const { return at::_ops::ceil::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::ceil_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ceil_() const { return at::_ops::ceil_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::unsafe_chunk(Tensor self, int chunks, int dim=0) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unsafe_chunk(int64_t chunks, int64_t dim) const { return at::_ops::unsafe_chunk::call(const_cast\u003cTensor\u0026\u003e(*this), chunks, dim); } // aten::chunk(Tensor(a -\u003e *) self, int chunks, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::chunk(int64_t chunks, int64_t dim) const { return at::_ops::chunk::call(const_cast\u003cTensor\u0026\u003e(*this), chunks, dim); } // aten::tensor_split.sections(Tensor(a -\u003e *) self, SymInt sections, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::tensor_split(int64_t sections, int64_t dim) const { return at::_ops::tensor_split_sections::call(const_cast\u003cTensor\u0026\u003e(*this), sections, dim); } // aten::tensor_split.sections(Tensor(a -\u003e *) self, SymInt sections, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::tensor_split_symint(c10::SymInt sections, int64_t dim) const { return at::_ops::tensor_split_sections::call(const_cast\u003cTensor\u0026\u003e(*this), sections, dim); } // aten::tensor_split.indices(Tensor(a -\u003e *) self, SymInt[] indices, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::tensor_split(at::IntArrayRef indices, int64_t dim) const { return at::_ops::tensor_split_indices::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(indices), dim); } // aten::tensor_split.indices(Tensor(a -\u003e *) self, SymInt[] indices, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::tensor_split_symint(c10::SymIntArrayRef indices, int64_t dim) const { return at::_ops::tensor_split_indices::call(const_cast\u003cTensor\u0026\u003e(*this), indices, dim); } // aten::tensor_split.tensor_indices_or_sections(Tensor(a -\u003e *) self, Tensor tensor_indices_or_sections, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::tensor_split(const at::Tensor \u0026 tensor_indices_or_sections, int64_t dim) const { return at::_ops::tensor_split_tensor_indices_or_sections::call(const_cast\u003cTensor\u0026\u003e(*this), tensor_indices_or_sections, dim); } // aten::clamp(Tensor self, Scalar? min=None, Scalar? max=None) -\u003e Tensor inline at::Tensor Tensor::clamp(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max) const { return at::_ops::clamp::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -\u003e Tensor inline at::Tensor Tensor::clamp(const ::std::optional\u003cat::Tensor\u003e \u0026 min, const ::std::optional\u003cat::Tensor\u003e \u0026 max) const { return at::_ops::clamp_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max) const { return at::_ops::clamp_::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_(const ::std::optional\u003cat::Tensor\u003e \u0026 min, const ::std::optional\u003cat::Tensor\u003e \u0026 max) const { return at::_ops::clamp__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clamp_max(Tensor self, Scalar max) -\u003e Tensor inline at::Tensor Tensor::clamp_max(const at::Scalar \u0026 max) const { return at::_ops::clamp_max::call(const_cast\u003cTensor\u0026\u003e(*this), max); } // aten::clamp_max.Tensor(Tensor self, Tensor max) -\u003e Tensor inline at::Tensor Tensor::clamp_max(const at::Tensor \u0026 max) const { return at::_ops::clamp_max_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), max); } // aten::clamp_max_(Tensor(a!) self, Scalar max) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_max_(const at::Scalar \u0026 max) const { return at::_ops::clamp_max_::call(const_cast\u003cTensor\u0026\u003e(*this), max); } // aten::clamp_max_.Tensor(Tensor(a!) self, Tensor max) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_max_(const at::Tensor \u0026 max) const { return at::_ops::clamp_max__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), max); } // aten::clamp_min(Tensor self, Scalar min) -\u003e Tensor inline at::Tensor Tensor::clamp_min(const at::Scalar \u0026 min) const { return at::_ops::clamp_min::call(const_cast\u003cTensor\u0026\u003e(*this), min); } // aten::clamp_min.Tensor(Tensor self, Tensor min) -\u003e Tensor inline at::Tensor Tensor::clamp_min(const at::Tensor \u0026 min) const { return at::_ops::clamp_min_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min); } // aten::clamp_min_(Tensor(a!) self, Scalar min) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_min_(const at::Scalar \u0026 min) const { return at::_ops::clamp_min_::call(const_cast\u003cTensor\u0026\u003e(*this), min); } // aten::clamp_min_.Tensor(Tensor(a!) self, Tensor min) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clamp_min_(const at::Tensor \u0026 min) const { return at::_ops::clamp_min__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min); } // aten::clip(Tensor self, Scalar? min=None, Scalar? max=None) -\u003e Tensor inline at::Tensor Tensor::clip(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max) const { return at::_ops::clip::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clip.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -\u003e Tensor inline at::Tensor Tensor::clip(const ::std::optional\u003cat::Tensor\u003e \u0026 min, const ::std::optional\u003cat::Tensor\u003e \u0026 max) const { return at::_ops::clip_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clip_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clip_(const ::std::optional\u003cat::Scalar\u003e \u0026 min, const ::std::optional\u003cat::Scalar\u003e \u0026 max) const { return at::_ops::clip_::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::clip_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::clip_(const ::std::optional\u003cat::Tensor\u003e \u0026 min, const ::std::optional\u003cat::Tensor\u003e \u0026 max) const { return at::_ops::clip__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), min, max); } // aten::contiguous(Tensor(a) self, *, MemoryFormat memory_format=contiguous_format) -\u003e Tensor(a) inline at::Tensor Tensor::__dispatch_contiguous(at::MemoryFormat memory_format) const { return at::_ops::contiguous::call(const_cast\u003cTensor\u0026\u003e(*this), memory_format); } // aten::copy_(Tensor(a!) self, Tensor src, bool non_blocking=False) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::copy_(const at::Tensor \u0026 src, bool non_blocking) const { return at::_ops::copy_::call(const_cast\u003cTensor\u0026\u003e(*this), src, non_blocking); } // aten::cos(Tensor self) -\u003e Tensor inline at::Tensor Tensor::cos() const { return at::_ops::cos::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::cos_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cos_() const { return at::_ops::cos_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::cosh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::cosh() const { return at::_ops::cosh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::cosh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cosh_() const { return at::_ops::cosh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::count_nonzero.dim_IntList(Tensor self, int[] dim) -\u003e Tensor inline at::Tensor Tensor::count_nonzero(at::IntArrayRef dim) const { return at::_ops::count_nonzero_dim_IntList::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::count_nonzero(Tensor self, int? dim=None) -\u003e Tensor inline at::Tensor Tensor::count_nonzero(::std::optional\u003cint64_t\u003e dim) const { return at::_ops::count_nonzero::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::cov(Tensor self, *, int correction=1, Tensor? fweights=None, Tensor? aweights=None) -\u003e Tensor inline at::Tensor Tensor::cov(int64_t correction, const ::std::optional\u003cat::Tensor\u003e \u0026 fweights, const ::std::optional\u003cat::Tensor\u003e \u0026 aweights) const { return at::_ops::cov::call(const_cast\u003cTensor\u0026\u003e(*this), correction, fweights, aweights); } // aten::corrcoef(Tensor self) -\u003e Tensor inline at::Tensor Tensor::corrcoef() const { return at::_ops::corrcoef::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::cummax(Tensor self, int dim) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::cummax(int64_t dim) const { return at::_ops::cummax::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::cummax.dimname(Tensor self, Dimname dim) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::cummax(at::Dimname dim) const { return at::_ops::cummax_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::cummin(Tensor self, int dim) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::cummin(int64_t dim) const { return at::_ops::cummin::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::cummin.dimname(Tensor self, Dimname dim) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::cummin(at::Dimname dim) const { return at::_ops::cummin_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::cumprod(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumprod::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumprod_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cumprod_(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumprod_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumprod.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::cumprod(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumprod_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cumprod_(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumprod__dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::cumsum(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumsum::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cumsum_(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumsum_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumsum.dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::cumsum(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumsum_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::cumsum_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cumsum_(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::cumsum__dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::diag_embed(Tensor self, int offset=0, int dim1=-2, int dim2=-1) -\u003e Tensor inline at::Tensor Tensor::diag_embed(int64_t offset, int64_t dim1, int64_t dim2) const { return at::_ops::diag_embed::call(const_cast\u003cTensor\u0026\u003e(*this), offset, dim1, dim2); } // aten::diagflat(Tensor self, int offset=0) -\u003e Tensor inline at::Tensor Tensor::diagflat(int64_t offset) const { return at::_ops::diagflat::call(const_cast\u003cTensor\u0026\u003e(*this), offset); } // aten::diagonal(Tensor(a) self, int offset=0, int dim1=0, int dim2=1) -\u003e Tensor(a) inline at::Tensor Tensor::diagonal(int64_t offset, int64_t dim1, int64_t dim2) const { return at::_ops::diagonal::call(const_cast\u003cTensor\u0026\u003e(*this), offset, dim1, dim2); } // aten::diagonal.Dimname(Tensor(a) self, *, Dimname outdim, Dimname dim1, Dimname dim2, int offset=0) -\u003e Tensor(a) inline at::Tensor Tensor::diagonal(at::Dimname outdim, at::Dimname dim1, at::Dimname dim2, int64_t offset) const { return at::_ops::diagonal_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), outdim, dim1, dim2, offset); } // aten::fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fill_diagonal_(const at::Scalar \u0026 fill_value, bool wrap) const { return at::_ops::fill_diagonal_::call(const_cast\u003cTensor\u0026\u003e(*this), fill_value, wrap); } // aten::diff(Tensor self, int n=1, int dim=-1, Tensor? prepend=None, Tensor? append=None) -\u003e Tensor inline at::Tensor Tensor::diff(int64_t n, int64_t dim, const ::std::optional\u003cat::Tensor\u003e \u0026 prepend, const ::std::optional\u003cat::Tensor\u003e \u0026 append) const { return at::_ops::diff::call(const_cast\u003cTensor\u0026\u003e(*this), n, dim, prepend, append); } // aten::div.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::div(const at::Tensor \u0026 other) const { return at::_ops::div_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::div_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::div_(const at::Tensor \u0026 other) const { return at::_ops::div__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -\u003e Tensor inline at::Tensor Tensor::div(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::div_Tensor_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::div_(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::div__Tensor_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::div.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::div(const at::Scalar \u0026 other) const { return at::_ops::div_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::div_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::div_(const at::Scalar \u0026 other) const { return at::_ops::div__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -\u003e Tensor inline at::Tensor Tensor::div(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::div_Scalar_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::div_(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::div__Scalar_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::divide.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::divide(const at::Tensor \u0026 other) const { return at::_ops::divide_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::divide_(const at::Tensor \u0026 other) const { return at::_ops::divide__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::divide.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::divide(const at::Scalar \u0026 other) const { return at::_ops::divide_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::divide_(const at::Scalar \u0026 other) const { return at::_ops::divide__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::divide.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -\u003e Tensor inline at::Tensor Tensor::divide(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::divide_Tensor_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::divide_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::divide_(const at::Tensor \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::divide__Tensor_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::divide.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -\u003e Tensor inline at::Tensor Tensor::divide(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::divide_Scalar_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::divide_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::divide_(const at::Scalar \u0026 other, ::std::optional\u003cc10::string_view\u003e rounding_mode) const { return at::_ops::divide__Scalar_mode::call(const_cast\u003cTensor\u0026\u003e(*this), other, rounding_mode); } // aten::true_divide.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::true_divide(const at::Tensor \u0026 other) const { return at::_ops::true_divide_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::true_divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::true_divide_(const at::Tensor \u0026 other) const { return at::_ops::true_divide__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::true_divide.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::true_divide(const at::Scalar \u0026 other) const { return at::_ops::true_divide_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::true_divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::true_divide_(const at::Scalar \u0026 other) const { return at::_ops::true_divide__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::dot(Tensor self, Tensor tensor) -\u003e Tensor inline at::Tensor Tensor::dot(const at::Tensor \u0026 tensor) const { return at::_ops::dot::call(const_cast\u003cTensor\u0026\u003e(*this), tensor); } // aten::vdot(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::vdot(const at::Tensor \u0026 other) const { return at::_ops::vdot::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty(at::IntArrayRef size, at::TensorOptions options) const { return at::_ops::new_empty::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_empty::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory); } // aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_symint(c10::SymIntArrayRef size, at::TensorOptions options) const { return at::_ops::new_empty::call(const_cast\u003cTensor\u0026\u003e(*this), size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_empty(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_empty::call(const_cast\u003cTensor\u0026\u003e(*this), size, dtype, layout, device, pin_memory); } // aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_strided(at::IntArrayRef size, at::IntArrayRef stride, at::TensorOptions options) const { return at::_ops::new_empty_strided::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_strided(at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_empty_strided::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), dtype, layout, device, pin_memory); } // aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, at::TensorOptions options) const { return at::_ops::new_empty_strided::call(const_cast\u003cTensor\u0026\u003e(*this), size, stride, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_empty_strided(Tensor self, SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_empty_strided_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_empty_strided::call(const_cast\u003cTensor\u0026\u003e(*this), size, stride, dtype, layout, device, pin_memory); } // aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_full(at::IntArrayRef size, const at::Scalar \u0026 fill_value, at::TensorOptions options) const { return at::_ops::new_full::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), fill_value, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_full(at::IntArrayRef size, const at::Scalar \u0026 fill_value, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_full::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), fill_value, dtype, layout, device, pin_memory); } // aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_full_symint(c10::SymIntArrayRef size, const at::Scalar \u0026 fill_value, at::TensorOptions options) const { return at::_ops::new_full::call(const_cast\u003cTensor\u0026\u003e(*this), size, fill_value, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_full(Tensor self, SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_full_symint(c10::SymIntArrayRef size, const at::Scalar \u0026 fill_value, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_full::call(const_cast\u003cTensor\u0026\u003e(*this), size, fill_value, dtype, layout, device, pin_memory); } // aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_zeros(at::IntArrayRef size, at::TensorOptions options) const { return at::_ops::new_zeros::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_zeros(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_zeros::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory); } // aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_zeros_symint(c10::SymIntArrayRef size, at::TensorOptions options) const { return at::_ops::new_zeros::call(const_cast\u003cTensor\u0026\u003e(*this), size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_zeros(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_zeros_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_zeros::call(const_cast\u003cTensor\u0026\u003e(*this), size, dtype, layout, device, pin_memory); } // aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_ones(at::IntArrayRef size, at::TensorOptions options) const { return at::_ops::new_ones::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_ones(at::IntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_ones::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), dtype, layout, device, pin_memory); } // aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_ones_symint(c10::SymIntArrayRef size, at::TensorOptions options) const { return at::_ops::new_ones::call(const_cast\u003cTensor\u0026\u003e(*this), size, c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt()); } // aten::new_ones(Tensor self, SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -\u003e Tensor inline at::Tensor Tensor::new_ones_symint(c10::SymIntArrayRef size, ::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory) const { return at::_ops::new_ones::call(const_cast\u003cTensor\u0026\u003e(*this), size, dtype, layout, device, pin_memory); } // aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::resize_(at::IntArrayRef size, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::resize_::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), memory_format); } // aten::resize_(Tensor(a!) self, SymInt[] size, *, MemoryFormat? memory_format=None) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::resize__symint(c10::SymIntArrayRef size, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::resize_::call(const_cast\u003cTensor\u0026\u003e(*this), size, memory_format); } // aten::erf(Tensor self) -\u003e Tensor inline at::Tensor Tensor::erf() const { return at::_ops::erf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::erf_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::erf_() const { return at::_ops::erf_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::erfc(Tensor self) -\u003e Tensor inline at::Tensor Tensor::erfc() const { return at::_ops::erfc::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::erfc_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::erfc_() const { return at::_ops::erfc_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::exp(Tensor self) -\u003e Tensor inline at::Tensor Tensor::exp() const { return at::_ops::exp::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::exp_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::exp_() const { return at::_ops::exp_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::exp2(Tensor self) -\u003e Tensor inline at::Tensor Tensor::exp2() const { return at::_ops::exp2::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::exp2_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::exp2_() const { return at::_ops::exp2_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::expm1(Tensor self) -\u003e Tensor inline at::Tensor Tensor::expm1() const { return at::_ops::expm1::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::expm1_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::expm1_() const { return at::_ops::expm1_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -\u003e Tensor(a) inline at::Tensor Tensor::expand(at::IntArrayRef size, bool implicit) const { return at::_ops::expand::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), implicit); } // aten::expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -\u003e Tensor(a) inline at::Tensor Tensor::expand_symint(c10::SymIntArrayRef size, bool implicit) const { return at::_ops::expand::call(const_cast\u003cTensor\u0026\u003e(*this), size, implicit); } // aten::expand_as(Tensor(a) self, Tensor other) -\u003e Tensor(a) inline at::Tensor Tensor::expand_as(const at::Tensor \u0026 other) const { return at::_ops::expand_as::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::flatten.using_ints(Tensor(a) self, int start_dim=0, int end_dim=-1) -\u003e Tensor(a) inline at::Tensor Tensor::flatten(int64_t start_dim, int64_t end_dim) const { return at::_ops::flatten_using_ints::call(const_cast\u003cTensor\u0026\u003e(*this), start_dim, end_dim); } // aten::flatten.named_out_dim(Tensor(a) self, int start_dim, int end_dim, Dimname out_dim) -\u003e Tensor(a) inline at::Tensor Tensor::flatten(int64_t start_dim, int64_t end_dim, at::Dimname out_dim) const { return at::_ops::flatten_named_out_dim::call(const_cast\u003cTensor\u0026\u003e(*this), start_dim, end_dim, out_dim); } // aten::flatten.using_names(Tensor(a) self, Dimname start_dim, Dimname end_dim, Dimname out_dim) -\u003e Tensor(a) inline at::Tensor Tensor::flatten(at::Dimname start_dim, at::Dimname end_dim, at::Dimname out_dim) const { return at::_ops::flatten_using_names::call(const_cast\u003cTensor\u0026\u003e(*this), start_dim, end_dim, out_dim); } // aten::flatten.DimnameList(Tensor(a) self, Dimname[] dims, Dimname out_dim) -\u003e Tensor(a) inline at::Tensor Tensor::flatten(at::DimnameList dims, at::Dimname out_dim) const { return at::_ops::flatten_DimnameList::call(const_cast\u003cTensor\u0026\u003e(*this), dims, out_dim); } // aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -\u003e Tensor(a) inline at::Tensor Tensor::unflatten(int64_t dim, at::IntArrayRef sizes) const { return at::_ops::unflatten_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, c10::fromIntArrayRefSlow(sizes)); } // aten::unflatten.int(Tensor(a) self, int dim, SymInt[] sizes) -\u003e Tensor(a) inline at::Tensor Tensor::unflatten_symint(int64_t dim, c10::SymIntArrayRef sizes) const { return at::_ops::unflatten_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, sizes); } // aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -\u003e Tensor(a) inline at::Tensor Tensor::unflatten(at::Dimname dim, at::IntArrayRef sizes, at::DimnameList names) const { return at::_ops::unflatten_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, c10::fromIntArrayRefSlow(sizes), names); } // aten::unflatten.Dimname(Tensor(a) self, Dimname dim, SymInt[] sizes, Dimname[] names) -\u003e Tensor(a) inline at::Tensor Tensor::unflatten_symint(at::Dimname dim, c10::SymIntArrayRef sizes, at::DimnameList names) const { return at::_ops::unflatten_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, sizes, names); } // aten::fill_.Scalar(Tensor(a!) self, Scalar value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fill_(const at::Scalar \u0026 value) const { return at::_ops::fill__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), value); } // aten::fill_.Tensor(Tensor(a!) self, Tensor value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fill_(const at::Tensor \u0026 value) const { return at::_ops::fill__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), value); } // aten::floor(Tensor self) -\u003e Tensor inline at::Tensor Tensor::floor() const { return at::_ops::floor::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::floor_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::floor_() const { return at::_ops::floor_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::floor_divide(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::floor_divide(const at::Tensor \u0026 other) const { return at::_ops::floor_divide::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::floor_divide_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::floor_divide_(const at::Tensor \u0026 other) const { return at::_ops::floor_divide__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::floor_divide.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::floor_divide(const at::Scalar \u0026 other) const { return at::_ops::floor_divide_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::floor_divide_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::floor_divide_(const at::Scalar \u0026 other) const { return at::_ops::floor_divide__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::frac(Tensor self) -\u003e Tensor inline at::Tensor Tensor::frac() const { return at::_ops::frac::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::frac_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::frac_() const { return at::_ops::frac_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::gcd(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::gcd(const at::Tensor \u0026 other) const { return at::_ops::gcd::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::gcd_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::gcd_(const at::Tensor \u0026 other) const { return at::_ops::gcd_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lcm(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::lcm(const at::Tensor \u0026 other) const { return at::_ops::lcm::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lcm_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lcm_(const at::Tensor \u0026 other) const { return at::_ops::lcm_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::index.Tensor(Tensor self, Tensor?[] indices) -\u003e Tensor inline at::Tensor Tensor::index(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices) const { return at::_ops::index_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), indices); } // aten::index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_copy_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const { return at::_ops::index_copy_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source); } // aten::index_copy(Tensor self, int dim, Tensor index, Tensor source) -\u003e Tensor inline at::Tensor Tensor::index_copy(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const { return at::_ops::index_copy::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source); } // aten::index_copy_.dimname(Tensor(a!) self, Dimname dim, Tensor index, Tensor source) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_copy_(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const { return at::_ops::index_copy__dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source); } // aten::index_copy.dimname(Tensor self, Dimname dim, Tensor index, Tensor source) -\u003e Tensor inline at::Tensor Tensor::index_copy(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source) const { return at::_ops::index_copy_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source); } // aten::index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_put_(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices, const at::Tensor \u0026 values, bool accumulate) const { return at::_ops::index_put_::call(const_cast\u003cTensor\u0026\u003e(*this), indices, values, accumulate); } // aten::index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -\u003e Tensor inline at::Tensor Tensor::index_put(const c10::List\u003c::std::optional\u003cat::Tensor\u003e\u003e \u0026 indices, const at::Tensor \u0026 values, bool accumulate) const { return at::_ops::index_put::call(const_cast\u003cTensor\u0026\u003e(*this), indices, values, accumulate); } // aten::isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -\u003e Tensor inline at::Tensor Tensor::isclose(const at::Tensor \u0026 other, double rtol, double atol, bool equal_nan) const { return at::_ops::isclose::call(const_cast\u003cTensor\u0026\u003e(*this), other, rtol, atol, equal_nan); } // aten::isnan(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isnan() const { return at::_ops::isnan::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_distributed(Tensor self) -\u003e bool inline bool Tensor::is_distributed() const { return at::_ops::is_distributed::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_floating_point(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_floating_point() const { return at::_ops::is_floating_point::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_complex(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_complex() const { return at::_ops::is_complex::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_conj(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_conj() const { return at::_ops::is_conj::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_is_zerotensor(Tensor self) -\u003e bool inline bool Tensor::__dispatch__is_zerotensor() const { return at::_ops::_is_zerotensor::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_neg(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_neg() const { return at::_ops::is_neg::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::isreal(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isreal() const { return at::_ops::isreal::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_nonzero(Tensor self) -\u003e bool inline bool Tensor::is_nonzero() const { return at::_ops::is_nonzero::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_same_size(Tensor self, Tensor other) -\u003e bool inline bool Tensor::is_same_size(const at::Tensor \u0026 other) const { return at::_ops::is_same_size::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::is_signed(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_signed() const { return at::_ops::is_signed::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_inference(Tensor self) -\u003e bool inline bool Tensor::__dispatch_is_inference() const { return at::_ops::is_inference::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::kron(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::kron(const at::Tensor \u0026 other) const { return at::_ops::kron::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::kthvalue(int64_t k, int64_t dim, bool keepdim) const { return at::_ops::kthvalue::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, keepdim); } // aten::kthvalue(Tensor self, SymInt k, int dim=-1, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::kthvalue_symint(c10::SymInt k, int64_t dim, bool keepdim) const { return at::_ops::kthvalue::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, keepdim); } // aten::kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::kthvalue(int64_t k, at::Dimname dim, bool keepdim) const { return at::_ops::kthvalue_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, keepdim); } // aten::kthvalue.dimname(Tensor self, SymInt k, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::kthvalue_symint(c10::SymInt k, at::Dimname dim, bool keepdim) const { return at::_ops::kthvalue_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, keepdim); } // aten::nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -\u003e Tensor inline at::Tensor Tensor::nan_to_num(::std::optional\u003cdouble\u003e nan, ::std::optional\u003cdouble\u003e posinf, ::std::optional\u003cdouble\u003e neginf) const { return at::_ops::nan_to_num::call(const_cast\u003cTensor\u0026\u003e(*this), nan, posinf, neginf); } // aten::nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::nan_to_num_(::std::optional\u003cdouble\u003e nan, ::std::optional\u003cdouble\u003e posinf, ::std::optional\u003cdouble\u003e neginf) const { return at::_ops::nan_to_num_::call(const_cast\u003cTensor\u0026\u003e(*this), nan, posinf, neginf); } // aten::ldexp.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::ldexp(const at::Tensor \u0026 other) const { return at::_ops::ldexp_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ldexp_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ldexp_(const at::Tensor \u0026 other) const { return at::_ops::ldexp_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::log(Tensor self) -\u003e Tensor inline at::Tensor Tensor::log() const { return at::_ops::log::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::log_() const { return at::_ops::log_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log10(Tensor self) -\u003e Tensor inline at::Tensor Tensor::log10() const { return at::_ops::log10::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log10_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::log10_() const { return at::_ops::log10_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log1p(Tensor self) -\u003e Tensor inline at::Tensor Tensor::log1p() const { return at::_ops::log1p::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log1p_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::log1p_() const { return at::_ops::log1p_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log2(Tensor self) -\u003e Tensor inline at::Tensor Tensor::log2() const { return at::_ops::log2::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::log2_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::log2_() const { return at::_ops::log2_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logaddexp(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::logaddexp(const at::Tensor \u0026 other) const { return at::_ops::logaddexp::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::logaddexp2(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::logaddexp2(const at::Tensor \u0026 other) const { return at::_ops::logaddexp2::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::xlogy.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::xlogy(const at::Tensor \u0026 other) const { return at::_ops::xlogy_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::xlogy.Scalar_Other(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::xlogy(const at::Scalar \u0026 other) const { return at::_ops::xlogy_Scalar_Other::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::xlogy_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::xlogy_(const at::Tensor \u0026 other) const { return at::_ops::xlogy__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::xlogy_(const at::Scalar \u0026 other) const { return at::_ops::xlogy__Scalar_Other::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::log_softmax(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::log_softmax_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::log_softmax(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::log_softmax_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::logcumsumexp(Tensor self, int dim) -\u003e Tensor inline at::Tensor Tensor::logcumsumexp(int64_t dim) const { return at::_ops::logcumsumexp::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::logcumsumexp.dimname(Tensor self, Dimname dim) -\u003e Tensor inline at::Tensor Tensor::logcumsumexp(at::Dimname dim) const { return at::_ops::logcumsumexp_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::logsumexp(Tensor self, int[1] dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::logsumexp(at::IntArrayRef dim, bool keepdim) const { return at::_ops::logsumexp::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::logsumexp(at::DimnameList dim, bool keepdim) const { return at::_ops::logsumexp_names::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::matmul(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::matmul(const at::Tensor \u0026 other) const { return at::_ops::matmul::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::matrix_power(Tensor self, int n) -\u003e Tensor inline at::Tensor Tensor::matrix_power(int64_t n) const { return at::_ops::matrix_power::call(const_cast\u003cTensor\u0026\u003e(*this), n); } // aten::matrix_exp(Tensor self) -\u003e Tensor inline at::Tensor Tensor::matrix_exp() const { return at::_ops::matrix_exp::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -\u003e (Tensor min, Tensor max) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::aminmax(::std::optional\u003cint64_t\u003e dim, bool keepdim) const { return at::_ops::aminmax::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::max.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::max(int64_t dim, bool keepdim) const { return at::_ops::max_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::max(at::Dimname dim, bool keepdim) const { return at::_ops::max_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::amax(Tensor self, int[1] dim=[], bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::amax(at::IntArrayRef dim, bool keepdim) const { return at::_ops::amax::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::mean(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::mean(::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::mean::call(const_cast\u003cTensor\u0026\u003e(*this), dtype); } // aten::mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::mean(at::OptionalIntArrayRef dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::mean_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::mean(at::DimnameList dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::mean_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::nanmean(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::nanmean(at::OptionalIntArrayRef dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::nanmean::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::median(Tensor self) -\u003e Tensor inline at::Tensor Tensor::median() const { return at::_ops::median::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::median.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::median(int64_t dim, bool keepdim) const { return at::_ops::median_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::median.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::median(at::Dimname dim, bool keepdim) const { return at::_ops::median_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::nanmedian(Tensor self) -\u003e Tensor inline at::Tensor Tensor::nanmedian() const { return at::_ops::nanmedian::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::nanmedian.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::nanmedian(int64_t dim, bool keepdim) const { return at::_ops::nanmedian_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::nanmedian.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::nanmedian(at::Dimname dim, bool keepdim) const { return at::_ops::nanmedian_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::min.dim(Tensor self, int dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::min(int64_t dim, bool keepdim) const { return at::_ops::min_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::min(at::Dimname dim, bool keepdim) const { return at::_ops::min_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::amin(Tensor self, int[1] dim=[], bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::amin(at::IntArrayRef dim, bool keepdim) const { return at::_ops::amin::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::mm(Tensor self, Tensor mat2) -\u003e Tensor inline at::Tensor Tensor::mm(const at::Tensor \u0026 mat2) const { return at::_ops::mm::call(const_cast\u003cTensor\u0026\u003e(*this), mat2); } // aten::mode(Tensor self, int dim=-1, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::mode(int64_t dim, bool keepdim) const { return at::_ops::mode::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::mode.dimname(Tensor self, Dimname dim, bool keepdim=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::mode(at::Dimname dim, bool keepdim) const { return at::_ops::mode_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim); } // aten::mul.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::mul(const at::Tensor \u0026 other) const { return at::_ops::mul_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::mul_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::mul_(const at::Tensor \u0026 other) const { return at::_ops::mul__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::mul.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::mul(const at::Scalar \u0026 other) const { return at::_ops::mul_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::mul_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::mul_(const at::Scalar \u0026 other) const { return at::_ops::mul__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::multiply.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::multiply(const at::Tensor \u0026 other) const { return at::_ops::multiply_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::multiply_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::multiply_(const at::Tensor \u0026 other) const { return at::_ops::multiply__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::multiply.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::multiply(const at::Scalar \u0026 other) const { return at::_ops::multiply_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::multiply_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::multiply_(const at::Scalar \u0026 other) const { return at::_ops::multiply__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::mv(Tensor self, Tensor vec) -\u003e Tensor inline at::Tensor Tensor::mv(const at::Tensor \u0026 vec) const { return at::_ops::mv::call(const_cast\u003cTensor\u0026\u003e(*this), vec); } // aten::mvlgamma(Tensor self, int p) -\u003e Tensor inline at::Tensor Tensor::mvlgamma(int64_t p) const { return at::_ops::mvlgamma::call(const_cast\u003cTensor\u0026\u003e(*this), p); } // aten::mvlgamma_(Tensor(a!) self, int p) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::mvlgamma_(int64_t p) const { return at::_ops::mvlgamma_::call(const_cast\u003cTensor\u0026\u003e(*this), p); } // aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -\u003e Tensor inline at::Tensor Tensor::narrow_copy(int64_t dim, int64_t start, int64_t length) const { return at::_ops::narrow_copy::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::narrow_copy(Tensor self, int dim, SymInt start, SymInt length) -\u003e Tensor inline at::Tensor Tensor::narrow_copy_symint(int64_t dim, c10::SymInt start, c10::SymInt length) const { return at::_ops::narrow_copy::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -\u003e Tensor(a) inline at::Tensor Tensor::narrow(int64_t dim, int64_t start, int64_t length) const { return at::_ops::narrow::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::narrow(Tensor(a) self, int dim, SymInt start, SymInt length) -\u003e Tensor(a) inline at::Tensor Tensor::narrow_symint(int64_t dim, c10::SymInt start, c10::SymInt length) const { return at::_ops::narrow::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -\u003e Tensor(a) inline at::Tensor Tensor::narrow(int64_t dim, const at::Tensor \u0026 start, int64_t length) const { return at::_ops::narrow_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::narrow.Tensor(Tensor(a) self, int dim, Tensor start, SymInt length) -\u003e Tensor(a) inline at::Tensor Tensor::narrow_symint(int64_t dim, const at::Tensor \u0026 start, c10::SymInt length) const { return at::_ops::narrow_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, length); } // aten::permute(Tensor(a) self, int[] dims) -\u003e Tensor(a) inline at::Tensor Tensor::permute(at::IntArrayRef dims) const { return at::_ops::permute::call(const_cast\u003cTensor\u0026\u003e(*this), dims); } // aten::movedim.intlist(Tensor(a) self, int[] source, int[] destination) -\u003e Tensor(a) inline at::Tensor Tensor::movedim(at::IntArrayRef source, at::IntArrayRef destination) const { return at::_ops::movedim_intlist::call(const_cast\u003cTensor\u0026\u003e(*this), source, destination); } // aten::movedim.int(Tensor(a) self, int source, int destination) -\u003e Tensor(a) inline at::Tensor Tensor::movedim(int64_t source, int64_t destination) const { return at::_ops::movedim_int::call(const_cast\u003cTensor\u0026\u003e(*this), source, destination); } // aten::moveaxis.intlist(Tensor(a) self, int[] source, int[] destination) -\u003e Tensor(a) inline at::Tensor Tensor::moveaxis(at::IntArrayRef source, at::IntArrayRef destination) const { return at::_ops::moveaxis_intlist::call(const_cast\u003cTensor\u0026\u003e(*this), source, destination); } // aten::moveaxis.int(Tensor(a) self, int source, int destination) -\u003e Tensor(a) inline at::Tensor Tensor::moveaxis(int64_t source, int64_t destination) const { return at::_ops::moveaxis_int::call(const_cast\u003cTensor\u0026\u003e(*this), source, destination); } // aten::numpy_T(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::numpy_T() const { return at::_ops::numpy_T::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::matrix_H(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::matrix_H() const { return at::_ops::matrix_H::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::mT(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::mT() const { return at::_ops::mT::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::mH(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::mH() const { return at::_ops::mH::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::adjoint(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::adjoint() const { return at::_ops::adjoint::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_pinned(Tensor self, Device? device=None) -\u003e bool inline bool Tensor::is_pinned(::std::optional\u003cat::Device\u003e device) const { return at::_ops::is_pinned::call(const_cast\u003cTensor\u0026\u003e(*this), device); } // aten::pin_memory(Tensor(a) self, Device? device=None) -\u003e Tensor(a) inline at::Tensor Tensor::pin_memory(::std::optional\u003cat::Device\u003e device) const { return at::_ops::pin_memory::call(const_cast\u003cTensor\u0026\u003e(*this), device); } // aten::pinverse(Tensor self, float rcond=1e-15) -\u003e Tensor inline at::Tensor Tensor::pinverse(double rcond) const { return at::_ops::pinverse::call(const_cast\u003cTensor\u0026\u003e(*this), rcond); } // aten::rad2deg(Tensor self) -\u003e Tensor inline at::Tensor Tensor::rad2deg() const { return at::_ops::rad2deg::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::rad2deg_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::rad2deg_() const { return at::_ops::rad2deg_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::deg2rad(Tensor self) -\u003e Tensor inline at::Tensor Tensor::deg2rad() const { return at::_ops::deg2rad::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::deg2rad_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::deg2rad_() const { return at::_ops::deg2rad_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::ravel(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::ravel() const { return at::_ops::ravel::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::reciprocal(Tensor self) -\u003e Tensor inline at::Tensor Tensor::reciprocal() const { return at::_ops::reciprocal::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::reciprocal_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::reciprocal_() const { return at::_ops::reciprocal_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::neg(Tensor self) -\u003e Tensor inline at::Tensor Tensor::neg() const { return at::_ops::neg::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::neg_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::neg_() const { return at::_ops::neg_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::negative(Tensor self) -\u003e Tensor inline at::Tensor Tensor::negative() const { return at::_ops::negative::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::negative_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::negative_() const { return at::_ops::negative_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::repeat(Tensor self, SymInt[] repeats) -\u003e Tensor inline at::Tensor Tensor::repeat(at::IntArrayRef repeats) const { return at::_ops::repeat::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(repeats)); } // aten::repeat(Tensor self, SymInt[] repeats) -\u003e Tensor inline at::Tensor Tensor::repeat_symint(c10::SymIntArrayRef repeats) const { return at::_ops::repeat::call(const_cast\u003cTensor\u0026\u003e(*this), repeats); } // aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor inline at::Tensor Tensor::repeat_interleave(const at::Tensor \u0026 repeats, ::std::optional\u003cint64_t\u003e dim, ::std::optional\u003cint64_t\u003e output_size) const { return at::_ops::repeat_interleave_self_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), repeats, dim, output_size.has_value() ? ::std::make_optional(c10::SymInt(*output_size)) : ::std::nullopt); } // aten::repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor inline at::Tensor Tensor::repeat_interleave_symint(const at::Tensor \u0026 repeats, ::std::optional\u003cint64_t\u003e dim, ::std::optional\u003cc10::SymInt\u003e output_size) const { return at::_ops::repeat_interleave_self_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), repeats, dim, output_size); } // aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor inline at::Tensor Tensor::repeat_interleave(int64_t repeats, ::std::optional\u003cint64_t\u003e dim, ::std::optional\u003cint64_t\u003e output_size) const { return at::_ops::repeat_interleave_self_int::call(const_cast\u003cTensor\u0026\u003e(*this), repeats, dim, output_size.has_value() ? ::std::make_optional(c10::SymInt(*output_size)) : ::std::nullopt); } // aten::repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -\u003e Tensor inline at::Tensor Tensor::repeat_interleave_symint(c10::SymInt repeats, ::std::optional\u003cint64_t\u003e dim, ::std::optional\u003cc10::SymInt\u003e output_size) const { return at::_ops::repeat_interleave_self_int::call(const_cast\u003cTensor\u0026\u003e(*this), repeats, dim, output_size); } // aten::reshape(Tensor(a) self, SymInt[] shape) -\u003e Tensor(a) inline at::Tensor Tensor::reshape(at::IntArrayRef shape) const { return at::_ops::reshape::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(shape)); } // aten::reshape(Tensor(a) self, SymInt[] shape) -\u003e Tensor(a) inline at::Tensor Tensor::reshape_symint(c10::SymIntArrayRef shape) const { return at::_ops::reshape::call(const_cast\u003cTensor\u0026\u003e(*this), shape); } // aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -\u003e Tensor(a) inline at::Tensor Tensor::_reshape_alias(at::IntArrayRef size, at::IntArrayRef stride) const { return at::_ops::_reshape_alias::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride)); } // aten::_reshape_alias(Tensor(a) self, SymInt[] size, SymInt[] stride) -\u003e Tensor(a) inline at::Tensor Tensor::_reshape_alias_symint(c10::SymIntArrayRef size, c10::SymIntArrayRef stride) const { return at::_ops::_reshape_alias::call(const_cast\u003cTensor\u0026\u003e(*this), size, stride); } // aten::reshape_as(Tensor(a) self, Tensor other) -\u003e Tensor(a) inline at::Tensor Tensor::reshape_as(const at::Tensor \u0026 other) const { return at::_ops::reshape_as::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::round(Tensor self) -\u003e Tensor inline at::Tensor Tensor::round() const { return at::_ops::round::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::round_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::round_() const { return at::_ops::round_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::round.decimals(Tensor self, *, int decimals) -\u003e Tensor inline at::Tensor Tensor::round(int64_t decimals) const { return at::_ops::round_decimals::call(const_cast\u003cTensor\u0026\u003e(*this), decimals); } // aten::round_.decimals(Tensor(a!) self, *, int decimals) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::round_(int64_t decimals) const { return at::_ops::round__decimals::call(const_cast\u003cTensor\u0026\u003e(*this), decimals); } // aten::relu(Tensor self) -\u003e Tensor inline at::Tensor Tensor::relu() const { return at::_ops::relu::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::relu_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::relu_() const { return at::_ops::relu_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::prelu(Tensor self, Tensor weight) -\u003e Tensor inline at::Tensor Tensor::prelu(const at::Tensor \u0026 weight) const { return at::_ops::prelu::call(const_cast\u003cTensor\u0026\u003e(*this), weight); } // aten::hardshrink(Tensor self, Scalar lambd=0.5) -\u003e Tensor inline at::Tensor Tensor::hardshrink(const at::Scalar \u0026 lambd) const { return at::_ops::hardshrink::call(const_cast\u003cTensor\u0026\u003e(*this), lambd); } // aten::hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -\u003e Tensor inline at::Tensor Tensor::hardshrink_backward(const at::Tensor \u0026 grad_out, const at::Scalar \u0026 lambd) const { return at::_ops::hardshrink_backward::call(grad_out, const_cast\u003cTensor\u0026\u003e(*this), lambd); } // aten::rsqrt(Tensor self) -\u003e Tensor inline at::Tensor Tensor::rsqrt() const { return at::_ops::rsqrt::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::rsqrt_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::rsqrt_() const { return at::_ops::rsqrt_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::select.Dimname(Tensor(a) self, Dimname dim, int index) -\u003e Tensor(a) inline at::Tensor Tensor::select(at::Dimname dim, int64_t index) const { return at::_ops::select_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index); } // aten::select.int(Tensor(a) self, int dim, SymInt index) -\u003e Tensor(a) inline at::Tensor Tensor::select(int64_t dim, int64_t index) const { return at::_ops::select_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index); } // aten::select.int(Tensor(a) self, int dim, SymInt index) -\u003e Tensor(a) inline at::Tensor Tensor::select_symint(int64_t dim, c10::SymInt index) const { return at::_ops::select_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index); } // aten::sigmoid(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sigmoid() const { return at::_ops::sigmoid::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sigmoid_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sigmoid_() const { return at::_ops::sigmoid_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logit(Tensor self, float? eps=None) -\u003e Tensor inline at::Tensor Tensor::logit(::std::optional\u003cdouble\u003e eps) const { return at::_ops::logit::call(const_cast\u003cTensor\u0026\u003e(*this), eps); } // aten::logit_(Tensor(a!) self, float? eps=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::logit_(::std::optional\u003cdouble\u003e eps) const { return at::_ops::logit_::call(const_cast\u003cTensor\u0026\u003e(*this), eps); } // aten::sin(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sin() const { return at::_ops::sin::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sin_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sin_() const { return at::_ops::sin_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sinc(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sinc() const { return at::_ops::sinc::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sinc_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sinc_() const { return at::_ops::sinc_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sinh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sinh() const { return at::_ops::sinh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sinh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sinh_() const { return at::_ops::sinh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::detach(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::detach() const { return at::_ops::detach::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::detach_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::detach_() const { return at::_ops::detach_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::size.Dimname(Tensor self, Dimname dim) -\u003e int inline int64_t Tensor::size(at::Dimname dim) const { return at::_ops::size_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a) inline at::Tensor Tensor::slice(int64_t dim, ::std::optional\u003cint64_t\u003e start, ::std::optional\u003cint64_t\u003e end, int64_t step) const { return at::_ops::slice_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start.has_value() ? ::std::make_optional(c10::SymInt(*start)) : ::std::nullopt, end.has_value() ? ::std::make_optional(c10::SymInt(*end)) : ::std::nullopt, step); } // aten::slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a) inline at::Tensor Tensor::slice_symint(int64_t dim, ::std::optional\u003cc10::SymInt\u003e start, ::std::optional\u003cc10::SymInt\u003e end, c10::SymInt step) const { return at::_ops::slice_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, start, end, step); } // aten::slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a) inline at::Tensor Tensor::slice_inverse(const at::Tensor \u0026 src, int64_t dim, ::std::optional\u003cint64_t\u003e start, ::std::optional\u003cint64_t\u003e end, int64_t step) const { return at::_ops::slice_inverse::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, start.has_value() ? ::std::make_optional(c10::SymInt(*start)) : ::std::nullopt, end.has_value() ? ::std::make_optional(c10::SymInt(*end)) : ::std::nullopt, step); } // aten::slice_inverse(Tensor(a) self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor(a) inline at::Tensor Tensor::slice_inverse_symint(const at::Tensor \u0026 src, int64_t dim, ::std::optional\u003cc10::SymInt\u003e start, ::std::optional\u003cc10::SymInt\u003e end, c10::SymInt step) const { return at::_ops::slice_inverse::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, start, end, step); } // aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor inline at::Tensor Tensor::slice_scatter(const at::Tensor \u0026 src, int64_t dim, ::std::optional\u003cint64_t\u003e start, ::std::optional\u003cint64_t\u003e end, int64_t step) const { return at::_ops::slice_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, start.has_value() ? ::std::make_optional(c10::SymInt(*start)) : ::std::nullopt, end.has_value() ? ::std::make_optional(c10::SymInt(*end)) : ::std::nullopt, step); } // aten::slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -\u003e Tensor inline at::Tensor Tensor::slice_scatter_symint(const at::Tensor \u0026 src, int64_t dim, ::std::optional\u003cc10::SymInt\u003e start, ::std::optional\u003cc10::SymInt\u003e end, c10::SymInt step) const { return at::_ops::slice_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, start, end, step); } // aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -\u003e Tensor inline at::Tensor Tensor::select_scatter(const at::Tensor \u0026 src, int64_t dim, int64_t index) const { return at::_ops::select_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, index); } // aten::select_scatter(Tensor self, Tensor src, int dim, SymInt index) -\u003e Tensor inline at::Tensor Tensor::select_scatter_symint(const at::Tensor \u0026 src, int64_t dim, c10::SymInt index) const { return at::_ops::select_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, dim, index); } // aten::diagonal_scatter(Tensor self, Tensor src, int offset=0, int dim1=0, int dim2=1) -\u003e Tensor inline at::Tensor Tensor::diagonal_scatter(const at::Tensor \u0026 src, int64_t offset, int64_t dim1, int64_t dim2) const { return at::_ops::diagonal_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, offset, dim1, dim2); } // aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor inline at::Tensor Tensor::as_strided_scatter(const at::Tensor \u0026 src, at::IntArrayRef size, at::IntArrayRef stride, ::std::optional\u003cint64_t\u003e storage_offset) const { return at::_ops::as_strided_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride), storage_offset.has_value() ? ::std::make_optional(c10::SymInt(*storage_offset)) : ::std::nullopt); } // aten::as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -\u003e Tensor inline at::Tensor Tensor::as_strided_scatter_symint(const at::Tensor \u0026 src, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional\u003cc10::SymInt\u003e storage_offset) const { return at::_ops::as_strided_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), src, size, stride, storage_offset); } // aten::smm(Tensor self, Tensor mat2) -\u003e Tensor inline at::Tensor Tensor::smm(const at::Tensor \u0026 mat2) const { return at::_ops::smm::call(const_cast\u003cTensor\u0026\u003e(*this), mat2); } // aten::softmax.int(Tensor self, int dim, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::softmax(int64_t dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::softmax_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::softmax(at::Dimname dim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::softmax_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, dtype); } // aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unsafe_split(int64_t split_size, int64_t dim) const { return at::_ops::unsafe_split_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), split_size, dim); } // aten::unsafe_split.Tensor(Tensor self, SymInt split_size, int dim=0) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unsafe_split_symint(c10::SymInt split_size, int64_t dim) const { return at::_ops::unsafe_split_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), split_size, dim); } // aten::split.Tensor(Tensor(a -\u003e *) self, SymInt split_size, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split(int64_t split_size, int64_t dim) const { return at::_ops::split_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), split_size, dim); } // aten::split.Tensor(Tensor(a -\u003e *) self, SymInt split_size, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split_symint(c10::SymInt split_size, int64_t dim) const { return at::_ops::split_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), split_size, dim); } // aten::split.sizes(Tensor(a -\u003e *) self, SymInt[] split_size, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split(at::IntArrayRef split_size, int64_t dim) const { return at::_ops::split_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(split_size), dim); } // aten::split.sizes(Tensor(a -\u003e *) self, SymInt[] split_size, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split_symint(c10::SymIntArrayRef split_size, int64_t dim) const { return at::_ops::split_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), split_size, dim); } // aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unsafe_split_with_sizes(at::IntArrayRef split_sizes, int64_t dim) const { return at::_ops::unsafe_split_with_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(split_sizes), dim); } // aten::unsafe_split_with_sizes(Tensor self, SymInt[] split_sizes, int dim=0) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unsafe_split_with_sizes_symint(c10::SymIntArrayRef split_sizes, int64_t dim) const { return at::_ops::unsafe_split_with_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), split_sizes, dim); } // aten::split_with_sizes(Tensor(a -\u003e *) self, SymInt[] split_sizes, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split_with_sizes(at::IntArrayRef split_sizes, int64_t dim) const { return at::_ops::split_with_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(split_sizes), dim); } // aten::split_with_sizes(Tensor(a -\u003e *) self, SymInt[] split_sizes, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::split_with_sizes_symint(c10::SymIntArrayRef split_sizes, int64_t dim) const { return at::_ops::split_with_sizes::call(const_cast\u003cTensor\u0026\u003e(*this), split_sizes, dim); } // aten::hsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::hsplit(int64_t sections) const { return at::_ops::hsplit_int::call(const_cast\u003cTensor\u0026\u003e(*this), sections); } // aten::hsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::hsplit(at::IntArrayRef indices) const { return at::_ops::hsplit_array::call(const_cast\u003cTensor\u0026\u003e(*this), indices); } // aten::vsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::vsplit(int64_t sections) const { return at::_ops::vsplit_int::call(const_cast\u003cTensor\u0026\u003e(*this), sections); } // aten::vsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::vsplit(at::IntArrayRef indices) const { return at::_ops::vsplit_array::call(const_cast\u003cTensor\u0026\u003e(*this), indices); } // aten::dsplit.int(Tensor(a -\u003e *) self, int sections) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::dsplit(int64_t sections) const { return at::_ops::dsplit_int::call(const_cast\u003cTensor\u0026\u003e(*this), sections); } // aten::dsplit.array(Tensor(a -\u003e *) self, int[] indices) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::dsplit(at::IntArrayRef indices) const { return at::_ops::dsplit_array::call(const_cast\u003cTensor\u0026\u003e(*this), indices); } // aten::squeeze(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::squeeze() const { return at::_ops::squeeze::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::squeeze.dim(Tensor(a) self, int dim) -\u003e Tensor(a) inline at::Tensor Tensor::squeeze(int64_t dim) const { return at::_ops::squeeze_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::squeeze.dimname(Tensor(a) self, Dimname dim) -\u003e Tensor(a) inline at::Tensor Tensor::squeeze(at::Dimname dim) const { return at::_ops::squeeze_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::squeeze.dims(Tensor(a) self, int[] dim) -\u003e Tensor(a) inline at::Tensor Tensor::squeeze(at::IntArrayRef dim) const { return at::_ops::squeeze_dims::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::squeeze_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::squeeze_() const { return at::_ops::squeeze_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::squeeze_.dim(Tensor(a!) self, int dim) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::squeeze_(int64_t dim) const { return at::_ops::squeeze__dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::squeeze_.dims(Tensor(a!) self, int[] dim) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::squeeze_(at::IntArrayRef dim) const { return at::_ops::squeeze__dims::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::squeeze_.dimname(Tensor(a!) self, Dimname dim) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::squeeze_(at::Dimname dim) const { return at::_ops::squeeze__dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::sspaddmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::sspaddmm(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::sspaddmm::call(const_cast\u003cTensor\u0026\u003e(*this), mat1, mat2, beta, alpha); } // aten::stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -\u003e Tensor inline at::Tensor Tensor::stft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length, ::std::optional\u003cint64_t\u003e win_length, const ::std::optional\u003cat::Tensor\u003e \u0026 window, bool normalized, ::std::optional\u003cbool\u003e onesided, ::std::optional\u003cbool\u003e return_complex, ::std::optional\u003cbool\u003e align_to_window) const { return at::_ops::stft::call(const_cast\u003cTensor\u0026\u003e(*this), n_fft, hop_length, win_length, window, normalized, onesided, return_complex, align_to_window); } // aten::stft.center(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, str pad_mode=\"reflect\", bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -\u003e Tensor inline at::Tensor Tensor::stft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length, ::std::optional\u003cint64_t\u003e win_length, const ::std::optional\u003cat::Tensor\u003e \u0026 window, bool center, c10::string_view pad_mode, bool normalized, ::std::optional\u003cbool\u003e onesided, ::std::optional\u003cbool\u003e return_complex, ::std::optional\u003cbool\u003e align_to_window) const { return at::_ops::stft_center::call(const_cast\u003cTensor\u0026\u003e(*this), n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex, align_to_window); } // aten::istft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool center=True, bool normalized=False, bool? onesided=None, int? length=None, bool return_complex=False) -\u003e Tensor inline at::Tensor Tensor::istft(int64_t n_fft, ::std::optional\u003cint64_t\u003e hop_length, ::std::optional\u003cint64_t\u003e win_length, const ::std::optional\u003cat::Tensor\u003e \u0026 window, bool center, bool normalized, ::std::optional\u003cbool\u003e onesided, ::std::optional\u003cint64_t\u003e length, bool return_complex) const { return at::_ops::istft::call(const_cast\u003cTensor\u0026\u003e(*this), n_fft, hop_length, win_length, window, center, normalized, onesided, length, return_complex); } // aten::stride.Dimname(Tensor self, Dimname dim) -\u003e int inline int64_t Tensor::stride(at::Dimname dim) const { return at::_ops::stride_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::sum(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::sum(::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::sum::call(const_cast\u003cTensor\u0026\u003e(*this), dtype); } // aten::sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::sum(at::OptionalIntArrayRef dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::sum_dim_IntList::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::sum(at::DimnameList dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::sum_dim_DimnameList::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::nansum(Tensor self, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::nansum(at::OptionalIntArrayRef dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::nansum::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::hash_tensor(Tensor self, int[1] dim=[], *, bool keepdim=False, int mode=0) -\u003e Tensor inline at::Tensor Tensor::hash_tensor(at::IntArrayRef dim, bool keepdim, int64_t mode) const { return at::_ops::hash_tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, mode); } // aten::sum_to_size(Tensor self, SymInt[] size) -\u003e Tensor inline at::Tensor Tensor::sum_to_size(at::IntArrayRef size) const { return at::_ops::sum_to_size::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size)); } // aten::sum_to_size(Tensor self, SymInt[] size) -\u003e Tensor inline at::Tensor Tensor::sum_to_size_symint(c10::SymIntArrayRef size) const { return at::_ops::sum_to_size::call(const_cast\u003cTensor\u0026\u003e(*this), size); } // aten::sqrt(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sqrt() const { return at::_ops::sqrt::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sqrt_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sqrt_() const { return at::_ops::sqrt_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::square(Tensor self) -\u003e Tensor inline at::Tensor Tensor::square() const { return at::_ops::square::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::square_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::square_() const { return at::_ops::square_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::std(Tensor self, bool unbiased=True) -\u003e Tensor inline at::Tensor Tensor::std(bool unbiased) const { return at::_ops::std::call(const_cast\u003cTensor\u0026\u003e(*this), unbiased); } // aten::std.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::std(at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) const { return at::_ops::std_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, unbiased, keepdim); } // aten::std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::std(at::OptionalIntArrayRef dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction, bool keepdim) const { return at::_ops::std_correction::call(const_cast\u003cTensor\u0026\u003e(*this), dim, correction, keepdim); } // aten::std.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::std(at::DimnameList dim, bool unbiased, bool keepdim) const { return at::_ops::std_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, unbiased, keepdim); } // aten::std.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::std(at::DimnameList dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction, bool keepdim) const { return at::_ops::std_correction_names::call(const_cast\u003cTensor\u0026\u003e(*this), dim, correction, keepdim); } // aten::prod(Tensor self, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::prod(::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::prod::call(const_cast\u003cTensor\u0026\u003e(*this), dtype); } // aten::prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::prod(int64_t dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::prod_dim_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::prod.dim_Dimname(Tensor self, Dimname dim, bool keepdim=False, *, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::prod(at::Dimname dim, bool keepdim, ::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::prod_dim_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, keepdim, dtype); } // aten::t(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::t() const { return at::_ops::t::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::t_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::t_() const { return at::_ops::t_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::tan(Tensor self) -\u003e Tensor inline at::Tensor Tensor::tan() const { return at::_ops::tan::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::tan_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::tan_() const { return at::_ops::tan_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::tanh(Tensor self) -\u003e Tensor inline at::Tensor Tensor::tanh() const { return at::_ops::tanh::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::tanh_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::tanh_() const { return at::_ops::tanh_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::tile(Tensor self, SymInt[] dims) -\u003e Tensor inline at::Tensor Tensor::tile(at::IntArrayRef dims) const { return at::_ops::tile::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(dims)); } // aten::tile(Tensor self, SymInt[] dims) -\u003e Tensor inline at::Tensor Tensor::tile_symint(c10::SymIntArrayRef dims) const { return at::_ops::tile::call(const_cast\u003cTensor\u0026\u003e(*this), dims); } // aten::transpose.int(Tensor(a) self, int dim0, int dim1) -\u003e Tensor(a) inline at::Tensor Tensor::transpose(int64_t dim0, int64_t dim1) const { return at::_ops::transpose_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim0, dim1); } // aten::transpose.Dimname(Tensor(a) self, Dimname dim0, Dimname dim1) -\u003e Tensor(a) inline at::Tensor Tensor::transpose(at::Dimname dim0, at::Dimname dim1) const { return at::_ops::transpose_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim0, dim1); } // aten::transpose_(Tensor(a!) self, int dim0, int dim1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::transpose_(int64_t dim0, int64_t dim1) const { return at::_ops::transpose_::call(const_cast\u003cTensor\u0026\u003e(*this), dim0, dim1); } // aten::flip(Tensor self, int[] dims) -\u003e Tensor inline at::Tensor Tensor::flip(at::IntArrayRef dims) const { return at::_ops::flip::call(const_cast\u003cTensor\u0026\u003e(*this), dims); } // aten::fliplr(Tensor self) -\u003e Tensor inline at::Tensor Tensor::fliplr() const { return at::_ops::fliplr::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::flipud(Tensor self) -\u003e Tensor inline at::Tensor Tensor::flipud() const { return at::_ops::flipud::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -\u003e Tensor inline at::Tensor Tensor::roll(at::IntArrayRef shifts, at::IntArrayRef dims) const { return at::_ops::roll::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(shifts), dims); } // aten::roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -\u003e Tensor inline at::Tensor Tensor::roll_symint(c10::SymIntArrayRef shifts, at::IntArrayRef dims) const { return at::_ops::roll::call(const_cast\u003cTensor\u0026\u003e(*this), shifts, dims); } // aten::rot90(Tensor self, int k=1, int[] dims=[0,1]) -\u003e Tensor inline at::Tensor Tensor::rot90(int64_t k, at::IntArrayRef dims) const { return at::_ops::rot90::call(const_cast\u003cTensor\u0026\u003e(*this), k, dims); } // aten::_nested_tensor_size(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_nested_tensor_size() const { return at::_ops::_nested_tensor_size::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_nested_tensor_strides(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_nested_tensor_strides() const { return at::_ops::_nested_tensor_strides::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_nested_tensor_storage_offsets(Tensor self) -\u003e Tensor inline at::Tensor Tensor::_nested_tensor_storage_offsets() const { return at::_ops::_nested_tensor_storage_offsets::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::trunc(Tensor self) -\u003e Tensor inline at::Tensor Tensor::trunc() const { return at::_ops::trunc::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::trunc_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::trunc_() const { return at::_ops::trunc_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::fix(Tensor self) -\u003e Tensor inline at::Tensor Tensor::fix() const { return at::_ops::fix::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::fix_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fix_() const { return at::_ops::fix_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::type_as(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::type_as(const at::Tensor \u0026 other) const { return at::_ops::type_as::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::unsqueeze(Tensor(a) self, int dim) -\u003e Tensor(a) inline at::Tensor Tensor::unsqueeze(int64_t dim) const { return at::_ops::unsqueeze::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::unsqueeze_(Tensor(a!) self, int dim) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::unsqueeze_(int64_t dim) const { return at::_ops::unsqueeze_::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::var(Tensor self, bool unbiased=True) -\u003e Tensor inline at::Tensor Tensor::var(bool unbiased) const { return at::_ops::var::call(const_cast\u003cTensor\u0026\u003e(*this), unbiased); } // aten::var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::var(at::OptionalIntArrayRef dim, bool unbiased, bool keepdim) const { return at::_ops::var_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, unbiased, keepdim); } // aten::var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::var(at::OptionalIntArrayRef dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction, bool keepdim) const { return at::_ops::var_correction::call(const_cast\u003cTensor\u0026\u003e(*this), dim, correction, keepdim); } // aten::var.names_dim(Tensor self, Dimname[1] dim, bool unbiased=True, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::var(at::DimnameList dim, bool unbiased, bool keepdim) const { return at::_ops::var_names_dim::call(const_cast\u003cTensor\u0026\u003e(*this), dim, unbiased, keepdim); } // aten::var.correction_names(Tensor self, Dimname[1] dim, *, Scalar? correction=None, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::var(at::DimnameList dim, const ::std::optional\u003cat::Scalar\u003e \u0026 correction, bool keepdim) const { return at::_ops::var_correction_names::call(const_cast\u003cTensor\u0026\u003e(*this), dim, correction, keepdim); } // aten::view_as(Tensor(a) self, Tensor other) -\u003e Tensor(a) inline at::Tensor Tensor::view_as(const at::Tensor \u0026 other) const { return at::_ops::view_as::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::where.self(Tensor condition, Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::where(const at::Tensor \u0026 condition, const at::Tensor \u0026 other) const { return at::_ops::where_self::call(condition, const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::where.ScalarOther(Tensor condition, Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::where(const at::Tensor \u0026 condition, const at::Scalar \u0026 other) const { return at::_ops::where_ScalarOther::call(condition, const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -\u003e Tensor inline at::Tensor Tensor::norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::ScalarType dtype) const { return at::_ops::norm_ScalarOpt_dtype::call(const_cast\u003cTensor\u0026\u003e(*this), p, dtype); } // aten::norm.Scalar(Tensor self, Scalar p=2) -\u003e Tensor inline at::Tensor Tensor::norm(const at::Scalar \u0026 p) const { return at::_ops::norm_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), p); } // aten::norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -\u003e Tensor inline at::Tensor Tensor::norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) const { return at::_ops::norm_ScalarOpt_dim_dtype::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, keepdim, dtype); } // aten::norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::IntArrayRef dim, bool keepdim) const { return at::_ops::norm_ScalarOpt_dim::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, keepdim); } // aten::norm.names_ScalarOpt_dim_dtype(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim, *, ScalarType dtype) -\u003e Tensor inline at::Tensor Tensor::norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::DimnameList dim, bool keepdim, at::ScalarType dtype) const { return at::_ops::norm_names_ScalarOpt_dim_dtype::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, keepdim, dtype); } // aten::norm.names_ScalarOpt_dim(Tensor self, Scalar? p, Dimname[1] dim, bool keepdim=False) -\u003e Tensor inline at::Tensor Tensor::norm(const ::std::optional\u003cat::Scalar\u003e \u0026 p, at::DimnameList dim, bool keepdim) const { return at::_ops::norm_names_ScalarOpt_dim::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, keepdim); } // aten::frexp.Tensor(Tensor self) -\u003e (Tensor mantissa, Tensor exponent) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::frexp() const { return at::_ops::frexp_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::clone(Tensor self, *, MemoryFormat? memory_format=None) -\u003e Tensor inline at::Tensor Tensor::clone(::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::clone::call(const_cast\u003cTensor\u0026\u003e(*this), memory_format); } // aten::positive(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::positive() const { return at::_ops::positive::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::resize_as_(Tensor(a!) self, Tensor the_template, *, MemoryFormat? memory_format=None) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::resize_as_(const at::Tensor \u0026 the_template, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::resize_as_::call(const_cast\u003cTensor\u0026\u003e(*this), the_template, memory_format); } // aten::resize_as_sparse_(Tensor(a!) self, Tensor the_template) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::resize_as_sparse_(const at::Tensor \u0026 the_template) const { return at::_ops::resize_as_sparse_::call(const_cast\u003cTensor\u0026\u003e(*this), the_template); } // aten::zero_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::zero_() const { return at::_ops::zero_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::sub(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::sub_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sub_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::sub__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::sub(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::sub_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sub_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::sub__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::subtract.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::subtract(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::subtract_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::subtract_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::subtract_(const at::Tensor \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::subtract__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::subtract.Scalar(Tensor self, Scalar other, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::subtract(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::subtract_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::subtract_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::subtract_(const at::Scalar \u0026 other, const at::Scalar \u0026 alpha) const { return at::_ops::subtract__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other, alpha); } // aten::heaviside(Tensor self, Tensor values) -\u003e Tensor inline at::Tensor Tensor::heaviside(const at::Tensor \u0026 values) const { return at::_ops::heaviside::call(const_cast\u003cTensor\u0026\u003e(*this), values); } // aten::heaviside_(Tensor(a!) self, Tensor values) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::heaviside_(const at::Tensor \u0026 values) const { return at::_ops::heaviside_::call(const_cast\u003cTensor\u0026\u003e(*this), values); } // aten::addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::addmm(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addmm::call(const_cast\u003cTensor\u0026\u003e(*this), mat1, mat2, beta, alpha); } // aten::addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addmm_(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addmm_::call(const_cast\u003cTensor\u0026\u003e(*this), mat1, mat2, beta, alpha); } // aten::_addmm_activation(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, bool use_gelu=False) -\u003e Tensor inline at::Tensor Tensor::_addmm_activation(const at::Tensor \u0026 mat1, const at::Tensor \u0026 mat2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha, bool use_gelu) const { return at::_ops::_addmm_activation::call(const_cast\u003cTensor\u0026\u003e(*this), mat1, mat2, beta, alpha, use_gelu); } // aten::sparse_resize_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::sparse_resize_(at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const { return at::_ops::sparse_resize_::call(const_cast\u003cTensor\u0026\u003e(*this), size, sparse_dim, dense_dim); } // aten::sparse_resize_and_clear_(Tensor(a!) self, int[] size, int sparse_dim, int dense_dim) -\u003e Tensor(a!) inline const at::Tensor \u0026 Tensor::sparse_resize_and_clear_(at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) const { return at::_ops::sparse_resize_and_clear_::call(const_cast\u003cTensor\u0026\u003e(*this), size, sparse_dim, dense_dim); } // aten::sparse_mask(Tensor self, Tensor mask) -\u003e Tensor inline at::Tensor Tensor::sparse_mask(const at::Tensor \u0026 mask) const { return at::_ops::sparse_mask::call(const_cast\u003cTensor\u0026\u003e(*this), mask); } // aten::_sparse_mask_projection(Tensor self, Tensor mask, bool accumulate_matches=False) -\u003e Tensor inline at::Tensor Tensor::_sparse_mask_projection(const at::Tensor \u0026 mask, bool accumulate_matches) const { return at::_ops::_sparse_mask_projection::call(const_cast\u003cTensor\u0026\u003e(*this), mask, accumulate_matches); } // aten::to_dense(Tensor self, ScalarType? dtype=None, *, bool? masked_grad=None) -\u003e Tensor inline at::Tensor Tensor::to_dense(::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cbool\u003e masked_grad) const { return at::_ops::to_dense::call(const_cast\u003cTensor\u0026\u003e(*this), dtype, masked_grad); } // aten::_to_dense(Tensor self, ScalarType? dtype=None, bool? masked_grad=None) -\u003e Tensor inline at::Tensor Tensor::_to_dense(::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cbool\u003e masked_grad) const { return at::_ops::_to_dense::call(const_cast\u003cTensor\u0026\u003e(*this), dtype, masked_grad); } // aten::sparse_dim(Tensor self) -\u003e int inline int64_t Tensor::sparse_dim() const { return at::_ops::sparse_dim::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_dimI(Tensor self) -\u003e int inline int64_t Tensor::_dimI() const { return at::_ops::_dimI::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::dense_dim(Tensor self) -\u003e int inline int64_t Tensor::dense_dim() const { return at::_ops::dense_dim::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_dimV(Tensor self) -\u003e int inline int64_t Tensor::_dimV() const { return at::_ops::_dimV::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_nnz(Tensor self) -\u003e int inline int64_t Tensor::_nnz() const { return at::_ops::_nnz::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::coalesce(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::coalesce() const { return at::_ops::coalesce::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_coalesced(Tensor self) -\u003e bool inline bool Tensor::is_coalesced() const { return at::_ops::is_coalesced::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::_indices() const { return at::_ops::_indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_values(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::_values() const { return at::_ops::_values::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_coalesced_(Tensor(a!) self, bool coalesced) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::_coalesced_(bool coalesced) const { return at::_ops::_coalesced_::call(const_cast\u003cTensor\u0026\u003e(*this), coalesced); } // aten::indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::indices() const { return at::_ops::indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::values(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::values() const { return at::_ops::values::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::crow_indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::crow_indices() const { return at::_ops::crow_indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::col_indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::col_indices() const { return at::_ops::col_indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::ccol_indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::ccol_indices() const { return at::_ops::ccol_indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::row_indices(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::row_indices() const { return at::_ops::row_indices::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::unbind.int(Tensor(a -\u003e *) self, int dim=0) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unbind(int64_t dim) const { return at::_ops::unbind_int::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::unbind.Dimname(Tensor(a -\u003e *) self, Dimname dim) -\u003e Tensor(a)[] inline ::std::vector\u003cat::Tensor\u003e Tensor::unbind(at::Dimname dim) const { return at::_ops::unbind_Dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim); } // aten::to_sparse.sparse_dim(Tensor self, int sparse_dim) -\u003e Tensor inline at::Tensor Tensor::to_sparse(int64_t sparse_dim) const { return at::_ops::to_sparse_sparse_dim::call(const_cast\u003cTensor\u0026\u003e(*this), sparse_dim); } // aten::_to_sparse.sparse_dim(Tensor self, int sparse_dim) -\u003e Tensor inline at::Tensor Tensor::_to_sparse(int64_t sparse_dim) const { return at::_ops::_to_sparse_sparse_dim::call(const_cast\u003cTensor\u0026\u003e(*this), sparse_dim); } // aten::to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::to_sparse(::std::optional\u003cat::Layout\u003e layout, at::OptionalIntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::to_sparse::call(const_cast\u003cTensor\u0026\u003e(*this), layout, blocksize, dense_dim); } // aten::_to_sparse(Tensor self, *, Layout? layout=None, int[2]? blocksize=None, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::_to_sparse(::std::optional\u003cat::Layout\u003e layout, at::OptionalIntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::_to_sparse::call(const_cast\u003cTensor\u0026\u003e(*this), layout, blocksize, dense_dim); } // aten::to_sparse_csr(Tensor self, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::to_sparse_csr(::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::to_sparse_csr::call(const_cast\u003cTensor\u0026\u003e(*this), dense_dim); } // aten::_to_sparse_csr(Tensor self, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::_to_sparse_csr(::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::_to_sparse_csr::call(const_cast\u003cTensor\u0026\u003e(*this), dense_dim); } // aten::to_sparse_csc(Tensor self, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::to_sparse_csc(::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::to_sparse_csc::call(const_cast\u003cTensor\u0026\u003e(*this), dense_dim); } // aten::_to_sparse_csc(Tensor self, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::_to_sparse_csc(::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::_to_sparse_csc::call(const_cast\u003cTensor\u0026\u003e(*this), dense_dim); } // aten::to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::to_sparse_bsr(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::to_sparse_bsr::call(const_cast\u003cTensor\u0026\u003e(*this), blocksize, dense_dim); } // aten::_to_sparse_bsr(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::_to_sparse_bsr(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::_to_sparse_bsr::call(const_cast\u003cTensor\u0026\u003e(*this), blocksize, dense_dim); } // aten::to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::to_sparse_bsc(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::to_sparse_bsc::call(const_cast\u003cTensor\u0026\u003e(*this), blocksize, dense_dim); } // aten::_to_sparse_bsc(Tensor self, int[2] blocksize, int? dense_dim=None) -\u003e Tensor inline at::Tensor Tensor::_to_sparse_bsc(at::IntArrayRef blocksize, ::std::optional\u003cint64_t\u003e dense_dim) const { return at::_ops::_to_sparse_bsc::call(const_cast\u003cTensor\u0026\u003e(*this), blocksize, dense_dim); } // aten::to_mkldnn(Tensor self, ScalarType? dtype=None) -\u003e Tensor inline at::Tensor Tensor::to_mkldnn(::std::optional\u003cat::ScalarType\u003e dtype) const { return at::_ops::to_mkldnn::call(const_cast\u003cTensor\u0026\u003e(*this), dtype); } // aten::dequantize.self(Tensor self) -\u003e Tensor inline at::Tensor Tensor::dequantize() const { return at::_ops::dequantize_self::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::q_scale(Tensor self) -\u003e float inline double Tensor::q_scale() const { return at::_ops::q_scale::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::q_zero_point(Tensor self) -\u003e int inline int64_t Tensor::q_zero_point() const { return at::_ops::q_zero_point::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::q_per_channel_scales(Tensor self) -\u003e Tensor inline at::Tensor Tensor::q_per_channel_scales() const { return at::_ops::q_per_channel_scales::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::q_per_channel_zero_points(Tensor self) -\u003e Tensor inline at::Tensor Tensor::q_per_channel_zero_points() const { return at::_ops::q_per_channel_zero_points::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::q_per_channel_axis(Tensor self) -\u003e int inline int64_t Tensor::q_per_channel_axis() const { return at::_ops::q_per_channel_axis::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::int_repr(Tensor self) -\u003e Tensor inline at::Tensor Tensor::int_repr() const { return at::_ops::int_repr::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::qscheme(Tensor self) -\u003e QScheme inline at::QScheme Tensor::qscheme() const { return at::_ops::qscheme::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::_autocast_to_reduced_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled, ScalarType cuda_dtype, ScalarType cpu_dtype) -\u003e Tensor(a) inline at::Tensor Tensor::_autocast_to_reduced_precision(bool cuda_enabled, bool cpu_enabled, at::ScalarType cuda_dtype, at::ScalarType cpu_dtype) const { return at::_ops::_autocast_to_reduced_precision::call(const_cast\u003cTensor\u0026\u003e(*this), cuda_enabled, cpu_enabled, cuda_dtype, cpu_dtype); } // aten::_autocast_to_full_precision(Tensor(a) self, bool cuda_enabled, bool cpu_enabled) -\u003e Tensor(a) inline at::Tensor Tensor::_autocast_to_full_precision(bool cuda_enabled, bool cpu_enabled) const { return at::_ops::_autocast_to_full_precision::call(const_cast\u003cTensor\u0026\u003e(*this), cuda_enabled, cpu_enabled); } // aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a) inline at::Tensor Tensor::to(at::TensorOptions options, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::to_dtype_layout::call(const_cast\u003cTensor\u0026\u003e(*this), c10::optTypeMetaToScalarType(options.dtype_opt()), options.layout_opt(), options.device_opt(), options.pinned_memory_opt(), non_blocking, copy, c10::impl::check_tensor_options_and_extract_memory_format(options, memory_format)); } // aten::to.dtype_layout(Tensor(a) self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a) inline at::Tensor Tensor::to(::std::optional\u003cat::ScalarType\u003e dtype, ::std::optional\u003cat::Layout\u003e layout, ::std::optional\u003cat::Device\u003e device, ::std::optional\u003cbool\u003e pin_memory, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::to_dtype_layout::call(const_cast\u003cTensor\u0026\u003e(*this), dtype, layout, device, pin_memory, non_blocking, copy, memory_format); } // aten::to.device(Tensor(a) self, Device device, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a) inline at::Tensor Tensor::to(at::Device device, at::ScalarType dtype, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::to_device::call(const_cast\u003cTensor\u0026\u003e(*this), device, dtype, non_blocking, copy, memory_format); } // aten::to.dtype(Tensor(a) self, ScalarType dtype, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a) inline at::Tensor Tensor::to(at::ScalarType dtype, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::to_dtype::call(const_cast\u003cTensor\u0026\u003e(*this), dtype, non_blocking, copy, memory_format); } // aten::to.other(Tensor(a) self, Tensor other, bool non_blocking=False, bool copy=False, MemoryFormat? memory_format=None) -\u003e Tensor(a) inline at::Tensor Tensor::to(const at::Tensor \u0026 other, bool non_blocking, bool copy, ::std::optional\u003cat::MemoryFormat\u003e memory_format) const { return at::_ops::to_other::call(const_cast\u003cTensor\u0026\u003e(*this), other, non_blocking, copy, memory_format); } // aten::item(Tensor self) -\u003e Scalar inline at::Scalar Tensor::item() const { return at::_ops::item::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::set_.source_Storage(Tensor(a!) self, Storage source) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set_(at::Storage source) const { return at::_ops::set__source_Storage::call(const_cast\u003cTensor\u0026\u003e(*this), source); } // aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set_(at::Storage source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) const { return at::_ops::set__source_Storage_storage_offset::call(const_cast\u003cTensor\u0026\u003e(*this), source, storage_offset, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride)); } // aten::set_.source_Storage_storage_offset(Tensor(a!) self, Storage source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set__symint(at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) const { return at::_ops::set__source_Storage_storage_offset::call(const_cast\u003cTensor\u0026\u003e(*this), source, storage_offset, size, stride); } // aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set_(const at::Tensor \u0026 source, int64_t storage_offset, at::IntArrayRef size, at::IntArrayRef stride) const { return at::_ops::set__source_Tensor_storage_offset::call(const_cast\u003cTensor\u0026\u003e(*this), source, storage_offset, c10::fromIntArrayRefSlow(size), c10::fromIntArrayRefSlow(stride)); } // aten::set_.source_Tensor_storage_offset(Tensor(a!) self, Tensor source, SymInt storage_offset, SymInt[] size, SymInt[] stride=[]) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set__symint(const at::Tensor \u0026 source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) const { return at::_ops::set__source_Tensor_storage_offset::call(const_cast\u003cTensor\u0026\u003e(*this), source, storage_offset, size, stride); } // aten::set_.source_Tensor(Tensor(a!) self, Tensor source) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set_(const at::Tensor \u0026 source) const { return at::_ops::set__source_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), source); } // aten::set_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::set_() const { return at::_ops::set_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::is_set_to(Tensor self, Tensor tensor) -\u003e bool inline bool Tensor::is_set_to(const at::Tensor \u0026 tensor) const { return at::_ops::is_set_to::call(const_cast\u003cTensor\u0026\u003e(*this), tensor); } // aten::masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::masked_fill_(const at::Tensor \u0026 mask, const at::Scalar \u0026 value) const { return at::_ops::masked_fill__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), mask, value); } // aten::masked_fill.Scalar(Tensor self, Tensor mask, Scalar value) -\u003e Tensor inline at::Tensor Tensor::masked_fill(const at::Tensor \u0026 mask, const at::Scalar \u0026 value) const { return at::_ops::masked_fill_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), mask, value); } // aten::masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::masked_fill_(const at::Tensor \u0026 mask, const at::Tensor \u0026 value) const { return at::_ops::masked_fill__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), mask, value); } // aten::masked_fill.Tensor(Tensor self, Tensor mask, Tensor value) -\u003e Tensor inline at::Tensor Tensor::masked_fill(const at::Tensor \u0026 mask, const at::Tensor \u0026 value) const { return at::_ops::masked_fill_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), mask, value); } // aten::masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::masked_scatter_(const at::Tensor \u0026 mask, const at::Tensor \u0026 source) const { return at::_ops::masked_scatter_::call(const_cast\u003cTensor\u0026\u003e(*this), mask, source); } // aten::masked_scatter(Tensor self, Tensor mask, Tensor source) -\u003e Tensor inline at::Tensor Tensor::masked_scatter(const at::Tensor \u0026 mask, const at::Tensor \u0026 source) const { return at::_ops::masked_scatter::call(const_cast\u003cTensor\u0026\u003e(*this), mask, source); } // aten::view(Tensor(a) self, SymInt[] size) -\u003e Tensor(a) inline at::Tensor Tensor::view(at::IntArrayRef size) const { return at::_ops::view::call(const_cast\u003cTensor\u0026\u003e(*this), c10::fromIntArrayRefSlow(size)); } // aten::view(Tensor(a) self, SymInt[] size) -\u003e Tensor(a) inline at::Tensor Tensor::view_symint(c10::SymIntArrayRef size) const { return at::_ops::view::call(const_cast\u003cTensor\u0026\u003e(*this), size); } // aten::view.dtype(Tensor(a) self, ScalarType dtype) -\u003e Tensor(a) inline at::Tensor Tensor::view(at::ScalarType dtype) const { return at::_ops::view_dtype::call(const_cast\u003cTensor\u0026\u003e(*this), dtype); } // aten::put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::put_(const at::Tensor \u0026 index, const at::Tensor \u0026 source, bool accumulate) const { return at::_ops::put_::call(const_cast\u003cTensor\u0026\u003e(*this), index, source, accumulate); } // aten::put(Tensor self, Tensor index, Tensor source, bool accumulate=False) -\u003e Tensor inline at::Tensor Tensor::put(const at::Tensor \u0026 index, const at::Tensor \u0026 source, bool accumulate) const { return at::_ops::put::call(const_cast\u003cTensor\u0026\u003e(*this), index, source, accumulate); } // aten::index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_add_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha) const { return at::_ops::index_add_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source, alpha); } // aten::index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::index_add(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha) const { return at::_ops::index_add::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source, alpha); } // aten::index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::index_add(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, const at::Scalar \u0026 alpha) const { return at::_ops::index_add_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source, alpha); } // aten::index_reduce_(Tensor(a!) self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_reduce_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, c10::string_view reduce, bool include_self) const { return at::_ops::index_reduce_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source, reduce, include_self); } // aten::index_reduce(Tensor self, int dim, Tensor index, Tensor source, str reduce, *, bool include_self=True) -\u003e Tensor inline at::Tensor Tensor::index_reduce(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 source, c10::string_view reduce, bool include_self) const { return at::_ops::index_reduce::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, source, reduce, include_self); } // aten::index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_fill_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::index_fill__int_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -\u003e Tensor inline at::Tensor Tensor::index_fill(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::index_fill_int_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_fill_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const { return at::_ops::index_fill__int_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -\u003e Tensor inline at::Tensor Tensor::index_fill(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const { return at::_ops::index_fill_int_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill_.Dimname_Scalar(Tensor(a!) self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_fill_(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::index_fill__Dimname_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill_.Dimname_Tensor(Tensor(a!) self, Dimname dim, Tensor index, Tensor value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::index_fill_(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const { return at::_ops::index_fill__Dimname_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill.Dimname_Scalar(Tensor self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor inline at::Tensor Tensor::index_fill(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::index_fill_Dimname_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::index_fill.Dimname_Tensor(Tensor self, Dimname dim, Tensor index, Tensor value) -\u003e Tensor inline at::Tensor Tensor::index_fill(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 value) const { return at::_ops::index_fill_Dimname_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::scatter.src(Tensor self, int dim, Tensor index, Tensor src) -\u003e Tensor inline at::Tensor Tensor::scatter(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter_src::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter__src::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter.value(Tensor self, int dim, Tensor index, Scalar value) -\u003e Tensor inline at::Tensor Tensor::scatter(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::scatter_value::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::scatter__value::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::scatter.reduce(Tensor self, int dim, Tensor index, Tensor src, *, str reduce) -\u003e Tensor inline at::Tensor Tensor::scatter(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce) const { return at::_ops::scatter_reduce::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src, reduce); } // aten::scatter_.reduce(Tensor(a!) self, int dim, Tensor index, Tensor src, *, str reduce) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce) const { return at::_ops::scatter__reduce::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src, reduce); } // aten::scatter.value_reduce(Tensor self, int dim, Tensor index, Scalar value, *, str reduce) -\u003e Tensor inline at::Tensor Tensor::scatter(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value, c10::string_view reduce) const { return at::_ops::scatter_value_reduce::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value, reduce); } // aten::scatter_.value_reduce(Tensor(a!) self, int dim, Tensor index, Scalar value, *, str reduce) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_(int64_t dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value, c10::string_view reduce) const { return at::_ops::scatter__value_reduce::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value, reduce); } // aten::scatter.dimname_src(Tensor self, Dimname dim, Tensor index, Tensor src) -\u003e Tensor inline at::Tensor Tensor::scatter(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter_dimname_src::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter.dimname_value(Tensor self, Dimname dim, Tensor index, Scalar value) -\u003e Tensor inline at::Tensor Tensor::scatter(at::Dimname dim, const at::Tensor \u0026 index, const at::Scalar \u0026 value) const { return at::_ops::scatter_dimname_value::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, value); } // aten::scatter_add(Tensor self, int dim, Tensor index, Tensor src) -\u003e Tensor inline at::Tensor Tensor::scatter_add(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter_add::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_add_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter_add_::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -\u003e Tensor inline at::Tensor Tensor::scatter_add(at::Dimname dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src) const { return at::_ops::scatter_add_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src); } // aten::scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -\u003e Tensor inline at::Tensor Tensor::scatter_reduce(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce, bool include_self) const { return at::_ops::scatter_reduce_two::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src, reduce, include_self); } // aten::scatter_reduce_.two(Tensor(a!) self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::scatter_reduce_(int64_t dim, const at::Tensor \u0026 index, const at::Tensor \u0026 src, c10::string_view reduce, bool include_self) const { return at::_ops::scatter_reduce__two::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, src, reduce, include_self); } // aten::eq_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::eq_(const at::Scalar \u0026 other) const { return at::_ops::eq__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::eq_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::eq_(const at::Tensor \u0026 other) const { return at::_ops::eq__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_and.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::bitwise_and(const at::Scalar \u0026 other) const { return at::_ops::bitwise_and_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_and.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::bitwise_and(const at::Tensor \u0026 other) const { return at::_ops::bitwise_and_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_and_(const at::Scalar \u0026 other) const { return at::_ops::bitwise_and__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_and_(const at::Tensor \u0026 other) const { return at::_ops::bitwise_and__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__and__.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::__and__(const at::Scalar \u0026 other) const { return at::_ops::__and___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__and__.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::__and__(const at::Tensor \u0026 other) const { return at::_ops::__and___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__iand__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__iand__(const at::Scalar \u0026 other) const { return at::_ops::__iand___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__iand__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__iand__(const at::Tensor \u0026 other) const { return at::_ops::__iand___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_or.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::bitwise_or(const at::Scalar \u0026 other) const { return at::_ops::bitwise_or_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_or.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::bitwise_or(const at::Tensor \u0026 other) const { return at::_ops::bitwise_or_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_or_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_or_(const at::Scalar \u0026 other) const { return at::_ops::bitwise_or__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_or_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_or_(const at::Tensor \u0026 other) const { return at::_ops::bitwise_or__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__or__.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::__or__(const at::Scalar \u0026 other) const { return at::_ops::__or___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__or__.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::__or__(const at::Tensor \u0026 other) const { return at::_ops::__or___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ior__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ior__(const at::Scalar \u0026 other) const { return at::_ops::__ior___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ior__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ior__(const at::Tensor \u0026 other) const { return at::_ops::__ior___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_xor.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::bitwise_xor(const at::Scalar \u0026 other) const { return at::_ops::bitwise_xor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_xor.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::bitwise_xor(const at::Tensor \u0026 other) const { return at::_ops::bitwise_xor_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_xor_(const at::Scalar \u0026 other) const { return at::_ops::bitwise_xor__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_xor_(const at::Tensor \u0026 other) const { return at::_ops::bitwise_xor__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__xor__.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::__xor__(const at::Scalar \u0026 other) const { return at::_ops::__xor___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__xor__.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::__xor__(const at::Tensor \u0026 other) const { return at::_ops::__xor___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ixor__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ixor__(const at::Scalar \u0026 other) const { return at::_ops::__ixor___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ixor__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ixor__(const at::Tensor \u0026 other) const { return at::_ops::__ixor___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__lshift__.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::__lshift__(const at::Scalar \u0026 other) const { return at::_ops::__lshift___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__lshift__.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::__lshift__(const at::Tensor \u0026 other) const { return at::_ops::__lshift___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ilshift__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ilshift__(const at::Scalar \u0026 other) const { return at::_ops::__ilshift___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__ilshift__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__ilshift__(const at::Tensor \u0026 other) const { return at::_ops::__ilshift___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_left_shift.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::bitwise_left_shift(const at::Tensor \u0026 other) const { return at::_ops::bitwise_left_shift_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_left_shift_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_left_shift_(const at::Tensor \u0026 other) const { return at::_ops::bitwise_left_shift__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_left_shift.Tensor_Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::bitwise_left_shift(const at::Scalar \u0026 other) const { return at::_ops::bitwise_left_shift_Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_left_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_left_shift_(const at::Scalar \u0026 other) const { return at::_ops::bitwise_left_shift__Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__rshift__.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::__rshift__(const at::Scalar \u0026 other) const { return at::_ops::__rshift___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__rshift__.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::__rshift__(const at::Tensor \u0026 other) const { return at::_ops::__rshift___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__irshift__.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__irshift__(const at::Scalar \u0026 other) const { return at::_ops::__irshift___Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::__irshift__.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::__irshift__(const at::Tensor \u0026 other) const { return at::_ops::__irshift___Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_right_shift.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::bitwise_right_shift(const at::Tensor \u0026 other) const { return at::_ops::bitwise_right_shift_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_right_shift_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_right_shift_(const at::Tensor \u0026 other) const { return at::_ops::bitwise_right_shift__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_right_shift.Tensor_Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::bitwise_right_shift(const at::Scalar \u0026 other) const { return at::_ops::bitwise_right_shift_Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::bitwise_right_shift_.Tensor_Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::bitwise_right_shift_(const at::Scalar \u0026 other) const { return at::_ops::bitwise_right_shift__Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::tril_(Tensor(a!) self, SymInt diagonal=0) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::tril_(int64_t diagonal) const { return at::_ops::tril_::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::tril_(Tensor(a!) self, SymInt diagonal=0) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::tril__symint(c10::SymInt diagonal) const { return at::_ops::tril_::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::triu_(Tensor(a!) self, SymInt diagonal=0) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::triu_(int64_t diagonal) const { return at::_ops::triu_::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::triu_(Tensor(a!) self, SymInt diagonal=0) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::triu__symint(c10::SymInt diagonal) const { return at::_ops::triu_::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::digamma_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::digamma_() const { return at::_ops::digamma_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lerp_(const at::Tensor \u0026 end, const at::Scalar \u0026 weight) const { return at::_ops::lerp__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), end, weight); } // aten::lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lerp_(const at::Tensor \u0026 end, const at::Tensor \u0026 weight) const { return at::_ops::lerp__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), end, weight); } // aten::addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addbmm_(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addbmm_::call(const_cast\u003cTensor\u0026\u003e(*this), batch1, batch2, beta, alpha); } // aten::addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -\u003e Tensor inline at::Tensor Tensor::addbmm(const at::Tensor \u0026 batch1, const at::Tensor \u0026 batch2, const at::Scalar \u0026 beta, const at::Scalar \u0026 alpha) const { return at::_ops::addbmm::call(const_cast\u003cTensor\u0026\u003e(*this), batch1, batch2, beta, alpha); } // aten::random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::random_(int64_t from, ::std::optional\u003cint64_t\u003e to, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::random__from::call(const_cast\u003cTensor\u0026\u003e(*this), from, to, generator); } // aten::random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::random_(int64_t to, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::random__to::call(const_cast\u003cTensor\u0026\u003e(*this), to, generator); } // aten::random_(Tensor(a!) self, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::random_(::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::random_::call(const_cast\u003cTensor\u0026\u003e(*this), generator); } // aten::uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::uniform_(double from, double to, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::uniform_::call(const_cast\u003cTensor\u0026\u003e(*this), from, to, generator); } // aten::cauchy_(Tensor(a!) self, float median=0, float sigma=1, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::cauchy_(double median, double sigma, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::cauchy_::call(const_cast\u003cTensor\u0026\u003e(*this), median, sigma, generator); } // aten::log_normal_(Tensor(a!) self, float mean=1, float std=2, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::log_normal_(double mean, double std, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::log_normal_::call(const_cast\u003cTensor\u0026\u003e(*this), mean, std, generator); } // aten::exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::exponential_(double lambd, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::exponential_::call(const_cast\u003cTensor\u0026\u003e(*this), lambd, generator); } // aten::geometric_(Tensor(a!) self, float p, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::geometric_(double p, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::geometric_::call(const_cast\u003cTensor\u0026\u003e(*this), p, generator); } // aten::diag(Tensor self, int diagonal=0) -\u003e Tensor inline at::Tensor Tensor::diag(int64_t diagonal) const { return at::_ops::diag::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::cross(Tensor self, Tensor other, int? dim=None) -\u003e Tensor inline at::Tensor Tensor::cross(const at::Tensor \u0026 other, ::std::optional\u003cint64_t\u003e dim) const { return at::_ops::cross::call(const_cast\u003cTensor\u0026\u003e(*this), other, dim); } // aten::triu(Tensor self, SymInt diagonal=0) -\u003e Tensor inline at::Tensor Tensor::triu(int64_t diagonal) const { return at::_ops::triu::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::triu(Tensor self, SymInt diagonal=0) -\u003e Tensor inline at::Tensor Tensor::triu_symint(c10::SymInt diagonal) const { return at::_ops::triu::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::tril(Tensor self, SymInt diagonal=0) -\u003e Tensor inline at::Tensor Tensor::tril(int64_t diagonal) const { return at::_ops::tril::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::tril(Tensor self, SymInt diagonal=0) -\u003e Tensor inline at::Tensor Tensor::tril_symint(c10::SymInt diagonal) const { return at::_ops::tril::call(const_cast\u003cTensor\u0026\u003e(*this), diagonal); } // aten::trace(Tensor self) -\u003e Tensor inline at::Tensor Tensor::trace() const { return at::_ops::trace::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::ne.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::ne(const at::Scalar \u0026 other) const { return at::_ops::ne_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ne.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::ne(const at::Tensor \u0026 other) const { return at::_ops::ne_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ne_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ne_(const at::Scalar \u0026 other) const { return at::_ops::ne__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ne_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ne_(const at::Tensor \u0026 other) const { return at::_ops::ne__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::not_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::not_equal(const at::Scalar \u0026 other) const { return at::_ops::not_equal_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::not_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::not_equal(const at::Tensor \u0026 other) const { return at::_ops::not_equal_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::not_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::not_equal_(const at::Scalar \u0026 other) const { return at::_ops::not_equal__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::not_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::not_equal_(const at::Tensor \u0026 other) const { return at::_ops::not_equal__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::eq.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::eq(const at::Scalar \u0026 other) const { return at::_ops::eq_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::eq.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::eq(const at::Tensor \u0026 other) const { return at::_ops::eq_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ge.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::ge(const at::Scalar \u0026 other) const { return at::_ops::ge_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ge.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::ge(const at::Tensor \u0026 other) const { return at::_ops::ge_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ge_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ge_(const at::Scalar \u0026 other) const { return at::_ops::ge__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::ge_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::ge_(const at::Tensor \u0026 other) const { return at::_ops::ge__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::greater_equal(const at::Scalar \u0026 other) const { return at::_ops::greater_equal_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::greater_equal(const at::Tensor \u0026 other) const { return at::_ops::greater_equal_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::greater_equal_(const at::Scalar \u0026 other) const { return at::_ops::greater_equal__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::greater_equal_(const at::Tensor \u0026 other) const { return at::_ops::greater_equal__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::le.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::le(const at::Scalar \u0026 other) const { return at::_ops::le_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::le.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::le(const at::Tensor \u0026 other) const { return at::_ops::le_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::le_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::le_(const at::Scalar \u0026 other) const { return at::_ops::le__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::le_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::le_(const at::Tensor \u0026 other) const { return at::_ops::le__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_equal.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::less_equal(const at::Scalar \u0026 other) const { return at::_ops::less_equal_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_equal.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::less_equal(const at::Tensor \u0026 other) const { return at::_ops::less_equal_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_equal_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::less_equal_(const at::Scalar \u0026 other) const { return at::_ops::less_equal__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_equal_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::less_equal_(const at::Tensor \u0026 other) const { return at::_ops::less_equal__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::gt.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::gt(const at::Scalar \u0026 other) const { return at::_ops::gt_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::gt.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::gt(const at::Tensor \u0026 other) const { return at::_ops::gt_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::gt_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::gt_(const at::Scalar \u0026 other) const { return at::_ops::gt__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::gt_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::gt_(const at::Tensor \u0026 other) const { return at::_ops::gt__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::greater(const at::Scalar \u0026 other) const { return at::_ops::greater_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::greater(const at::Tensor \u0026 other) const { return at::_ops::greater_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::greater_(const at::Scalar \u0026 other) const { return at::_ops::greater__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::greater_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::greater_(const at::Tensor \u0026 other) const { return at::_ops::greater__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lt.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::lt(const at::Scalar \u0026 other) const { return at::_ops::lt_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lt.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::lt(const at::Tensor \u0026 other) const { return at::_ops::lt_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lt_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lt_(const at::Scalar \u0026 other) const { return at::_ops::lt__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lt_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lt_(const at::Tensor \u0026 other) const { return at::_ops::lt__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::less(const at::Scalar \u0026 other) const { return at::_ops::less_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::less(const at::Tensor \u0026 other) const { return at::_ops::less_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::less_(const at::Scalar \u0026 other) const { return at::_ops::less__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::less_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::less_(const at::Tensor \u0026 other) const { return at::_ops::less__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::take(Tensor self, Tensor index) -\u003e Tensor inline at::Tensor Tensor::take(const at::Tensor \u0026 index) const { return at::_ops::take::call(const_cast\u003cTensor\u0026\u003e(*this), index); } // aten::take_along_dim(Tensor self, Tensor indices, int? dim=None) -\u003e Tensor inline at::Tensor Tensor::take_along_dim(const at::Tensor \u0026 indices, ::std::optional\u003cint64_t\u003e dim) const { return at::_ops::take_along_dim::call(const_cast\u003cTensor\u0026\u003e(*this), indices, dim); } // aten::index_select(Tensor self, int dim, Tensor index) -\u003e Tensor inline at::Tensor Tensor::index_select(int64_t dim, const at::Tensor \u0026 index) const { return at::_ops::index_select::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index); } // aten::index_select.dimname(Tensor self, Dimname dim, Tensor index) -\u003e Tensor inline at::Tensor Tensor::index_select(at::Dimname dim, const at::Tensor \u0026 index) const { return at::_ops::index_select_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index); } // aten::masked_select(Tensor self, Tensor mask) -\u003e Tensor inline at::Tensor Tensor::masked_select(const at::Tensor \u0026 mask) const { return at::_ops::masked_select::call(const_cast\u003cTensor\u0026\u003e(*this), mask); } // aten::nonzero(Tensor self) -\u003e Tensor inline at::Tensor Tensor::nonzero() const { return at::_ops::nonzero::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -\u003e Tensor inline at::Tensor Tensor::nonzero_static(int64_t size, int64_t fill_value) const { return at::_ops::nonzero_static::call(const_cast\u003cTensor\u0026\u003e(*this), size, fill_value); } // aten::nonzero_static(Tensor self, *, SymInt size, int fill_value=-1) -\u003e Tensor inline at::Tensor Tensor::nonzero_static_symint(c10::SymInt size, int64_t fill_value) const { return at::_ops::nonzero_static::call(const_cast\u003cTensor\u0026\u003e(*this), size, fill_value); } // aten::nonzero_numpy(Tensor self) -\u003e Tensor[] inline ::std::vector\u003cat::Tensor\u003e Tensor::nonzero_numpy() const { return at::_ops::nonzero_numpy::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::argwhere(Tensor self) -\u003e Tensor inline at::Tensor Tensor::argwhere() const { return at::_ops::argwhere::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -\u003e Tensor inline at::Tensor Tensor::gather(int64_t dim, const at::Tensor \u0026 index, bool sparse_grad) const { return at::_ops::gather::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, sparse_grad); } // aten::gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -\u003e Tensor inline at::Tensor Tensor::gather(at::Dimname dim, const at::Tensor \u0026 index, bool sparse_grad) const { return at::_ops::gather_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, index, sparse_grad); } // aten::addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor inline at::Tensor Tensor::addcmul(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value) const { return at::_ops::addcmul::call(const_cast\u003cTensor\u0026\u003e(*this), tensor1, tensor2, value); } // aten::addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addcmul_(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value) const { return at::_ops::addcmul_::call(const_cast\u003cTensor\u0026\u003e(*this), tensor1, tensor2, value); } // aten::addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor inline at::Tensor Tensor::addcdiv(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value) const { return at::_ops::addcdiv::call(const_cast\u003cTensor\u0026\u003e(*this), tensor1, tensor2, value); } // aten::addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::addcdiv_(const at::Tensor \u0026 tensor1, const at::Tensor \u0026 tensor2, const at::Scalar \u0026 value) const { return at::_ops::addcdiv_::call(const_cast\u003cTensor\u0026\u003e(*this), tensor1, tensor2, value); } // aten::triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -\u003e (Tensor solution, Tensor cloned_coefficient) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::triangular_solve(const at::Tensor \u0026 A, bool upper, bool transpose, bool unitriangular) const { return at::_ops::triangular_solve::call(const_cast\u003cTensor\u0026\u003e(*this), A, upper, transpose, unitriangular); } // aten::svd(Tensor self, bool some=True, bool compute_uv=True) -\u003e (Tensor U, Tensor S, Tensor V) inline ::std::tuple\u003cat::Tensor,at::Tensor,at::Tensor\u003e Tensor::svd(bool some, bool compute_uv) const { return at::_ops::svd::call(const_cast\u003cTensor\u0026\u003e(*this), some, compute_uv); } // aten::swapaxes(Tensor(a) self, int axis0, int axis1) -\u003e Tensor(a) inline at::Tensor Tensor::swapaxes(int64_t axis0, int64_t axis1) const { return at::_ops::swapaxes::call(const_cast\u003cTensor\u0026\u003e(*this), axis0, axis1); } // aten::swapaxes_(Tensor(a!) self, int axis0, int axis1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::swapaxes_(int64_t axis0, int64_t axis1) const { return at::_ops::swapaxes_::call(const_cast\u003cTensor\u0026\u003e(*this), axis0, axis1); } // aten::swapdims(Tensor(a) self, int dim0, int dim1) -\u003e Tensor(a) inline at::Tensor Tensor::swapdims(int64_t dim0, int64_t dim1) const { return at::_ops::swapdims::call(const_cast\u003cTensor\u0026\u003e(*this), dim0, dim1); } // aten::swapdims_(Tensor(a!) self, int dim0, int dim1) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::swapdims_(int64_t dim0, int64_t dim1) const { return at::_ops::swapdims_::call(const_cast\u003cTensor\u0026\u003e(*this), dim0, dim1); } // aten::cholesky(Tensor self, bool upper=False) -\u003e Tensor inline at::Tensor Tensor::cholesky(bool upper) const { return at::_ops::cholesky::call(const_cast\u003cTensor\u0026\u003e(*this), upper); } // aten::cholesky_solve(Tensor self, Tensor input2, bool upper=False) -\u003e Tensor inline at::Tensor Tensor::cholesky_solve(const at::Tensor \u0026 input2, bool upper) const { return at::_ops::cholesky_solve::call(const_cast\u003cTensor\u0026\u003e(*this), input2, upper); } // aten::cholesky_inverse(Tensor self, bool upper=False) -\u003e Tensor inline at::Tensor Tensor::cholesky_inverse(bool upper) const { return at::_ops::cholesky_inverse::call(const_cast\u003cTensor\u0026\u003e(*this), upper); } // aten::qr(Tensor self, bool some=True) -\u003e (Tensor Q, Tensor R) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::qr(bool some) const { return at::_ops::qr::call(const_cast\u003cTensor\u0026\u003e(*this), some); } // aten::geqrf(Tensor self) -\u003e (Tensor a, Tensor tau) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::geqrf() const { return at::_ops::geqrf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::orgqr(Tensor self, Tensor input2) -\u003e Tensor inline at::Tensor Tensor::orgqr(const at::Tensor \u0026 input2) const { return at::_ops::orgqr::call(const_cast\u003cTensor\u0026\u003e(*this), input2); } // aten::ormqr(Tensor self, Tensor input2, Tensor input3, bool left=True, bool transpose=False) -\u003e Tensor inline at::Tensor Tensor::ormqr(const at::Tensor \u0026 input2, const at::Tensor \u0026 input3, bool left, bool transpose) const { return at::_ops::ormqr::call(const_cast\u003cTensor\u0026\u003e(*this), input2, input3, left, transpose); } // aten::lu_solve(Tensor self, Tensor LU_data, Tensor LU_pivots) -\u003e Tensor inline at::Tensor Tensor::lu_solve(const at::Tensor \u0026 LU_data, const at::Tensor \u0026 LU_pivots) const { return at::_ops::lu_solve::call(const_cast\u003cTensor\u0026\u003e(*this), LU_data, LU_pivots); } // aten::multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -\u003e Tensor inline at::Tensor Tensor::multinomial(int64_t num_samples, bool replacement, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::multinomial::call(const_cast\u003cTensor\u0026\u003e(*this), num_samples, replacement, generator); } // aten::multinomial(Tensor self, SymInt num_samples, bool replacement=False, *, Generator? generator=None) -\u003e Tensor inline at::Tensor Tensor::multinomial_symint(c10::SymInt num_samples, bool replacement, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::multinomial::call(const_cast\u003cTensor\u0026\u003e(*this), num_samples, replacement, generator); } // aten::lgamma_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::lgamma_() const { return at::_ops::lgamma_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::lgamma(Tensor self) -\u003e Tensor inline at::Tensor Tensor::lgamma() const { return at::_ops::lgamma::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::digamma(Tensor self) -\u003e Tensor inline at::Tensor Tensor::digamma() const { return at::_ops::digamma::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::polygamma(int n, Tensor self) -\u003e Tensor inline at::Tensor Tensor::polygamma(int64_t n) const { return at::_ops::polygamma::call(n, const_cast\u003cTensor\u0026\u003e(*this)); } // aten::polygamma_(Tensor(a!) self, int n) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::polygamma_(int64_t n) const { return at::_ops::polygamma_::call(const_cast\u003cTensor\u0026\u003e(*this), n); } // aten::erfinv(Tensor self) -\u003e Tensor inline at::Tensor Tensor::erfinv() const { return at::_ops::erfinv::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::erfinv_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::erfinv_() const { return at::_ops::erfinv_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::i0(Tensor self) -\u003e Tensor inline at::Tensor Tensor::i0() const { return at::_ops::i0::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::i0_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::i0_() const { return at::_ops::i0_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sign(Tensor self) -\u003e Tensor inline at::Tensor Tensor::sign() const { return at::_ops::sign::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::sign_(Tensor(a!) self) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::sign_() const { return at::_ops::sign_::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::signbit(Tensor self) -\u003e Tensor inline at::Tensor Tensor::signbit() const { return at::_ops::signbit::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::dist(Tensor self, Tensor other, Scalar p=2) -\u003e Tensor inline at::Tensor Tensor::dist(const at::Tensor \u0026 other, const at::Scalar \u0026 p) const { return at::_ops::dist::call(const_cast\u003cTensor\u0026\u003e(*this), other, p); } // aten::atan2_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::atan2_(const at::Tensor \u0026 other) const { return at::_ops::atan2_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::atan2(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::atan2(const at::Tensor \u0026 other) const { return at::_ops::atan2::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::arctan2(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::arctan2(const at::Tensor \u0026 other) const { return at::_ops::arctan2::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::arctan2_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::arctan2_(const at::Tensor \u0026 other) const { return at::_ops::arctan2_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::lerp.Scalar(Tensor self, Tensor end, Scalar weight) -\u003e Tensor inline at::Tensor Tensor::lerp(const at::Tensor \u0026 end, const at::Scalar \u0026 weight) const { return at::_ops::lerp_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), end, weight); } // aten::lerp.Tensor(Tensor self, Tensor end, Tensor weight) -\u003e Tensor inline at::Tensor Tensor::lerp(const at::Tensor \u0026 end, const at::Tensor \u0026 weight) const { return at::_ops::lerp_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), end, weight); } // aten::histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -\u003e Tensor inline at::Tensor Tensor::histc(int64_t bins, const at::Scalar \u0026 min, const at::Scalar \u0026 max) const { return at::_ops::histc::call(const_cast\u003cTensor\u0026\u003e(*this), bins, min, max); } // aten::histogram.bins_tensor(Tensor self, Tensor bins, *, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor bin_edges) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::histogram(const at::Tensor \u0026 bins, const ::std::optional\u003cat::Tensor\u003e \u0026 weight, bool density) const { return at::_ops::histogram_bins_tensor::call(const_cast\u003cTensor\u0026\u003e(*this), bins, weight, density); } // aten::histogram.bin_ct(Tensor self, int bins=100, *, float[]? range=None, Tensor? weight=None, bool density=False) -\u003e (Tensor hist, Tensor bin_edges) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::histogram(int64_t bins, ::std::optional\u003cat::ArrayRef\u003cdouble\u003e\u003e range, const ::std::optional\u003cat::Tensor\u003e \u0026 weight, bool density) const { return at::_ops::histogram_bin_ct::call(const_cast\u003cTensor\u0026\u003e(*this), bins, range, weight, density); } // aten::fmod.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::fmod(const at::Scalar \u0026 other) const { return at::_ops::fmod_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::fmod_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fmod_(const at::Scalar \u0026 other) const { return at::_ops::fmod__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::fmod.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::fmod(const at::Tensor \u0026 other) const { return at::_ops::fmod_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::fmod_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::fmod_(const at::Tensor \u0026 other) const { return at::_ops::fmod__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::hypot(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::hypot(const at::Tensor \u0026 other) const { return at::_ops::hypot::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::hypot_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::hypot_(const at::Tensor \u0026 other) const { return at::_ops::hypot_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::igamma(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::igamma(const at::Tensor \u0026 other) const { return at::_ops::igamma::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::igamma_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::igamma_(const at::Tensor \u0026 other) const { return at::_ops::igamma_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::igammac(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::igammac(const at::Tensor \u0026 other) const { return at::_ops::igammac::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::igammac_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::igammac_(const at::Tensor \u0026 other) const { return at::_ops::igammac_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::nextafter(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::nextafter(const at::Tensor \u0026 other) const { return at::_ops::nextafter::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::nextafter_(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::nextafter_(const at::Tensor \u0026 other) const { return at::_ops::nextafter_::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::remainder.Scalar(Tensor self, Scalar other) -\u003e Tensor inline at::Tensor Tensor::remainder(const at::Scalar \u0026 other) const { return at::_ops::remainder_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::remainder_.Scalar(Tensor(a!) self, Scalar other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::remainder_(const at::Scalar \u0026 other) const { return at::_ops::remainder__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::remainder.Tensor(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::remainder(const at::Tensor \u0026 other) const { return at::_ops::remainder_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::remainder_.Tensor(Tensor(a!) self, Tensor other) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::remainder_(const at::Tensor \u0026 other) const { return at::_ops::remainder__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::min(Tensor self) -\u003e Tensor inline at::Tensor Tensor::min() const { return at::_ops::min::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::fmin(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::fmin(const at::Tensor \u0026 other) const { return at::_ops::fmin::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::max(Tensor self) -\u003e Tensor inline at::Tensor Tensor::max() const { return at::_ops::max::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::fmax(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::fmax(const at::Tensor \u0026 other) const { return at::_ops::fmax::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::maximum(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::maximum(const at::Tensor \u0026 other) const { return at::_ops::maximum::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::max.other(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::max(const at::Tensor \u0026 other) const { return at::_ops::max_other::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::minimum(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::minimum(const at::Tensor \u0026 other) const { return at::_ops::minimum::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::min.other(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::min(const at::Tensor \u0026 other) const { return at::_ops::min_other::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::quantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=\u0027linear\u0027) -\u003e Tensor inline at::Tensor Tensor::quantile(const at::Tensor \u0026 q, ::std::optional\u003cint64_t\u003e dim, bool keepdim, c10::string_view interpolation) const { return at::_ops::quantile::call(const_cast\u003cTensor\u0026\u003e(*this), q, dim, keepdim, interpolation); } // aten::quantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=\u0027linear\u0027) -\u003e Tensor inline at::Tensor Tensor::quantile(double q, ::std::optional\u003cint64_t\u003e dim, bool keepdim, c10::string_view interpolation) const { return at::_ops::quantile_scalar::call(const_cast\u003cTensor\u0026\u003e(*this), q, dim, keepdim, interpolation); } // aten::nanquantile(Tensor self, Tensor q, int? dim=None, bool keepdim=False, *, str interpolation=\u0027linear\u0027) -\u003e Tensor inline at::Tensor Tensor::nanquantile(const at::Tensor \u0026 q, ::std::optional\u003cint64_t\u003e dim, bool keepdim, c10::string_view interpolation) const { return at::_ops::nanquantile::call(const_cast\u003cTensor\u0026\u003e(*this), q, dim, keepdim, interpolation); } // aten::nanquantile.scalar(Tensor self, float q, int? dim=None, bool keepdim=False, *, str interpolation=\u0027linear\u0027) -\u003e Tensor inline at::Tensor Tensor::nanquantile(double q, ::std::optional\u003cint64_t\u003e dim, bool keepdim, c10::string_view interpolation) const { return at::_ops::nanquantile_scalar::call(const_cast\u003cTensor\u0026\u003e(*this), q, dim, keepdim, interpolation); } // aten::sort(Tensor self, int dim=-1, bool descending=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::sort(int64_t dim, bool descending) const { return at::_ops::sort::call(const_cast\u003cTensor\u0026\u003e(*this), dim, descending); } // aten::sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::sort(::std::optional\u003cbool\u003e stable, int64_t dim, bool descending) const { return at::_ops::sort_stable::call(const_cast\u003cTensor\u0026\u003e(*this), stable, dim, descending); } // aten::sort.dimname(Tensor self, Dimname dim, bool descending=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::sort(at::Dimname dim, bool descending) const { return at::_ops::sort_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, descending); } // aten::sort.dimname_stable(Tensor self, *, bool? stable, Dimname dim, bool descending=False) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::sort(::std::optional\u003cbool\u003e stable, at::Dimname dim, bool descending) const { return at::_ops::sort_dimname_stable::call(const_cast\u003cTensor\u0026\u003e(*this), stable, dim, descending); } // aten::msort(Tensor self) -\u003e Tensor inline at::Tensor Tensor::msort() const { return at::_ops::msort::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::argsort(Tensor self, int dim=-1, bool descending=False) -\u003e Tensor inline at::Tensor Tensor::argsort(int64_t dim, bool descending) const { return at::_ops::argsort::call(const_cast\u003cTensor\u0026\u003e(*this), dim, descending); } // aten::argsort.stable(Tensor self, *, bool stable, int dim=-1, bool descending=False) -\u003e Tensor inline at::Tensor Tensor::argsort(bool stable, int64_t dim, bool descending) const { return at::_ops::argsort_stable::call(const_cast\u003cTensor\u0026\u003e(*this), stable, dim, descending); } // aten::argsort.dimname(Tensor self, Dimname dim, bool descending=False) -\u003e Tensor inline at::Tensor Tensor::argsort(at::Dimname dim, bool descending) const { return at::_ops::argsort_dimname::call(const_cast\u003cTensor\u0026\u003e(*this), dim, descending); } // aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::topk(int64_t k, int64_t dim, bool largest, bool sorted) const { return at::_ops::topk::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, largest, sorted); } // aten::topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -\u003e (Tensor values, Tensor indices) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::topk_symint(c10::SymInt k, int64_t dim, bool largest, bool sorted) const { return at::_ops::topk::call(const_cast\u003cTensor\u0026\u003e(*this), k, dim, largest, sorted); } // aten::all(Tensor self) -\u003e Tensor inline at::Tensor Tensor::all() const { return at::_ops::all::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::any(Tensor self) -\u003e Tensor inline at::Tensor Tensor::any() const { return at::_ops::any::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -\u003e Tensor inline at::Tensor Tensor::renorm(const at::Scalar \u0026 p, int64_t dim, const at::Scalar \u0026 maxnorm) const { return at::_ops::renorm::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, maxnorm); } // aten::renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::renorm_(const at::Scalar \u0026 p, int64_t dim, const at::Scalar \u0026 maxnorm) const { return at::_ops::renorm_::call(const_cast\u003cTensor\u0026\u003e(*this), p, dim, maxnorm); } // aten::unfold(Tensor(a) self, int dimension, int size, int step) -\u003e Tensor(a) inline at::Tensor Tensor::unfold(int64_t dimension, int64_t size, int64_t step) const { return at::_ops::unfold::call(const_cast\u003cTensor\u0026\u003e(*this), dimension, size, step); } // aten::equal(Tensor self, Tensor other) -\u003e bool inline bool Tensor::equal(const at::Tensor \u0026 other) const { return at::_ops::equal::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::pow.Tensor_Tensor(Tensor self, Tensor exponent) -\u003e Tensor inline at::Tensor Tensor::pow(const at::Tensor \u0026 exponent) const { return at::_ops::pow_Tensor_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::pow.Tensor_Scalar(Tensor self, Scalar exponent) -\u003e Tensor inline at::Tensor Tensor::pow(const at::Scalar \u0026 exponent) const { return at::_ops::pow_Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::pow_.Scalar(Tensor(a!) self, Scalar exponent) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::pow_(const at::Scalar \u0026 exponent) const { return at::_ops::pow__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::pow_.Tensor(Tensor(a!) self, Tensor exponent) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::pow_(const at::Tensor \u0026 exponent) const { return at::_ops::pow__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::float_power.Tensor_Tensor(Tensor self, Tensor exponent) -\u003e Tensor inline at::Tensor Tensor::float_power(const at::Tensor \u0026 exponent) const { return at::_ops::float_power_Tensor_Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::float_power.Tensor_Scalar(Tensor self, Scalar exponent) -\u003e Tensor inline at::Tensor Tensor::float_power(const at::Scalar \u0026 exponent) const { return at::_ops::float_power_Tensor_Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::float_power_.Scalar(Tensor(a!) self, Scalar exponent) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::float_power_(const at::Scalar \u0026 exponent) const { return at::_ops::float_power__Scalar::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::float_power_.Tensor(Tensor(a!) self, Tensor exponent) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::float_power_(const at::Tensor \u0026 exponent) const { return at::_ops::float_power__Tensor::call(const_cast\u003cTensor\u0026\u003e(*this), exponent); } // aten::normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -\u003e Tensor(a!) inline at::Tensor \u0026 Tensor::normal_(double mean, double std, ::std::optional\u003cat::Generator\u003e generator) const { return at::_ops::normal_::call(const_cast\u003cTensor\u0026\u003e(*this), mean, std, generator); } // aten::alias(Tensor(a) self) -\u003e Tensor(a) inline at::Tensor Tensor::alias() const { return at::_ops::alias::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::isfinite(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isfinite() const { return at::_ops::isfinite::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::isinf(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isinf() const { return at::_ops::isinf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::record_stream(Tensor(a!) self, Stream s) -\u003e () inline void Tensor::record_stream(at::Stream s) const { return at::_ops::record_stream::call(const_cast\u003cTensor\u0026\u003e(*this), s); } // aten::isposinf(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isposinf() const { return at::_ops::isposinf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::isneginf(Tensor self) -\u003e Tensor inline at::Tensor Tensor::isneginf() const { return at::_ops::isneginf::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::det(Tensor self) -\u003e Tensor inline at::Tensor Tensor::det() const { return at::_ops::det::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::slogdet(Tensor self) -\u003e (Tensor sign, Tensor logabsdet) inline ::std::tuple\u003cat::Tensor,at::Tensor\u003e Tensor::slogdet() const { return at::_ops::slogdet::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::logdet(Tensor self) -\u003e Tensor inline at::Tensor Tensor::logdet() const { return at::_ops::logdet::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::inverse(Tensor self) -\u003e Tensor inline at::Tensor Tensor::inverse() const { return at::_ops::inverse::call(const_cast\u003cTensor\u0026\u003e(*this)); } // aten::inner(Tensor self, Tensor other) -\u003e Tensor inline at::Tensor Tensor::inner(const at::Tensor \u0026 other) const { return at::_ops::inner::call(const_cast\u003cTensor\u0026\u003e(*this), other); } // aten::outer(Tensor self, Tensor vec2) -\u003e Tensor inline at::Tensor Tensor::outer(const at::Tensor \u0026 vec2) const { return at::_ops::outer::call(const_cast\u003cTensor\u0026\u003e(*this), vec2); } // aten::ger(Tensor self, Tensor vec2) -\u003e Tensor inline at::Tensor Tensor::ger(const at::Tensor \u0026 vec2) const { return at::_ops::ger::call(const_cast\u003cTensor\u0026\u003e(*this), vec2); } // aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -\u003e Tensor inline at::Tensor Tensor::to_padded_tensor(double padding, at::OptionalIntArrayRef output_size) const { return at::_ops::to_padded_tensor::call(const_cast\u003cTensor\u0026\u003e(*this), padding, output_size.has_value() ? ::std::make_optional(c10::fromIntArrayRefSlow(*output_size)) : ::std::nullopt); } // aten::to_padded_tensor(Tensor self, float padding, SymInt[]? output_size=None) -\u003e Tensor inline at::Tensor Tensor::to_padded_tensor_symint(double padding, at::OptionalSymIntArrayRef output_size) const { return at::_ops::to_padded_tensor::call(const_cast\u003cTensor\u0026\u003e(*this), padding, output_size); } } // namespace at namespace c10 { template \u003c\u003e struct MaybeOwnedTraits\u003cat::Tensor\u003e { using owned_type = at::Tensor; using borrow_type = at::Tensor; static borrow_type createBorrow(const owned_type\u0026 from) { // NOTE: this can be implemented without the special // unsafe_borrow_t Tensor constructor as // // return borrow_type(c10::intrusive_ptr\u003cat::TensorImpl, at::UndefinedTensorImpl\u003e::reclaim(from.unsafeGetTensorImpl())); // // but that hurts inlining due to the nullptr check in the // Tensor(c10::intrusive_ptr\u003c...\u003e) constructor. We already know // that from.impl_ isn\u0027t null because from is a valid Tensor, so // we needn\u0027t do the check again. (using __builtin_assume can // avoid this, but wouldn\u0027t be portable to MSVC.) return borrow_type(borrow_type::unsafe_borrow_t{}, from); } static void assignBorrow(borrow_type\u0026 lhs, const borrow_type\u0026 rhs) { lhs.unsafeReleaseTensorImpl(); // See above note: this can be implemented with public API // similarly to createBorrow(), but that would hurt inlining. lhs = borrow_type(borrow_type::unsafe_borrow_t{}, rhs); } static void destroyBorrow(borrow_type\u0026 toDestroy) { toDestroy.unsafeReleaseTensorImpl(); // \"leak\" it, but it was already +0. } static const owned_type\u0026 referenceFromBorrow(const borrow_type\u0026 borrow) { return borrow; } static const owned_type* pointerFromBorrow(const borrow_type\u0026 borrow) { return \u0026borrow; } static bool debugBorrowIsValid(const borrow_type\u0026 /*borrow*/) { return true; } }; template \u003c\u003e struct ExclusivelyOwnedTraits\u003cat::Tensor\u003e { using repr_type = at::Tensor; using pointer_type = at::Tensor*; using const_pointer_type = const at::Tensor*; static repr_type nullRepr() { return at::Tensor(); } template \u003cclass... Args\u003e static repr_type createInPlace(Args\u0026\u0026... args) { return at::Tensor(std::forward\u003cArgs\u003e(args)...); } static repr_type moveToRepr(at::Tensor\u0026\u0026 x) { return std::move(x); } static void destroyOwned(at::Tensor\u0026 x) { return ExclusivelyOwnedTraits\u003cat::TensorBase\u003e::destroyOwned(x); } static at::Tensor take(at::Tensor\u0026 x) { return std::move(x); } static pointer_type getImpl(repr_type\u0026 x) { return \u0026x; } static const_pointer_type getImpl(const repr_type\u0026 x) { return \u0026x; } }; } // namespace c10 namespace at { inline c10::MaybeOwned\u003cTensor\u003e borrow_from_optional_tensor( const std::optional\u003cTensor\u003e\u0026 opt) { return opt.has_value() ? c10::MaybeOwned\u003cTensor\u003e::borrowed(*opt) : c10::MaybeOwned\u003cTensor\u003e::owned(std::in_place); } inline c10::MaybeOwned\u003cTensor\u003e Tensor::expect_contiguous(MemoryFormat memory_format) const \u0026 { if (is_contiguous(memory_format)) { return c10::MaybeOwned\u003cTensor\u003e::borrowed(*this); } else { return c10::MaybeOwned\u003cTensor\u003e::owned(__dispatch_contiguous(memory_format)); } } } // namespace at",
       "author": {
         "@type": "Organization",
         "name": "PyTorch Contributors",
         "url": "https://pytorch.org"
       },
       "image": "https://pytorch.org/docs/stable/_static/img/pytorch_seo.png",
       "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "/api/program_listing_file_build_aten_src_ATen_core_TensorBody.h.html"
       },
       "datePublished": "2023-01-01T00:00:00Z",
       "dateModified": "2023-01-01T00:00:00Z"
     }
 </script>
  <script>
    // Tutorials Call to action event tracking
    $("[data-behavior='call-to-action-event']").on('click', function () {
      fbq('trackCustom', "Download", {
        tutorialTitle: $('h1:first').text(),
        downloadLink: this.href,
        tutorialLink: window.location.href,
        downloadTitle: $(this).attr("data-response")
      });
      if (typeof gtag === 'function') {
        gtag('event', 'click', {
          'event_category': $(this).attr("data-response"),
          'event_label': $("h1").first().text(),
          'tutorial_link': window.location.href
        });
      }
    });
  </script>
  
  </body>
</html>



<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File variable.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_torch_csrc_autograd_variable.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/cpp_theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active docs-active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torcharrow">
                  <span class="dropdown-title">torcharrow</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/data">
                  <span class="dropdown-title">TorchData</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchrec">
                  <span class="dropdown-title">TorchRec</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/torchx/">
                  <span class="dropdown-title">TorchX</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn about the PyTorch foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">GitHub</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/inference_mode.html">Inference Mode</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/maybe_owned.html">MaybeOwned&lt;Tensor&gt;</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_cuda_stream.html">Tensor CUDA Stream API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_indexing.html">Tensor Indexing API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/versioning.html">Library Versioning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Program Listing for File variable.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-variable-h">
<span id="program-listing-file-torch-csrc-autograd-variable-h"></span><h1>Program Listing for File variable.h<a class="headerlink" href="#program-listing-for-file-variable-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_autograd_variable.h.html#file-torch-csrc-autograd-variable-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/variable.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/utils/python_stub.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/Export.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/cpp_hook.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/edge.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/forward_grad.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/csrc/autograd/function_hook.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/NamedTensorUtils.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/core/Tensor.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;c10/util/Exception.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdint&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;mutex&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdexcept&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;string&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;utility&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span><span class="cp"></span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span><span class="w"></span>

<span class="k">using</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span><span class="w"></span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace autograd</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch</span>

<span class="c1">// The following are all internal APIs and should not be shown in libtorch docs.</span>
<span class="c1">// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS</span>
<span class="c1">// ... #endif`</span>

<span class="cp">#ifndef DOXYGEN_SHOULD_SKIP_THIS</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">autograd</span><span class="w"> </span><span class="p">{</span><span class="w"></span>

<span class="k">static</span><span class="w"> </span><span class="kr">inline</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="nf">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">isFloatingType</span><span class="p">(</span><span class="n">t</span><span class="p">)</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">isComplexType</span><span class="p">(</span><span class="n">t</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">struct</span><span class="w"> </span><span class="nc">Node</span><span class="p">;</span><span class="w"></span>


<span class="k">struct</span><span class="w"> </span><span class="nc">AutogradMeta</span><span class="p">;</span><span class="w"></span>
<span class="k">struct</span><span class="w"> </span><span class="nc">DifferentiableViewMeta</span><span class="p">;</span><span class="w"></span>

<span class="c1">// Private-ish functions for manipulating variables; we don&#39;t want to put them</span>
<span class="c1">// on Tensor proper</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">impl</span><span class="w"> </span><span class="p">{</span><span class="w"></span>

<span class="c1">// WARNING: This may return a nullptr.  If you require AutogradMeta to return</span>
<span class="c1">// a materialized structure, use materialize_autograd_meta instead.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="o">*</span><span class="w"> </span><span class="n">get_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="c1">// WARNING: This will return a nullptr if the Tensor is not a view.</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">DifferentiableViewMeta</span><span class="o">*</span><span class="w"> </span><span class="n">get_view_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="c1">// Returns the current autograd meta, materializing it if it was previously</span>
<span class="c1">// none.  This counts as a *mutating* operation, so do not call it on</span>
<span class="c1">// &quot;read-only&quot; operators; in particular, this is NOT thread safe</span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="o">*</span><span class="w"> </span><span class="n">materialize_autograd_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_grad_accumulator</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">try_get_grad_accumulator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_gradient_edge</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">edge</span><span class="p">);</span><span class="w"></span>

<span class="c1">// Autograd Graph Interaction</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">rebase_history</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="n">Node</span><span class="o">*</span><span class="w"> </span><span class="n">grad_fn_unsafe</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">bump_version</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_version_counter</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span><span class="w"> </span><span class="n">version_counter</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span><span class="w"> </span><span class="n">version_counter</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">set_name</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span><span class="w"> </span><span class="n">name</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">add_hook</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;</span><span class="w"> </span><span class="n">hook</span><span class="p">);</span><span class="w"></span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span><span class="w"> </span><span class="n">hooks</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>
<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">clear_hooks</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">);</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">create_cpp_hook</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_retains_grad_hooks</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace impl</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                            AutogradMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">AutogradMetaInterface</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="w"> </span><span class="n">name_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="n">Variable</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_accumulator_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="c1">// This field is used to store all the forward AD gradients</span>
<span class="w">  </span><span class="c1">// associated with this AutogradMeta (and the Tensor it corresponds to)</span>
<span class="w">  </span><span class="c1">// There is a semantic 1:1 correspondence between AutogradMeta and</span>
<span class="w">  </span><span class="c1">// ForwardGrad but:</span>
<span class="w">  </span><span class="c1">//   - This field is lazily populated.</span>
<span class="w">  </span><span class="c1">//   - This field is a shared_ptr but it must never be</span>
<span class="w">  </span><span class="c1">//     shared by multiple Tensors. See Note [ Using ForwardGrad ]</span>
<span class="w">  </span><span class="c1">// Any transition from not_initialized to initialized</span>
<span class="w">  </span><span class="c1">// must be protected by mutex_</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ForwardGrad</span><span class="o">&gt;</span><span class="w"> </span><span class="n">fw_grad_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="c1">// The hooks_ field is actually reused by both python and cpp logic</span>
<span class="w">  </span><span class="c1">// For both cases, we have a data structure, cpp_hooks_list_ (cpp)</span>
<span class="w">  </span><span class="c1">// or dict (python) which is the canonical copy.</span>
<span class="w">  </span><span class="c1">// Then, for both cases, we always register a single hook to</span>
<span class="w">  </span><span class="c1">// hooks_ which wraps all the hooks in the list/dict.</span>
<span class="w">  </span><span class="c1">// And, again in both cases, if the grad_fn exists on that tensor</span>
<span class="w">  </span><span class="c1">// we will additionally register a single hook to the grad_fn.</span>
<span class="w">  </span><span class="c1">//</span>
<span class="w">  </span><span class="c1">// Note that the cpp and python use cases aren&#39;t actually aware of</span>
<span class="w">  </span><span class="c1">// each other, so using both is not defined behavior.</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hooks_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">hooks_list</span><span class="o">&gt;</span><span class="w"> </span><span class="n">cpp_hooks_list_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="c1">// Only meaningful on leaf variables (must be false otherwise)</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span><span class="w"></span>

<span class="w">  </span><span class="c1">// Only meaningful on non-leaf variables (must be false otherwise)</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">retains_grad_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_view_</span><span class="p">{</span><span class="nb">false</span><span class="p">};</span><span class="w"></span>

<span class="w">  </span><span class="c1">// The &quot;output number&quot; of this variable; e.g., if this variable</span>
<span class="w">  </span><span class="c1">// was the second output of a function, then output_nr == 1.</span>
<span class="w">  </span><span class="c1">// We use this to make sure we can setup the backwards trace</span>
<span class="w">  </span><span class="c1">// correctly when this variable is passed to another function.</span>
<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">output_nr_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="c1">// Mutex to ensure that concurrent read operations that modify internal</span>
<span class="w">  </span><span class="c1">// state are still thread-safe. Used by grad_fn(), grad_accumulator(),</span>
<span class="w">  </span><span class="c1">// fw_grad() and set_fw_grad()</span>
<span class="w">  </span><span class="c1">// This is mutable because we need to be able to acquire this from const</span>
<span class="w">  </span><span class="c1">// version of this class for the functions above</span>
<span class="w">  </span><span class="k">mutable</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">mutex</span><span class="w"> </span><span class="n">mutex_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_requires_grad</span><span class="p">(</span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">,</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="p">)</span><span class="w"></span>
<span class="w">      </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="o">!</span><span class="n">requires_grad</span><span class="w"> </span><span class="o">||</span><span class="w"></span>
<span class="w">            </span><span class="n">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">typeMetaToScalarType</span><span class="p">(</span><span class="n">self_impl</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">())),</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;Only Tensors of floating point and complex dtype can require gradients&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">mutable_grad</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">grad_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="nf">fw_grad</span><span class="p">(</span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">)</span><span class="w"></span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="nf">set_fw_grad</span><span class="p">(</span><span class="w"></span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">new_grad</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">TensorBase</span><span class="o">&amp;</span><span class="w"> </span><span class="n">self</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="kt">uint64_t</span><span class="w"> </span><span class="n">level</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">is_inplace_op</span><span class="p">)</span><span class="w"> </span><span class="k">override</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="n">AutogradMeta</span><span class="p">(</span><span class="w"></span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Edge</span><span class="p">())</span><span class="w"></span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">grad_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">.</span><span class="n">function</span><span class="p">)),</span><span class="w"></span>

<span class="w">        </span><span class="n">output_nr_</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">.</span><span class="n">input_nr</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// set_requires_grad also checks error conditions.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_impl</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="c1">// NOLINTNEXTLINE(clang-analyzer-optin.cplusplus.VirtualCall)</span>
<span class="w">      </span><span class="n">set_requires_grad</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">,</span><span class="w"> </span><span class="n">self_impl</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="o">!</span><span class="n">grad_fn_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="o">!</span><span class="n">requires_grad_</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;requires_grad should be false if grad_fn is set&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="o">~</span><span class="n">AutogradMeta</span><span class="p">()</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// If AutogradMeta is being destroyed, it means that there is no other</span>
<span class="w">    </span><span class="c1">// reference to its corresponding Tensor. It implies that no other thread</span>
<span class="w">    </span><span class="c1">// can be using this object and so there is no need to lock mutex_ here to</span>
<span class="w">    </span><span class="c1">// guard the check if fw_grad_ is populated.</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">fw_grad_</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="c1">// See note [ Using ForwardGrad ]</span>
<span class="w">      </span><span class="n">fw_grad_</span><span class="o">-&gt;</span><span class="n">clear</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">ViewInfo</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">Variable</span><span class="w"> </span><span class="n">base_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_fn_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="nf">has_view_fn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">view_fn_</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_fn</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_view_fn</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;Can only access the view function if it exists.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">view_fn_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="n">ViewInfo</span><span class="w"> </span><span class="n">chain</span><span class="p">(</span><span class="w"></span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">base</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">tensor</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_func</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">)</span><span class="w"> </span><span class="k">const</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="n">ViewInfo</span><span class="p">(</span><span class="n">Variable</span><span class="w"> </span><span class="n">base</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span><span class="w"> </span><span class="n">view_fn</span><span class="p">)</span><span class="w"></span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="n">base_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">base</span><span class="p">)),</span><span class="w"> </span><span class="n">view_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">view_fn</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">base_</span><span class="p">.</span><span class="n">defined</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;base is undefined&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                     DifferentiableViewMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>



<span class="k">enum</span><span class="w"> </span><span class="k">class</span><span class="w"> </span><span class="nc">CreationMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="kt">uint8_t</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">DEFAULT</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">IN_CUSTOM_FUNCTION</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">MULTI_OUTPUT_NODE</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">NO_GRAD_MODE</span><span class="p">,</span><span class="w"></span>
<span class="w">  </span><span class="n">INFERENCE_MODE</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="kr">inline</span><span class="w"> </span><span class="n">CreationMeta</span><span class="w"> </span><span class="nf">propagate_creation_meta</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">prev_view_creation_meta</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">new_view_creation_meta</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="n">new_view_creation_meta</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">)</span><span class="w"></span>
<span class="w">      </span><span class="o">?</span><span class="w"> </span><span class="n">prev_view_creation_meta</span><span class="w"></span>
<span class="w">      </span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="n">prev_view_creation_meta</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">INFERENCE_MODE</span><span class="w"></span>
<span class="w">             </span><span class="o">?</span><span class="w"> </span><span class="n">prev_view_creation_meta</span><span class="w"></span>
<span class="w">             </span><span class="o">:</span><span class="w"> </span><span class="n">new_view_creation_meta</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">handle_view_on_rebase</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">DifferentiableViewMeta</span><span class="o">*</span><span class="w"> </span><span class="n">diff_view_meta</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">indirect</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">);</span><span class="w"></span>

<span class="k">struct</span><span class="w"> </span><span class="nc">TORCH_API</span><span class="w"> </span><span class="n">DifferentiableViewMeta</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="k">public</span><span class="w"> </span><span class="n">AutogradMeta</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w"> </span><span class="k">private</span><span class="o">:</span><span class="w"></span>
<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">;</span><span class="w"></span>

<span class="w">  </span><span class="c1">// Optimization to reduce the number of ViewInfo we create.</span>
<span class="w">  </span><span class="c1">// In the (very common) case where backward_info_ == forward_info_, we only</span>
<span class="w">  </span><span class="c1">// populate backward_info_ (that should be used as both the forward and</span>
<span class="w">  </span><span class="c1">// backward view information) and set shared_view_info_ = true. Invariants:</span>
<span class="w">  </span><span class="c1">//   - If shared_view_info_ is false, there is no special constraints on</span>
<span class="w">  </span><span class="c1">//     backward_info_ and forward_info_</span>
<span class="w">  </span><span class="c1">//   - If shared_view_info_ is true, we must have:</span>
<span class="w">  </span><span class="c1">//      - backward_info_.has_value() == true</span>
<span class="w">  </span><span class="c1">//      - forward_info_.has_value() == false</span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="p">;</span><span class="w"></span>


<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">attr_version_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta_</span><span class="p">;</span><span class="w"></span>

<span class="w"> </span><span class="k">public</span><span class="o">:</span><span class="w"></span>
<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="k">override</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">requires_grad_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">grad_fn_</span><span class="w"> </span><span class="o">||</span><span class="w"></span>
<span class="w">        </span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">()</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="n">get_backward_view</span><span class="p">().</span><span class="n">base_</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">());</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_bw_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">ViewInfo</span><span class="o">&amp;</span><span class="w"> </span><span class="n">get_backward_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;backward view info can only exist for backward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">get_attr_version</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">attr_version_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_attr_version</span><span class="p">(</span><span class="kt">uint32_t</span><span class="w"> </span><span class="n">new_attr_version</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">attr_version_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_attr_version</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">get_creation_meta</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">creation_meta_</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">void</span><span class="w"> </span><span class="n">set_creation_meta</span><span class="p">(</span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">new_creation_meta</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">creation_meta_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_creation_meta</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_fw_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="n">ViewInfo</span><span class="o">&amp;</span><span class="w"> </span><span class="n">get_forward_view</span><span class="p">()</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">has_fw_view</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;forward view info can only exist for forward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="o">!</span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="n">has_bw_view</span><span class="p">(),</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;forward view info can only exist for forward views.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">shared_view_info_</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="n">backward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">()</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="n">forward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="n">DifferentiableViewMeta</span><span class="p">(</span><span class="w"></span>
<span class="w">      </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">self_impl</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">);</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                        Variable Implementation</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="c1">// Factory Functions</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>



<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Differentiable view. Track history with DifferentiableViewMeta.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable_differentiable_view</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">backward_info</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span><span class="w"> </span><span class="n">forward_info</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">shared_view_info</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">CreationMeta</span><span class="w"> </span><span class="n">creation_meta</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="n">TORCH_CHECK</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">autograd_meta</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;Attempted to make a tensor into a differentiable view, but the &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;tensor already had autograd metadata associated with it.  If you are &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;using a __torch_dispatch__ mode, the most common cause for this &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;problem is that you used torch.overrides.enable_reentrant_dispatch() &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;improperly; tensors created within the extent of reentrant dispatch &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;MUST NOT be directly returned from __torch_dispatch__; instead, they &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;must be wrapped into fresh tensors that serve as the output.  If you &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;are not using wrappers, you probably don&#39;t need reentrant dispatch.  &quot;</span><span class="w"></span>
<span class="w">        </span><span class="s">&quot;If this doesn&#39;t seem applicable, please file a bug to PyTorch.&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span><span class="w"> </span><span class="n">data_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">unsafeGetTensorImpl</span><span class="p">();</span><span class="w"></span>
<span class="w">    </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_allow_tensor_metadata_change</span><span class="p">(</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">DifferentiableViewMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">backward_info</span><span class="p">),</span><span class="w"></span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">forward_info</span><span class="p">),</span><span class="w"></span>
<span class="w">        </span><span class="n">shared_view_info</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="n">creation_meta</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">data</span><span class="p">;</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Non-differentiable view. Just share version counter.</span>
<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable_non_differentiable_view</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">Variable</span><span class="w"> </span><span class="n">base</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="c1">// Currently all of non-differentiable view ops(detach/_indices/_values)</span>
<span class="w">    </span><span class="c1">// share the same TensorImpl as their base Tensor. Thus a new TensorImpl</span>
<span class="w">    </span><span class="c1">// allocation here is required.</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="cm">/*version_counter=*/</span><span class="n">impl</span><span class="o">::</span><span class="n">version_counter</span><span class="p">(</span><span class="n">base</span><span class="p">),</span><span class="w"></span>
<span class="w">        </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">requires_grad</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">().</span><span class="n">use_count</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"></span>
<span class="w">        </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">unique_version</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">unsafeReleaseIntrusivePtr</span><span class="p">();</span><span class="w"></span>
<span class="w">      </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_allow_tensor_metadata_change</span><span class="p">(</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="c1">// NOLINTNEXTLINE(bugprone-branch-clone)</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="w"></span>
<span class="w">            </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data_impl</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">));</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">data_impl</span><span class="p">));</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span><span class="w"></span>
<span class="w">          </span><span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">          </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="c1">// NOLINTNEXTLINE(bugprone-branch-clone)</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="w"></span>
<span class="w">            </span><span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="n">requires_grad</span><span class="p">));</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="kr">inline</span><span class="w"> </span><span class="n">Variable</span><span class="w"> </span><span class="nf">make_variable</span><span class="p">(</span><span class="w"></span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="n">Edge</span><span class="w"> </span><span class="n">gradient_edge</span><span class="p">,</span><span class="w"></span>
<span class="w">    </span><span class="kt">bool</span><span class="w"> </span><span class="n">allow_tensor_metadata_change</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">true</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span><span class="w"> </span><span class="p">{</span><span class="w"></span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">data_impl_copy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span><span class="w"></span>
<span class="w">        </span><span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span><span class="w"></span>
<span class="w">    </span><span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="w"></span>
<span class="w">        </span><span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">)));</span><span class="w"></span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">Variable</span><span class="p">();</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">utils</span><span class="w"> </span><span class="p">{</span><span class="w"></span>

<span class="n">TORCH_API</span><span class="w"> </span><span class="kt">bool</span><span class="w"> </span><span class="n">has_same_meta</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">base</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="n">Variable</span><span class="o">&amp;</span><span class="w"> </span><span class="n">other</span><span class="p">);</span><span class="w"></span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace utils</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace autograd</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch</span>

<span class="cp">#endif </span><span class="cm">/* DOXYGEN_SHOULD_SKIP_THIS */</span><span class="cp"></span>
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, PyTorch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File variable.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>
            
          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/torcharrow">torcharrow</a>
            </li>

            <li>
              <a href="https://pytorch.org/data">TorchData</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchrec">TorchRec</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/torchx/">TorchX</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>
            
           <ul class="resources-mobile-menu-items">

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>

            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
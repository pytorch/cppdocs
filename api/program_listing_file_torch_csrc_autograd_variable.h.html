


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File variable.h &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/api/program_listing_file_torch_csrc_autograd_variable.h.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/cpp_theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/audio/stable/index.html">
                  <span class="dropdown-title">torchaudio</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/text/stable/index.html">
                  <span class="dropdown-title">torchtext</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/vision/stable/index.html">
                  <span class="dropdown-title">torchvision</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/elastic/">
                  <span class="dropdown-title">TorchElastic</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/serve/">
                  <span class="dropdown-title">TorchServe</span>
                  <p></p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://pytorch.org/xla">
                  <span class="dropdown-title">PyTorch on XLA Devices</span>
                  <p></p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class="dropdown-title">About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/hub">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../installing.html">Installing C++ Distributions of PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../frontend.html">The C++ Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>
<p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_basics.html">Tensor Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_creation.html">Tensor Creation API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_cuda_stream.html">Tensor CUDA Stream API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tensor_indexing.html">Tensor Indexing API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/versioning.html">Library Versioning</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Program Listing for File variable.h</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            
              <!-- User defined GitHub URL -->
              <a href="https://github.com/pytorch/pytorch" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="program-listing-for-file-variable-h">
<span id="program-listing-file-torch-csrc-autograd-variable-h"></span><h1>Program Listing for File variable.h<a class="headerlink" href="#program-listing-for-file-variable-h" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_torch_csrc_autograd_variable.h.html#file-torch-csrc-autograd-variable-h"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">torch/csrc/autograd/variable.h</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#pragma once</span>

<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/utils/python_stub.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/WindowsTorchApiMacro.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/autograd/edge.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/autograd/function_hook.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/autograd/cpp_hook.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;torch/csrc/autograd/forward_grad.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;ATen/ATen.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;ATen/NamedTensorUtils.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;c10/util/Exception.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&lt;memory&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;mutex&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdexcept&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;string&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;utility&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;cstdint&gt;</span><span class="cp"></span>

<span class="k">namespace</span> <span class="n">torch</span> <span class="p">{</span> <span class="k">namespace</span> <span class="n">autograd</span> <span class="p">{</span>

<span class="k">using</span> <span class="n">Variable</span> <span class="o">=</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="p">;</span>

<span class="p">}</span> <span class="c1">// namespace autograd</span>
<span class="p">}</span> <span class="c1">// namespace torch</span>

<span class="c1">// The following are all internal APIs and should not be shown in libtorch docs.</span>
<span class="c1">// Therefore, we wrap the following code with `#ifndef DOXYGEN_SHOULD_SKIP_THIS ... #endif`</span>

<span class="cp">#ifndef DOXYGEN_SHOULD_SKIP_THIS</span>

<span class="k">namespace</span> <span class="n">torch</span> <span class="p">{</span> <span class="k">namespace</span> <span class="n">autograd</span> <span class="p">{</span>

<span class="k">static</span> <span class="kr">inline</span> <span class="kt">bool</span> <span class="n">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span> <span class="n">t</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nf">isFloatingType</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">||</span> <span class="n">isComplexType</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">struct</span> <span class="nc">Node</span><span class="p">;</span>


<span class="k">struct</span> <span class="nc">AutogradMeta</span><span class="p">;</span>
<span class="k">struct</span> <span class="nc">DifferentiableViewMeta</span><span class="p">;</span>

<span class="c1">// Private-ish functions for manipulating variables; we don&#39;t want to put them</span>
<span class="c1">// on Tensor proper</span>
<span class="k">namespace</span> <span class="n">impl</span> <span class="p">{</span>

  <span class="c1">// WARNING: This may return a nullptr.  If you require AutogradMeta to return</span>
  <span class="c1">// a materialized structure, use materialize_autograd_meta instead.</span>
  <span class="n">TORCH_API</span> <span class="n">AutogradMeta</span><span class="o">*</span> <span class="n">get_autograd_meta</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="c1">// Returns the current autograd meta, materializing it if it was previously</span>
  <span class="c1">// none.  This counts as a *mutating* operation, so do not call it on</span>
  <span class="c1">// &quot;read-only&quot; operators; in particular, this is NOT thread safe</span>
  <span class="n">TORCH_API</span> <span class="n">AutogradMeta</span><span class="o">*</span> <span class="n">materialize_autograd_meta</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">set_grad_accumulator</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_accumulator</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">try_get_grad_accumulator</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_accumulator</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="n">Edge</span> <span class="n">gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">set_gradient_edge</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="n">Edge</span> <span class="n">edge</span><span class="p">);</span>

  <span class="c1">// Autograd Graph Interaction</span>
  <span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">rebase_history</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="n">Edge</span> <span class="n">gradient_edge</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="n">Node</span><span class="o">*</span> <span class="n">grad_fn_unsafe</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">bump_version</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">set_version_counter</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span> <span class="n">version_counter</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">VariableVersion</span><span class="o">&amp;</span> <span class="n">version_counter</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="n">PyObject</span><span class="o">*</span> <span class="n">pyobj</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">set_pyobj</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="n">PyObject</span><span class="o">*</span> <span class="n">pyobj</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">set_name</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">string</span><span class="o">&amp;</span> <span class="n">name</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">add_hook</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;</span> <span class="n">hook</span><span class="p">);</span>
  <span class="n">TORCH_API</span> <span class="k">const</span> <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;&amp;</span> <span class="n">hooks</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">clear_hooks</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>

  <span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">create_cpp_hook</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                            AutogradMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>


<span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="nl">AutogradMeta</span> <span class="p">:</span> <span class="k">public</span> <span class="n">c10</span><span class="o">::</span><span class="n">AutogradMetaInterface</span> <span class="p">{</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">name_</span><span class="p">;</span>

  <span class="n">Variable</span> <span class="n">grad_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_fn_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">weak_ptr</span><span class="o">&lt;</span><span class="n">Node</span><span class="o">&gt;</span> <span class="n">grad_accumulator_</span><span class="p">;</span>

  <span class="c1">// This field is used to store all the forward AD gradients</span>
  <span class="c1">// associated with this AutogradMeta (and the Tensor it corresponds to)</span>
  <span class="c1">// There is a semantic 1:1 correspondence between AutogradMeta and</span>
  <span class="c1">// ForwardGrad but:</span>
  <span class="c1">//   - This field is lazily populated.</span>
  <span class="c1">//   - This field is a shared_ptr but it must never be</span>
  <span class="c1">//     shared by multiple Tensors. See Note [ Using ForwardGrad ]</span>
  <span class="c1">// Any transition from not_initialized to initialized</span>
  <span class="c1">// must be protected by mutex_</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">ForwardGrad</span><span class="o">&gt;</span> <span class="n">fw_grad_</span><span class="p">;</span>

  <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">FunctionPreHook</span><span class="o">&gt;&gt;</span> <span class="n">hooks_</span><span class="p">;</span>
  <span class="n">std</span><span class="o">::</span><span class="n">shared_ptr</span><span class="o">&lt;</span><span class="n">hooks_list</span><span class="o">&gt;</span> <span class="n">cpp_hooks_list</span><span class="p">;</span>

  <span class="c1">// Only meaningful on leaf variables (must be false otherwise)</span>
  <span class="kt">bool</span> <span class="n">requires_grad_</span><span class="p">;</span>

  <span class="c1">// Only meaningful on non-leaf variables (must be false otherwise)</span>
  <span class="kt">bool</span> <span class="n">retains_grad_</span><span class="p">;</span>

  <span class="kt">bool</span> <span class="n">is_view_</span><span class="p">;</span>

  <span class="c1">// The &quot;output number&quot; of this variable; e.g., if this variable</span>
  <span class="c1">// was the second output of a function, then output_nr == 1.</span>
  <span class="c1">// We use this to make sure we can setup the backwards trace</span>
  <span class="c1">// correctly when this variable is passed to another function.</span>
  <span class="kt">uint32_t</span> <span class="n">output_nr_</span><span class="p">;</span>

  <span class="c1">// Mutex to ensure that concurrent read operations that modify internal</span>
  <span class="c1">// state are still thread-safe. Used by grad_fn(), grad_accumulator(),</span>
  <span class="c1">// fw_grad() and set_fw_grad()</span>
  <span class="c1">// This is mutable because we need to be able to acquire this from const</span>
  <span class="c1">// version of this class for the functions above</span>
  <span class="k">mutable</span> <span class="n">std</span><span class="o">::</span><span class="n">mutex</span> <span class="n">mutex_</span><span class="p">;</span>

  <span class="kt">void</span> <span class="nf">set_requires_grad</span><span class="p">(</span><span class="kt">bool</span> <span class="n">requires_grad</span><span class="p">,</span> <span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span> <span class="n">self_impl</span><span class="p">)</span> <span class="k">override</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span>
      <span class="o">!</span><span class="n">requires_grad</span> <span class="o">||</span> <span class="n">isDifferentiableType</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">typeMetaToScalarType</span><span class="p">(</span><span class="n">self_impl</span><span class="o">-&gt;</span><span class="n">dtype</span><span class="p">())),</span>
      <span class="s">&quot;Only Tensors of floating point and complex dtype can require gradients&quot;</span><span class="p">);</span>
    <span class="n">requires_grad_</span> <span class="o">=</span> <span class="n">requires_grad</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">bool</span> <span class="nf">requires_grad</span><span class="p">()</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">requires_grad_</span> <span class="o">||</span> <span class="n">grad_fn_</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">Variable</span><span class="o">&amp;</span> <span class="nf">mutable_grad</span><span class="p">()</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">grad_</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">grad</span><span class="p">()</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">grad_</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">fw_grad</span><span class="p">(</span><span class="kt">uint64_t</span> <span class="n">level</span><span class="p">,</span> <span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">)</span> <span class="k">const</span> <span class="k">override</span><span class="p">;</span>

  <span class="kt">void</span> <span class="nf">set_fw_grad</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">new_grad</span><span class="p">,</span> <span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span> <span class="n">self</span><span class="p">,</span> <span class="kt">uint64_t</span> <span class="n">level</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">is_inplace_op</span><span class="p">)</span> <span class="k">override</span><span class="p">;</span>

  <span class="n">AutogradMeta</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span> <span class="n">self_impl</span> <span class="o">=</span> <span class="k">nullptr</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="n">Edge</span> <span class="n">gradient_edge</span> <span class="o">=</span> <span class="n">Edge</span><span class="p">()</span> <span class="p">)</span> <span class="p">{</span>
    <span class="n">grad_fn_</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">.</span><span class="n">function</span><span class="p">);</span>
    <span class="n">requires_grad_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="n">retains_grad_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="n">is_view_</span> <span class="o">=</span> <span class="nb">false</span><span class="p">;</span>
    <span class="n">output_nr_</span> <span class="o">=</span> <span class="n">gradient_edge</span><span class="p">.</span><span class="n">input_nr</span><span class="p">;</span>

    <span class="c1">// set_requires_grad also checks error conditions.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">TORCH_INTERNAL_ASSERT</span><span class="p">(</span><span class="n">self_impl</span><span class="p">);</span>
      <span class="n">set_requires_grad</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">self_impl</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span>
        <span class="o">!</span><span class="n">grad_fn_</span> <span class="o">||</span> <span class="o">!</span><span class="n">requires_grad_</span><span class="p">,</span>
        <span class="s">&quot;requires_grad should be false if grad_fn is set&quot;</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="o">~</span><span class="n">AutogradMeta</span><span class="p">()</span> <span class="k">override</span> <span class="p">{</span>
    <span class="c1">// If AutogradMeta is being destroyed, it means that there is no other reference to its</span>
    <span class="c1">// corresponding Tensor. It implies that no other thread can be using this object and so there is</span>
    <span class="c1">// no need to lock mutex_ here to guard the check if fw_grad_ is populated.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">fw_grad_</span><span class="p">)</span> <span class="p">{</span>
      <span class="c1">// See note [ Using ForwardGrad ]</span>
      <span class="n">fw_grad_</span><span class="o">-&gt;</span><span class="n">clear</span><span class="p">();</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="n">ViewInfo</span> <span class="p">{</span>
  <span class="n">Variable</span> <span class="n">base_</span><span class="p">;</span>

  <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">view_fn_</span><span class="p">;</span>

  <span class="kt">bool</span> <span class="nf">has_view_fn</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">view_fn_</span> <span class="o">!=</span> <span class="k">nullptr</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">view_fn</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_view_fn</span><span class="p">(),</span> <span class="s">&quot;Can only access the view function if it exists.&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">view_fn_</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">ViewInfo</span> <span class="n">chain</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span> <span class="o">&amp;</span> <span class="n">base</span><span class="p">,</span> <span class="k">const</span> <span class="n">Variable</span> <span class="o">&amp;</span> <span class="n">tensor</span><span class="p">,</span>
    <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">view_func</span><span class="o">=</span><span class="k">nullptr</span><span class="p">)</span> <span class="k">const</span><span class="p">;</span>

  <span class="n">ViewInfo</span><span class="p">(</span><span class="n">Variable</span> <span class="n">base</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">function</span><span class="o">&lt;</span><span class="n">Variable</span><span class="p">(</span><span class="k">const</span> <span class="n">Variable</span><span class="o">&amp;</span><span class="p">)</span><span class="o">&gt;</span> <span class="n">view_fn</span><span class="p">)</span> <span class="o">:</span>
    <span class="n">base_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">base</span><span class="p">)),</span>
    <span class="n">view_fn_</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">view_fn</span><span class="p">))</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">base_</span><span class="p">.</span><span class="n">defined</span><span class="p">(),</span> <span class="s">&quot;base is undefined&quot;</span><span class="p">);</span>
  <span class="p">}</span>
<span class="p">};</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                     DifferentiableViewMeta</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>




<span class="k">enum</span> <span class="k">class</span> <span class="nc">CreationMeta</span><span class="o">:</span> <span class="kt">uint8_t</span> <span class="p">{</span> <span class="n">DEFAULT</span><span class="p">,</span> <span class="n">IN_CUSTOM_FUNCTION</span><span class="p">,</span> <span class="n">MULTI_OUTPUT_NODE</span><span class="p">,</span>
                                   <span class="n">NO_GRAD_MODE</span><span class="p">,</span> <span class="n">MULTI_OUTPUT_SAFE</span> <span class="p">};</span>

<span class="kr">inline</span> <span class="n">CreationMeta</span> <span class="n">propagate_creation_meta</span><span class="p">(</span><span class="n">CreationMeta</span> <span class="n">prev_view_creation_meta</span><span class="p">,</span> <span class="n">CreationMeta</span> <span class="n">new_view_creation_meta</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">new_view_creation_meta</span> <span class="o">==</span> <span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">)</span> <span class="o">?</span> <span class="nl">prev_view_creation_meta</span> <span class="p">:</span> <span class="n">new_view_creation_meta</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">TORCH_API</span> <span class="kt">void</span> <span class="n">handle_view_on_rebase</span><span class="p">(</span><span class="n">DifferentiableViewMeta</span><span class="o">*</span> <span class="n">diff_view_meta</span><span class="p">,</span> <span class="kt">bool</span> <span class="n">indirect</span><span class="o">=</span><span class="nb">false</span><span class="p">);</span>

<span class="k">struct</span> <span class="nc">TORCH_API</span> <span class="nl">DifferentiableViewMeta</span> <span class="p">:</span> <span class="k">public</span> <span class="n">AutogradMeta</span> <span class="p">{</span>
<span class="k">private</span><span class="o">:</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">backward_info_</span><span class="p">;</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">forward_info_</span><span class="p">;</span>


  <span class="kt">uint32_t</span> <span class="n">attr_version</span><span class="p">;</span>
  <span class="n">CreationMeta</span> <span class="n">creation_meta</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
  <span class="kt">bool</span> <span class="n">requires_grad</span><span class="p">()</span> <span class="k">const</span> <span class="k">override</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">requires_grad_</span> <span class="o">||</span> <span class="n">grad_fn_</span> <span class="o">||</span> <span class="p">(</span><span class="n">has_bw_view</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">get_backward_view</span><span class="p">().</span><span class="n">base_</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">());</span>
  <span class="p">}</span>

  <span class="kt">bool</span> <span class="n">has_bw_view</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">backward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">ViewInfo</span><span class="o">&amp;</span> <span class="n">get_backward_view</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">(),</span> <span class="s">&quot;backward view info can only exist for backward views.&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">backward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="kt">uint32_t</span> <span class="n">get_attr_version</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">(),</span> <span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">attr_version</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">void</span> <span class="n">set_attr_version</span><span class="p">(</span><span class="kt">uint32_t</span> <span class="n">new_attr_version</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">(),</span> <span class="s">&quot;attr_version can only exist for backward views.&quot;</span><span class="p">);</span>
    <span class="n">attr_version</span> <span class="o">=</span> <span class="n">new_attr_version</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="n">CreationMeta</span> <span class="n">get_creation_meta</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">(),</span> <span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">creation_meta</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">void</span> <span class="n">set_creation_meta</span><span class="p">(</span><span class="n">CreationMeta</span> <span class="n">new_creation_meta</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_bw_view</span><span class="p">(),</span> <span class="s">&quot;creation_meta can only exist for backward views.&quot;</span><span class="p">);</span>
    <span class="n">creation_meta</span> <span class="o">=</span> <span class="n">new_creation_meta</span><span class="p">;</span>
  <span class="p">}</span>

  <span class="kt">bool</span> <span class="n">has_fw_view</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="k">return</span> <span class="n">forward_info_</span><span class="p">.</span><span class="n">has_value</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="k">const</span> <span class="n">ViewInfo</span><span class="o">&amp;</span> <span class="n">get_forward_view</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span>
    <span class="n">TORCH_CHECK</span><span class="p">(</span><span class="n">has_fw_view</span><span class="p">(),</span> <span class="s">&quot;forward view info can only exist for forward views.&quot;</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">forward_info_</span><span class="p">.</span><span class="n">value</span><span class="p">();</span>
  <span class="p">}</span>

  <span class="n">DifferentiableViewMeta</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span> <span class="n">self_impl</span><span class="p">,</span> <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">backward_info</span><span class="p">,</span>
    <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">forward_info</span><span class="p">,</span> <span class="n">CreationMeta</span> <span class="n">creation_meta</span><span class="o">=</span><span class="n">CreationMeta</span><span class="o">::</span><span class="n">DEFAULT</span><span class="p">);</span>
<span class="p">};</span>

<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>
<span class="c1">//                        Variable Implementation</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>

<span class="c1">// Factory Functions</span>
<span class="c1">//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span>



<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Differentiable view. Track history with DifferentiableViewMeta.</span>
<span class="kr">inline</span> <span class="n">Variable</span> <span class="n">make_variable_differentiable_view</span><span class="p">(</span>
    <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">data</span><span class="p">,</span>
    <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">backward_info</span><span class="p">,</span>
    <span class="n">c10</span><span class="o">::</span><span class="n">optional</span><span class="o">&lt;</span><span class="n">ViewInfo</span><span class="o">&gt;</span> <span class="n">forward_info</span><span class="p">,</span>
    <span class="n">CreationMeta</span> <span class="n">creation_meta</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">allow_tensor_metadata_change</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// If we already did a TensorImpl allocation for data, just reuse it.</span>
    <span class="c1">// Otherwise(e.g tensor.swapdim(0, 0) when we return the same tensor as input),</span>
    <span class="c1">// we have to use shallow_copy_and_detach to create a new TensorImpl to avoid</span>
    <span class="c1">// moving leaf node into graph interior. This guarantees only 1 TensorImpl</span>
    <span class="c1">// allocation happens in view ops.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">().</span><span class="n">unique</span><span class="p">()</span> <span class="o">&amp;&amp;</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">unique_version</span><span class="p">())</span> <span class="p">{</span>
      <span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">*</span> <span class="n">data_impl</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">unsafeGetTensorImpl</span><span class="p">();</span>
      <span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">DifferentiableViewMeta</span><span class="o">&gt;</span><span class="p">(</span>
      <span class="n">data_impl</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">backward_info</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">forward_info</span><span class="p">),</span>
      <span class="n">creation_meta</span><span class="p">));</span>
      <span class="k">return</span> <span class="n">data</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">TensorImpl</span><span class="o">&gt;</span> <span class="n">data_impl_copy</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
        <span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span>
        <span class="cm">/*allow_tensor_metadata_change=*/</span><span class="nb">true</span><span class="p">);</span>
      <span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">DifferentiableViewMeta</span><span class="o">&gt;</span><span class="p">(</span>
      <span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">backward_info</span><span class="p">),</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">forward_info</span><span class="p">),</span>
      <span class="n">creation_meta</span><span class="p">));</span>
      <span class="k">return</span> <span class="nf">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="c1">// See NOTE [ Autograd View Variables ] for details.</span>
<span class="c1">// Non-differentiable view. Just share version counter.</span>
<span class="kr">inline</span> <span class="n">Variable</span> <span class="n">make_variable_non_differentiable_view</span><span class="p">(</span>
    <span class="n">Variable</span> <span class="n">base</span><span class="p">,</span>
    <span class="k">const</span> <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">data</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">allow_tensor_metadata_change</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
    <span class="c1">// Currently all of non-differentiable view ops(detach/_indices/_values)</span>
    <span class="c1">// share the same TensorImpl as their base Tensor. Thus a new TensorImpl</span>
    <span class="c1">// allocation here is required.</span>
    <span class="k">auto</span> <span class="n">data_impl_copy</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
      <span class="cm">/*version_counter=*/</span><span class="n">impl</span><span class="o">::</span><span class="n">version_counter</span><span class="p">(</span><span class="n">base</span><span class="p">),</span>
      <span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
    <span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
    <span class="k">return</span> <span class="nf">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="kr">inline</span> <span class="n">Variable</span> <span class="n">make_variable</span><span class="p">(</span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">data</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">allow_tensor_metadata_change</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">().</span><span class="n">use_count</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">unique_version</span><span class="p">())</span> <span class="p">{</span>
      <span class="k">auto</span> <span class="n">data_impl</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">unsafeReleaseIntrusivePtr</span><span class="p">();</span>
      <span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_allow_tensor_metadata_change</span><span class="p">(</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data_impl</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="p">));</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">data_impl</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">data_impl</span><span class="p">));</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
      <span class="k">auto</span> <span class="n">data_impl_copy</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
        <span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span>
        <span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span>
          <span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">requires_grad</span><span class="p">));</span>
      <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="k">nullptr</span><span class="p">);</span>
      <span class="p">}</span>
      <span class="k">return</span> <span class="n">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
    <span class="p">}</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>

<span class="kr">inline</span> <span class="n">Variable</span> <span class="n">make_variable</span><span class="p">(</span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span> <span class="n">data</span><span class="p">,</span>
    <span class="n">Edge</span> <span class="n">gradient_edge</span><span class="p">,</span>
    <span class="kt">bool</span> <span class="n">allow_tensor_metadata_change</span> <span class="o">=</span> <span class="nb">true</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">defined</span><span class="p">())</span> <span class="p">{</span>
    <span class="k">auto</span> <span class="n">data_impl_copy</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">getIntrusivePtr</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">shallow_copy_and_detach</span><span class="p">(</span>
      <span class="cm">/*version_counter=*/</span><span class="mi">0</span><span class="p">,</span>
      <span class="cm">/*allow_tensor_metadata_change=*/</span><span class="n">allow_tensor_metadata_change</span><span class="p">);</span>
    <span class="n">data_impl_copy</span><span class="o">-&gt;</span><span class="n">set_autograd_meta</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">AutogradMeta</span><span class="o">&gt;</span><span class="p">(</span>
      <span class="n">data_impl_copy</span><span class="p">.</span><span class="n">get</span><span class="p">(),</span> <span class="nb">false</span><span class="p">,</span> <span class="n">std</span><span class="o">::</span><span class="n">move</span><span class="p">(</span><span class="n">gradient_edge</span><span class="p">)));</span>
    <span class="k">return</span> <span class="nf">Variable</span><span class="p">(</span><span class="n">data_impl_copy</span><span class="p">);</span>
  <span class="p">}</span>
  <span class="k">return</span> <span class="n">Variable</span><span class="p">();</span>
<span class="p">}</span>


<span class="p">}}</span> <span class="c1">// namespace torch::autograd</span>

<span class="cp">#endif </span><span class="cm">/* DOXYGEN_SHOULD_SKIP_THIS */</span><span class="cp"></span>
</pre></div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Program Listing for File variable.h</a></li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/language_data.js"></script>
         <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
         <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title" class="active">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/features">About</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>
:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file__var_log_chronos_job_2018-09-06-16_798925102_2851777208_pytorch_torch_csrc_api_include_torch_optim_optimizer.h:

Program Listing for File optimizer.h
====================================

- Return to documentation for :ref:`file__var_log_chronos_job_2018-09-06-16_798925102_2851777208_pytorch_torch_csrc_api_include_torch_optim_optimizer.h`

.. code-block:: cpp

   #pragma once
   
   #include <torch/csrc/autograd/generated/variable_factories.h>
   #include <torch/nn/cursor.h>
   #include <torch/tensor.h>
   
   #include <algorithm>
   #include <functional>
   #include <memory>
   #include <vector>
   
   namespace torch {
   namespace optim {
   namespace detail {
   
   class OptimizerBase {
    public:
     using ParameterCursor = torch::detail::CursorBase<Tensor>;
   
     explicit OptimizerBase(std::vector<Tensor> parameters);
   
     explicit OptimizerBase(const ParameterCursor& cursor);
   
     virtual ~OptimizerBase() = default;
   
     void add_parameters(const std::vector<Tensor>& parameters);
   
     void add_parameters(const ParameterCursor& cursor);
   
     virtual void zero_grad();
   
     const std::vector<Tensor>& parameters() const noexcept;
   
     std::vector<Tensor>& parameters() noexcept;
   
     size_t size() const noexcept;
   
    protected:
     OptimizerBase() = default;
   
     template <typename T>
     T& buffer_at(std::vector<T>& buffers, size_t index) {
       if (buffers.size() <= index) {
         const auto old_size = buffers.size();
         buffers.resize(index + 1);
         std::fill(buffers.begin() + old_size, buffers.end(), T{0});
       }
       return buffers[index];
     }
   
     Tensor& buffer_at(std::vector<Tensor>& buffers, size_t index) {
       if (buffers.size() <= index) {
         buffers.reserve(index);
         for (auto i = buffers.size(); i <= index; ++i) {
           buffers.push_back(torch::zeros_like(parameters_.at(i)));
         }
       }
       // Copy the buffer to the device and dtype of the parameter.
       const auto& parameter = parameters_.at(index);
       const auto& buffer = buffers.at(index);
       if (buffer.device() != parameter.device() ||
           buffer.dtype() != parameter.dtype()) {
         buffers[index] = buffer.to(parameter.device(), parameter.dtype());
       }
       return buffers[index];
     }
   
     std::vector<Tensor> parameters_;
   };
   } // namespace detail
   
   class Optimizer : public detail::OptimizerBase {
    public:
     using detail::OptimizerBase::OptimizerBase;
     virtual void step() = 0;
   };
   
   class LossClosureOptimizer : public detail::OptimizerBase {
    public:
     using LossClosure = std::function<Tensor()>;
     using detail::OptimizerBase::OptimizerBase;
     virtual Tensor step(LossClosure closure) = 0;
   };
   
   } // namespace optim
   } // namespace torch

:github_url: https://github.com/pytorch/pytorch


.. _program_listing_file_torch_csrc_autograd_function.h:

Program Listing for File function.h
===================================

|exhale_lsh| :ref:`Return to documentation for file <file_torch_csrc_autograd_function.h>` (``torch/csrc/autograd/function.h``)

.. |exhale_lsh| unicode:: U+021B0 .. UPWARDS ARROW WITH TIP LEFTWARDS

.. code-block:: cpp

   #pragma once
   
   #include <torch/csrc/autograd/edge.h>
   #include <torch/csrc/autograd/grad_mode.h>
   #include <torch/csrc/autograd/anomaly_mode.h>
   #include <torch/csrc/autograd/profiler.h>
   #include <torch/csrc/autograd/saved_variable.h>
   #include <torch/csrc/autograd/input_metadata.h>
   #include <torch/csrc/autograd/variable.h>
   #include <torch/csrc/utils/python_stub.h>
   #include <torch/csrc/utils/variadic.h>
   
   #include <ATen/ATen.h>
   #include <ATen/SequenceNumber.h>
   #include <c10/util/Exception.h>
   
   #include <algorithm>
   #include <cstdint>
   #include <initializer_list>
   #include <memory>
   #include <string>
   #include <utility>
   #include <vector>
   
   namespace torch { namespace autograd {
   
   struct Edge;
   struct FunctionPostHook;
   struct FunctionPreHook;
   
   using tensor_list = std::vector<at::Tensor>;
   using variable_list = std::vector<Variable>;
   using edge_list = std::vector<Edge>;
   using saved_variable_list = std::vector<SavedVariable>;
   using IndexRange = std::pair<size_t, size_t>;
   
   // Custom deleter to prevent stack overflows.
   TORCH_API void deleteNode(Node* function);
   
   // Guard that sets and restores the evaluating node
   class NodeGuard {
    public:
     explicit NodeGuard(std::shared_ptr<Node> node);
     ~NodeGuard();
   
    private:
     std::shared_ptr<Node> last_evaluating_node_;
   };
   
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   //                               Node
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   // A `Node` is an abstract class that represents an operation taking zero
   // or more input `Variable`s and producing zero or more output `Variable`s. All
   // functions in PyTorch's autograd machinery derive from this class and
   // override its `apply` method. Instances of such subclasses will then be
   // invokeable via the call operator.
   //
   //                    Nodes in the Autograd Graph
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   // When viewing the autograd system as a graph, `Node`s are the vertices or
   // nodes, connected to each other via (directed) `Edge`s, which themselves are
   // represented via (`Node`, input_nr) pairs. `Variable`s are the outputs to
   // and inputs of `Node`s, and travel between these edges during execution
   // of the graph. When two or more `Edge`s (from different sources) point at the
   // same input to a `Node`, the values produced along all of these edges are
   // implicitly summed prior to being forwarded to the target `Node`.
   //
   //                              Hierarchy
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   // Subclasses usually represent differentiable functions as well as their
   // gradient operators. Note, however, that due to the very general definition
   // of a `Node` taking *zero* or more inputs and producing *zero* or more
   // outputs, uses of `Node`s are flexible and extend beyond purely
   // mathematical operations. For example, the `AccumulateGrad` function is a
   // *sink*: it takes one input, but produces no outputs, instead accumulating
   // the input as a side effect. At the other extreme, the `GraphRoot` function
   // receives no inputs from other functions, but produces multiple outputs.
   //
   //                              Interface
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   // The most important method on `Node` is the call operator, which takes in
   // a list of variables and produces a list of variables. The precise size of
   // these lists can be determined with `num_inputs()` and `num_outputs()`.
   // `Node`s are stitched together via their `next_edge` interface, which let
   // you manipulate the set of outgoing edges of a `Node`. You can add an
   // edge with `add_next_edge()`, retrieve an edge with `next_edge(index)` and
   // iterate over them via the `next_edges()` method. Other methods exist for
   // integration with the JIT and other parts of PyTorch. Every `Node` has a
   // *sequence number* that increases monotonically in the order of `Node`
   // construction. It can be retrieved via the `sequence_nr()` method. Note that
   // this sequence number is *thread local*. This means that when `Node`s
   // `A`, `B` and `C` are created consecutively in the same thread, their
   // sequence numbers will be ordered `A` < `B` < `C`. If, however, `A` and `B`
   // are created in one thread and `C` is created in a new thread, there are *no
   // guarantees* w.r.t. the ordering of `C` relative to `A` or `B`.
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   struct TORCH_API Node : std::enable_shared_from_this<Node> {
    public:
     explicit Node(
         uint64_t sequence_nr,
         edge_list&& next_edges = edge_list())
         : sequence_nr_(sequence_nr),
         next_edges_(std::move(next_edges)) {
       if (AnomalyMode::is_enabled()) {
         metadata()->store_stack();
   
         // If anomaly mode is enabled and graph is constructed, then assign the
         // currently evaluating node as the parent of this node.
         // A parent is a Node where this Node is created.
         // We are tracking the parents to track multiple backward operations.
         assign_parent();
       }
   
       if (profiler::profilerEnabled()) {
         thread_id_ = at::RecordFunction::currentThreadId();
       }
     }
   
     explicit Node(edge_list&& next_edges = edge_list())
         : Node(at::sequence_number::get_and_increment(), std::move(next_edges)) {}
   
     Node(const Node& other) = delete;
     Node(Node&& other) = delete;
     Node& operator=(const Node& other) = delete;
     Node& operator=(Node&& other) = delete;
     virtual ~Node() = default;
   
     variable_list operator()(variable_list&& inputs) {
       // In the first iteration of named tensors, autograd ignores names and
       // operates on unnamed tensors. In the long term, autograd should
       // probably operate with names.
       at::NoNamesGuard no_names_guard;
   
       bool pre_sampled = false;
       if (at::shouldRunRecordFunction(&pre_sampled)) {
         // Using RecordFunction to trogger observers in the backward pass
         at::RecordFunction guard(at::RecordScope::BACKWARD_FUNCTION, pre_sampled);
         if (guard.isActive()) {
           // Using sequence number and thread id to correlate with
           // the forward pass function
           guard.setForwardThreadId(thread_id_);
           if (guard.needsInputs()) {
             guard.before(
               name(),
               std::vector<c10::IValue>(inputs.begin(), inputs.end()),
               sequence_nr());
           } else {
             guard.before(name(), sequence_nr());
           }
         }
         // keeping stack guard object alive during the call
         return apply(std::move(inputs));
       } else {
         return apply(std::move(inputs));
       }
     }
   
     // Graph Connectivity API
     //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
     // Inputs. NOTE: inputs of the grad_fn correspond to Tensor outputs of the
     // forward function.
   
     // Marker for expected undefined input
     struct undefined_input {};
   
     uint32_t add_input_metadata(
       const at::TensorOptions& options
     , at::IntArrayRef shape
     , at::Device device) noexcept {
       uint32_t input_nr = input_metadata_.size();
       input_metadata_.emplace_back(options, shape, device);
       return input_nr;
     }
   
     uint32_t add_input_metadata(const at::Tensor& t) noexcept {
       uint32_t input_nr = input_metadata_.size();
       input_metadata_.emplace_back(t);
       return input_nr;
     }
   
     uint32_t add_input_metadata(undefined_input u) noexcept {
       uint32_t input_nr = input_metadata_.size();
       input_metadata_.emplace_back();
       return input_nr;
     }
   
     uint32_t num_inputs() const noexcept {
       return input_metadata_.size();
     }
   
     const InputMetadata& input_metadata(size_t index) const {
       return input_metadata_[index];
     }
   
     c10::optional<c10::Stream> stream(const c10::DeviceType device_type) {
       for (const auto& metadata : input_metadata_) {
         if (metadata.device().type() == device_type) return metadata.stream();
       }
   
       return c10::nullopt;
     }
   
     void clear_input_metadata() {
       input_metadata_.clear();
     }
   
     // Outputs ("Next Edges")
   
     const Edge& next_edge(size_t index) const noexcept {
       return next_edges_[index];
     }
   
     void set_next_edge(size_t index, Edge edge) {
       next_edges_[index] = std::move(edge);
     }
   
     void add_next_edge(Edge edge) {
       next_edges_.push_back(std::move(edge));
     }
   
     void set_next_edges(edge_list&& next_edges) {
       next_edges_ = std::move(next_edges);
     }
   
     const edge_list& next_edges() const noexcept {
       return next_edges_;
     }
   
     edge_list& next_edges() noexcept {
       return next_edges_;
     }
   
     uint32_t num_outputs() const noexcept {
       return next_edges_.size();
     }
   
     // Miscellaneous Methods
     //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
     uint64_t sequence_nr() const noexcept {
       return sequence_nr_;
     }
   
     // assigning a node as a parent to this node
     void assign_parent();
   
     uint64_t thread_id() const noexcept {
       return thread_id_;
     }
   
     virtual std::string name() const;
   
     bool should_compute_output(size_t output_edge_index) const {
       TORCH_CHECK(output_edge_index < num_outputs(), "Index out of range");
       return next_edges_[output_edge_index].is_valid();
     }
   
     bool should_compute_output(std::initializer_list<IndexRange> idxs) const {
       return std::any_of(idxs.begin(), idxs.end(), [this](IndexRange range) {
         for (auto i = range.first; i < range.second; i++) {
           if (should_compute_output(i))
             return true;
         }
         return false;
       });
     }
   
     PyObject* pyobj() const noexcept {
       return pyobj_;
     }
   
     void set_pyobj(PyObject* pyobj) noexcept {
       pyobj_ = pyobj;
     }
   
     AnomalyMetadata* metadata() noexcept;
   
     // Hook API
     //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
     uintptr_t add_post_hook(std::unique_ptr<FunctionPostHook>&& post_hook) {
       post_hooks_.push_back(std::move(post_hook));
       // Use the raw pointer as the unique key to identify this hook. This key
       // can then be used in del_post_hook(key) to remove this hook.
       return reinterpret_cast<std::uintptr_t>(post_hooks_.back().get());
     }
   
     const std::vector<std::unique_ptr<FunctionPostHook>>& post_hooks() const
         noexcept {
       return post_hooks_;
     }
   
     // delete a post hook matching the key
     bool del_post_hook(const uintptr_t& key) {
       for (auto it = post_hooks_.begin(); it != post_hooks_.end(); ++it) {
         if (key == reinterpret_cast<std::uintptr_t>(it->get())) {
           post_hooks_.erase(it);
           return true;
         }
       }
       return false;
     }
   
     std::vector<std::unique_ptr<FunctionPostHook>>& post_hooks() noexcept {
       return post_hooks_;
     }
   
     void add_pre_hook(std::unique_ptr<FunctionPreHook>&& pre_hook) {
       pre_hooks_.push_back(std::move(pre_hook));
     }
   
     const std::vector<std::unique_ptr<FunctionPreHook>>& pre_hooks() const
         noexcept {
       return pre_hooks_;
     }
   
     std::vector<std::unique_ptr<FunctionPreHook>>& pre_hooks() noexcept {
       return pre_hooks_;
     }
   
     // Customization Points for Subclasses
     //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
     virtual void release_variables() {}
   
     virtual void will_release_variables() {}
   
     virtual bool is_traceable() {
       return false;
     }
   
     virtual bool passes_state_transparently() {
       return false;
     }
   
    protected:
     virtual variable_list apply(variable_list&& inputs) = 0;
   
     variable_list traced_apply(variable_list inputs);
   
     // Since `Node`s are neither copyable nor moveable, we can have const
     // fields.
     const uint64_t sequence_nr_;
   
     // Id of the thread that created the instance
     uint64_t thread_id_ = 0;
   
     // Note [Thread Safety on Autograd Node]
     // ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     // Autograd Engine let the owning thread which calls Engine::execute to drive the
     // GraphTask execution, there might be cases that part of the GraphTask is shared
     // across different `backward()` or `grad()` calls, i.e. fork new threads in the
     // middle of the forward and call `backward()` separately from different threads.
     // We need to protect the thread safety on NodeTask to prevent data racing on
     // shared variables read/write.
     //
     // NB: This is only needed for Autograd Nodes that runs on CPU, technically "CUDA",
     // "XLA" nodes don't need locking because device threads are always single threaded.
     //
     // Here we add a thread mutex to help protect the Node's thread safety, so that
     // different threads cannot race the shared data when executing the same NodeTask
     // from multiple CPU threads. It IS the user/developer responsibility to take
     // advantage of this mutex to protect the thread safety of their autograd Node.
     // The general strategy of thread safety on autograd Node:
     //
     // 1. User should lock the mutex during Node::release_variables() if the Node needs
     //    to release the variables on the fly, this serve the purpose that when we release
     //    saved_variables from one thread, no other threads can release the saved variables
     //    concurrently. call
     //    the Node::apply(),
     // 2. User should lock the mutex during Node::apply(), this is to ensure Node that
     //    writing to the shared variable are not racing across threads (i.e. AccumulateGrad
     //    and custom C++ Autograd Node if writing to shared variables )
     // 3. item 2 and item 3 should work together so that when we release saved variables
     //    from one thread, no other threads can call Node::apply(), this ensures the variable
     //    references from other threads aren't dangling.
     // 4. if the Node don't release any variables and no shared data read/write in the Node
     //    i.e. purely functional, user don't need to lock the mutex
     //
     // This way we could protect the thread safety on Autograd Node, but we could still
     // not protect the thread safety on Node pre/post C++ hooks (python hooks are
     // automatically thread safe), we rely on the user to write thread safe C++ hooks
     // if they want the hook to be correctly applied in multithreading environment.
     std::mutex mutex_;
   
     edge_list next_edges_;
     PyObject* pyobj_ = nullptr; // weak reference
     std::unique_ptr<AnomalyMetadata> anomaly_metadata_ = nullptr;
     std::vector<std::unique_ptr<FunctionPreHook>> pre_hooks_;
     std::vector<std::unique_ptr<FunctionPostHook>> post_hooks_;
     at::SmallVector<InputMetadata, 2> input_metadata_;
   };
   
   struct TraceableFunction : public Node {
     using Node::Node;
     bool is_traceable() final {
       return true;
     }
   };
   
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   //                       Associated Free Nodes
   //~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   
   namespace detail {
   // Implementation of `collect_next_edges` (see below).
   struct MakeNextFunctionList : IterArgs<MakeNextFunctionList> {
     edge_list next_edges;
     using IterArgs<MakeNextFunctionList>::operator();
     void operator()(const Variable& variable) {
       if (variable.defined()) {
         next_edges.push_back(impl::gradient_edge(variable));
       } else {
         next_edges.emplace_back();
       }
     }
     void operator()(const c10::optional<Variable>& variable) {
       if (variable.has_value() && variable->defined()) {
         next_edges.push_back(impl::gradient_edge(*variable));
       } else {
         next_edges.emplace_back();
       }
     }
   };
   } // namespace detail
   
   inline void create_gradient_edge(
       Variable& variable,
       std::shared_ptr<Node> function) {
     // Copy before move.
     const auto input_nr = function->add_input_metadata(variable);
     impl::set_gradient_edge(variable, {std::move(function), input_nr});
   }
   
   inline bool any_variable_requires_grad(const variable_list& variables) {
     return std::any_of(
         variables.begin(), variables.end(), [](const Variable& variable) {
           return variable.defined() && variable.requires_grad();
         });
   }
   
   template <typename... Variables>
   edge_list collect_next_edges(Variables&&... variables) {
     if (!GradMode::is_enabled())
       return {};
     detail::MakeNextFunctionList make;
     make.apply(std::forward<Variables>(variables)...);
     return std::move(make.next_edges);
   }
   }} // namespace torch::autograd
